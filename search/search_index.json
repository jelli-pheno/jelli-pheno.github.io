{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"jelli - JAX-based EFT Likelihoods","text":"<p><code>jelli</code> is a Python package for building and evaluating likelihood functions in the Effective Field Theory (EFT) framework.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>EFT Framework: Construction of likelihoods in EFTs, such as the Standard Model Effective Field Theory (SMEFT) and Weak Effective Theory (WET).</li> <li>Flexibility: Supports arbitrary observable predictions provided in the POPxf data format, and a multitude of experimental likelihood assumptions.</li> <li>JAX Integration: Built on JAX for high-performance numerical computing.</li> <li>Differentiable: Fully differentiable likelihood functions due to JAX's autodiff, enabling efficient gradient and Hessian computations, gradient-based optimization and sampling, and more.</li> <li>Fast: Utilizes JAX's Just-In-Time (JIT) compilation for optimized performance.</li> <li>Multi-scale: Interfaced with rgevolve for fast renormalization group evolution using the evolution matrix formalism.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>The package can be installed via pip:</p> <pre><code>pip install jelli\n</code></pre>"},{"location":"#documentation","title":"Documentation","text":"<p>A documentation website is coming soon.</p>"},{"location":"#citation","title":"Citation","text":"<p>A paper describing <code>jelli</code> is in preparation.</p>"},{"location":"#bugs-and-feature-requests","title":"Bugs and feature requests","text":"<p>Please report bugs and request features via the GitHub issues page.</p>"},{"location":"#contributors","title":"Contributors","text":"<p>Authors:</p> <ul> <li>Aleks Smolkovic (@alekssmolkovic)</li> <li>Peter Stangl (@peterstangl)</li> </ul>"},{"location":"#license","title":"License","text":"<p><code>jelli</code> is licensed under the MIT License.</p>"},{"location":"jelli/core/custom_basis/","title":"jelli.core.custom_basis","text":""},{"location":"jelli/core/custom_basis/#jelli.core.custom_basis.CustomBasis","title":"<code>CustomBasis</code>","text":"<p>A class to represent a custom parameter basis.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the custom basis.</p> required <code>parameters</code> <code>list or dict</code> <p>The parameters in the custom basis. If a list is provided, all parameters are assumed to be real. If a dict is provided, the keys are the parameter names and the values are the types (e.g., <code>R</code> for real, <code>C</code> for complex).</p> required <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the custom basis.</p> <code>parameters</code> <code>dict</code> <p>The parameters in the custom basis. The keys are the parameter names and the values are the types (e.g., <code>R</code> for real, <code>C</code> for complex).</p> <p>Methods:</p> Name Description <code>get_all_names</code> <p>Get all custom basis names.</p> <code>get</code> <p>Get a custom basis by name.</p> <code>get_all</code> <p>Get all custom basis objects.</p> <code>get_parameter_basis</code> <p>Get the parameter basis.</p> <p>Examples:</p> <p>Initialize a custom basis with real parameters:</p> <pre><code>&gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n</code></pre> <p>Initialize a custom basis with mixed parameters:</p> <pre><code>&gt;&gt;&gt; custom_basis = CustomBasis('example_basis', {'param1': 'R', 'param2': 'C'})\n</code></pre> <p>Get all custom basis names:</p> <pre><code>&gt;&gt;&gt; CustomBasis.get_all_names()\n</code></pre> <p>Get a custom basis by name:</p> <pre><code>&gt;&gt;&gt; custom_basis = CustomBasis.get('example_basis')\n</code></pre> <p>Get all custom basis objects:</p> <pre><code>&gt;&gt;&gt; custom_bases = CustomBasis.get_all()\n</code></pre> <p>Get the parameter basis:</p> <pre><code>&gt;&gt;&gt; parameter_basis = custom_basis.get_parameter_basis()\n</code></pre> Source code in <code>jelli/core/custom_basis.py</code> <pre><code>class CustomBasis:\n    '''\n    A class to represent a custom parameter basis.\n\n    Parameters\n    ----------\n    name : str\n        The name of the custom basis.\n    parameters : list or dict\n        The parameters in the custom basis. If a list is provided, all parameters are assumed to be real. If a dict is provided, the keys are the parameter names and the values are the types (e.g., `R` for real, `C` for complex).\n\n    Attributes\n    ----------\n    name : str\n        The name of the custom basis.\n    parameters : dict\n        The parameters in the custom basis. The keys are the parameter names and the values are the types (e.g., `R` for real, `C` for complex).\n\n    Methods\n    -------\n    get_all_names() -&gt; List[str]\n        Get all custom basis names.\n    get(name: str) -&gt; 'CustomBasis'\n        Get a custom basis by name.\n    get_all() -&gt; List['CustomBasis']\n        Get all custom basis objects.\n    get_parameter_basis() -&gt; List\n        Get the parameter basis.\n\n    Examples\n    --------\n    Initialize a custom basis with real parameters:\n\n    &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n\n    Initialize a custom basis with mixed parameters:\n\n    &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', {'param1': 'R', 'param2': 'C'})\n\n    Get all custom basis names:\n\n    &gt;&gt;&gt; CustomBasis.get_all_names()\n\n    Get a custom basis by name:\n\n    &gt;&gt;&gt; custom_basis = CustomBasis.get('example_basis')\n\n    Get all custom basis objects:\n\n    &gt;&gt;&gt; custom_bases = CustomBasis.get_all()\n\n    Get the parameter basis:\n\n    &gt;&gt;&gt; parameter_basis = custom_basis.get_parameter_basis()\n\n    '''\n\n    _custom_bases: Dict[str, 'CustomBasis'] = {}  # Class attribute to store all custom bases\n\n    def __init__(self, name: str, parameters: Union[List[str], Dict[str, str]]):\n        \"\"\"\n        Initialize the CustomBasis class.\n\n        Parameters\n        ----------\n        name : str\n            The name of the custom basis.\n        parameters : list or dict\n            The parameters in the custom basis. If a list is provided, all parameters are assumed to be real. If a dict is provided, the keys are the parameter names and the values are the types (e.g., `R` for real, `C` for complex).\n\n        Returns\n        --------\n        None\n\n        Examples\n        --------\n        Initialize a custom basis with real parameters:\n\n        &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n\n        Initialize a custom basis with mixed parameters:\n\n        &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', {'param1': 'R', 'param2': 'C'})\n        \"\"\"\n        self.name = name\n        if isinstance(parameters, list):\n            self.parameters = {param: 'R' for param in parameters}\n        elif isinstance(parameters, dict):\n            if not all(value in ['R', 'C'] for value in parameters.values()):\n                raise ValueError(\"All parameter types must be either 'R' or 'C'.\")\n            self.parameters = parameters\n        else:\n            raise ValueError(\"Parameters must be a list or a dictionary.\")\n        self._custom_bases[self.name] = self\n\n    @classmethod\n    def get_all_names(cls) -&gt; List[str]:\n        \"\"\"\n        Get all custom basis names.\n\n        Returns\n        --------\n        list\n            A list of all custom basis names.\n\n        Examples\n        --------\n            &gt;&gt;&gt; CustomBasis.get_all_names()\n        \"\"\"\n        return sorted(cls._custom_bases.keys())\n\n    @classmethod\n    def get(cls, name: str) -&gt; 'CustomBasis':\n        \"\"\"\n        Get a custom basis by name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the custom basis.\n        Returns\n        --------\n        CustomBasis\n            The custom basis object.\n\n        Examples\n        --------\n        &gt;&gt;&gt; CustomBasis.get('example_basis')\n        \"\"\"\n        return cls._custom_bases.get(name)\n\n    @classmethod\n    def get_all(cls) -&gt; List['CustomBasis']:\n        \"\"\"\n        Get all custom basis objects.\n\n        Returns\n        --------\n        list\n            A list of all custom basis objects.\n\n        Examples\n        --------\n            &gt;&gt;&gt; CustomBasis.get_all()\n        \"\"\"\n        return list(cls._custom_bases.values())\n\n    def get_parameter_basis(self, split_re_im=True) -&gt; List:\n        \"\"\"\n        Get the parameter basis.\n\n        Parameters\n        ----------\n        split_re_im : bool, optional\n            If `True`, split parameters into real and imaginary parts, otherwise return the parameters directly. Default is `True`.\n\n        Returns\n        --------\n        list\n            A list containing the parameter basis.\n\n        Examples\n        --------\n            &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n            &gt;&gt;&gt; custom_basis.get_parameter_basis('example_basis')\n        \"\"\"\n        if split_re_im:\n            parameter_basis = []\n            for parameter, parameter_type in self.parameters.items():\n                parameter_basis.append((parameter, 'R'))\n                if parameter_type == 'C':\n                    parameter_basis.append((parameter, 'I'))\n        else:\n            parameter_basis = self.parameters.keys()\n        return sorted(parameter_basis)\n</code></pre>"},{"location":"jelli/core/custom_basis/#jelli.core.custom_basis.CustomBasis.__init__","title":"<code>__init__(name, parameters)</code>","text":"<p>Initialize the CustomBasis class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the custom basis.</p> required <code>parameters</code> <code>list or dict</code> <p>The parameters in the custom basis. If a list is provided, all parameters are assumed to be real. If a dict is provided, the keys are the parameter names and the values are the types (e.g., <code>R</code> for real, <code>C</code> for complex).</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Initialize a custom basis with real parameters:</p> <pre><code>&gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n</code></pre> <p>Initialize a custom basis with mixed parameters:</p> <pre><code>&gt;&gt;&gt; custom_basis = CustomBasis('example_basis', {'param1': 'R', 'param2': 'C'})\n</code></pre> Source code in <code>jelli/core/custom_basis.py</code> <pre><code>def __init__(self, name: str, parameters: Union[List[str], Dict[str, str]]):\n    \"\"\"\n    Initialize the CustomBasis class.\n\n    Parameters\n    ----------\n    name : str\n        The name of the custom basis.\n    parameters : list or dict\n        The parameters in the custom basis. If a list is provided, all parameters are assumed to be real. If a dict is provided, the keys are the parameter names and the values are the types (e.g., `R` for real, `C` for complex).\n\n    Returns\n    --------\n    None\n\n    Examples\n    --------\n    Initialize a custom basis with real parameters:\n\n    &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n\n    Initialize a custom basis with mixed parameters:\n\n    &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', {'param1': 'R', 'param2': 'C'})\n    \"\"\"\n    self.name = name\n    if isinstance(parameters, list):\n        self.parameters = {param: 'R' for param in parameters}\n    elif isinstance(parameters, dict):\n        if not all(value in ['R', 'C'] for value in parameters.values()):\n            raise ValueError(\"All parameter types must be either 'R' or 'C'.\")\n        self.parameters = parameters\n    else:\n        raise ValueError(\"Parameters must be a list or a dictionary.\")\n    self._custom_bases[self.name] = self\n</code></pre>"},{"location":"jelli/core/custom_basis/#jelli.core.custom_basis.CustomBasis.get","title":"<code>get(name)</code>  <code>classmethod</code>","text":"<p>Get a custom basis by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the custom basis.</p> required <p>Returns:</p> Type Description <code>CustomBasis</code> <p>The custom basis object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; CustomBasis.get('example_basis')\n</code></pre> Source code in <code>jelli/core/custom_basis.py</code> <pre><code>@classmethod\ndef get(cls, name: str) -&gt; 'CustomBasis':\n    \"\"\"\n    Get a custom basis by name.\n\n    Parameters\n    ----------\n    name : str\n        The name of the custom basis.\n    Returns\n    --------\n    CustomBasis\n        The custom basis object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; CustomBasis.get('example_basis')\n    \"\"\"\n    return cls._custom_bases.get(name)\n</code></pre>"},{"location":"jelli/core/custom_basis/#jelli.core.custom_basis.CustomBasis.get_all","title":"<code>get_all()</code>  <code>classmethod</code>","text":"<p>Get all custom basis objects.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of all custom basis objects.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; CustomBasis.get_all()\n</code></pre> Source code in <code>jelli/core/custom_basis.py</code> <pre><code>@classmethod\ndef get_all(cls) -&gt; List['CustomBasis']:\n    \"\"\"\n    Get all custom basis objects.\n\n    Returns\n    --------\n    list\n        A list of all custom basis objects.\n\n    Examples\n    --------\n        &gt;&gt;&gt; CustomBasis.get_all()\n    \"\"\"\n    return list(cls._custom_bases.values())\n</code></pre>"},{"location":"jelli/core/custom_basis/#jelli.core.custom_basis.CustomBasis.get_all_names","title":"<code>get_all_names()</code>  <code>classmethod</code>","text":"<p>Get all custom basis names.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of all custom basis names.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; CustomBasis.get_all_names()\n</code></pre> Source code in <code>jelli/core/custom_basis.py</code> <pre><code>@classmethod\ndef get_all_names(cls) -&gt; List[str]:\n    \"\"\"\n    Get all custom basis names.\n\n    Returns\n    --------\n    list\n        A list of all custom basis names.\n\n    Examples\n    --------\n        &gt;&gt;&gt; CustomBasis.get_all_names()\n    \"\"\"\n    return sorted(cls._custom_bases.keys())\n</code></pre>"},{"location":"jelli/core/custom_basis/#jelli.core.custom_basis.CustomBasis.get_parameter_basis","title":"<code>get_parameter_basis(split_re_im=True)</code>","text":"<p>Get the parameter basis.</p> <p>Parameters:</p> Name Type Description Default <code>split_re_im</code> <code>bool</code> <p>If <code>True</code>, split parameters into real and imaginary parts, otherwise return the parameters directly. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>list</code> <p>A list containing the parameter basis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n&gt;&gt;&gt; custom_basis.get_parameter_basis('example_basis')\n</code></pre> Source code in <code>jelli/core/custom_basis.py</code> <pre><code>def get_parameter_basis(self, split_re_im=True) -&gt; List:\n    \"\"\"\n    Get the parameter basis.\n\n    Parameters\n    ----------\n    split_re_im : bool, optional\n        If `True`, split parameters into real and imaginary parts, otherwise return the parameters directly. Default is `True`.\n\n    Returns\n    --------\n    list\n        A list containing the parameter basis.\n\n    Examples\n    --------\n        &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n        &gt;&gt;&gt; custom_basis.get_parameter_basis('example_basis')\n    \"\"\"\n    if split_re_im:\n        parameter_basis = []\n        for parameter, parameter_type in self.parameters.items():\n            parameter_basis.append((parameter, 'R'))\n            if parameter_type == 'C':\n                parameter_basis.append((parameter, 'I'))\n    else:\n        parameter_basis = self.parameters.keys()\n    return sorted(parameter_basis)\n</code></pre>"},{"location":"jelli/core/experimental_correlations/","title":"jelli.core.experimental_correlations","text":""},{"location":"jelli/core/get_jitted_functions/","title":"jelli.core.get_jitted_functions","text":""},{"location":"jelli/core/global_likelihood/","title":"jelli.core.global_likelihood","text":""},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood","title":"<code>GlobalLikelihood</code>","text":"Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>class GlobalLikelihood():\n\n    def __init__(\n        self,\n        eft=None,\n        basis=None,\n        custom_basis=None,\n        include_observable_sectors=None,\n        exclude_observable_sectors=None,\n        include_measurements=None,\n        exclude_measurements=None,\n        custom_likelihoods=None,\n    ):\n\n        if custom_basis is not None:\n            if eft is not None or basis is not None:\n                raise ValueError(\"Please provide either `custom_basis`, or both `eft` and `basis`, but not both.\")\n        elif eft is not None and basis is None or basis is not None and eft is None:\n            raise ValueError(\"Please provide the `eft` when using the `basis` and vice versa.\")\n\n\n        # define attributes from arguments\n\n        self.eft = eft\n        self.basis = basis\n        self.custom_basis = custom_basis\n\n\n        # get names of all observable sectors and the basis mode, basis parameters, and reference scale\n\n        (\n            self.observable_sectors_gaussian,\n            self.observable_sectors_no_theory_uncertainty,\n            self.basis_mode\n        ) = self._get_observable_sectors(\n            include_observable_sectors,\n            exclude_observable_sectors\n        )\n        self.observable_sectors = self.observable_sectors_gaussian + self.observable_sectors_no_theory_uncertainty\n        self.parameter_basis_split_re_im, self.parameter_basis = self._get_parameter_basis()\n        self._reference_scale = self._get_reference_scale()\n\n        # get all measurements\n        observables_all = list(chain.from_iterable(\n            ObservableSector.get(observable_sector).observable_names\n            for observable_sector in self.observable_sectors\n        ))\n        self.include_measurements = Measurement.get_measurements(\n            observables=observables_all,\n            include_measurements=include_measurements,\n            exclude_measurements=exclude_measurements,\n        )\n        self.observables_constrained = set(chain.from_iterable(\n            measurement.constrained_observables\n            for measurement in self.include_measurements.values()\n        ))\n\n        # define attributes for observable sectors with no theory uncertainty\n\n        self.observables_no_theory_uncertainty = list(chain.from_iterable(\n            ObservableSector.get(observable_sector).observable_names\n            for observable_sector in self.observable_sectors_no_theory_uncertainty\n        ))\n        self.prediction_data_no_theory_uncertainty = [\n            ObservableSector.get(observable_sector).get_prediction_data(self.eft, self.basis)\n            for observable_sector in self.observable_sectors_no_theory_uncertainty\n        ]\n        self.prediction_function_no_theory_uncertainty = self._get_prediction_function_no_theory_uncertainty()\n\n\n        # define attributes for correlated observable sectors\n\n        (\n            self.observable_sectors_correlated,\n            self.cov_coeff_th_scaled,\n            self.cov_exp_scaled,\n            self.exp_central_scaled,\n            self.std_sm_exp,\n            self.std_exp,\n        ) = self._get_observable_sectors_correlated()\n\n        self.observables_correlated = [\n            list(chain.from_iterable(\n                ObservableSector.get(observable_sector).observable_names\n                for observable_sector in observable_sectors\n            ))\n            for observable_sectors in self.observable_sectors_correlated\n        ]\n        self.prediction_data_correlated = [\n            [\n                ObservableSector.get(observable_sector).get_prediction_data(self.eft, self.basis)\n                for observable_sector in observable_sectors\n            ]\n            for observable_sectors in self.observable_sectors_correlated\n        ]\n        self.prediction_function_correlated = [\n            self._get_prediction_function_gaussian(observable_sectors)\n            for observable_sectors in self.observable_sectors_correlated\n        ]\n\n        self.observables_gaussian = list(chain.from_iterable(\n            self.observables_correlated\n            ))\n\n        self.custom_likelihoods_gaussian, self.custom_likelihoods_no_theory_uncertainty = self._get_custom_likelihoods(custom_likelihoods)\n        self._observables_per_likelihood_no_theory_uncertainty, self._observables_per_likelihood_correlated = self._get_observables_per_likelihood()\n\n        _likelihoods_no_theory_uncertainty = sorted(self._observables_per_likelihood_no_theory_uncertainty.keys())\n        _likelihoods_correlated = sorted(self._observables_per_likelihood_correlated.keys())\n        _likelihoods_custom = sorted(set(self.custom_likelihoods_gaussian.keys()) | set(self.custom_likelihoods_no_theory_uncertainty.keys()))\n        _likelihoods = _likelihoods_correlated + _likelihoods_no_theory_uncertainty + _likelihoods_custom\n\n        self._observables_per_likelihood_no_theory_uncertainty.update(self.custom_likelihoods_no_theory_uncertainty)\n        self._observables_per_likelihood_correlated.update(self.custom_likelihoods_gaussian)\n        self._likelihood_indices_no_theory_uncertainty = jnp.array([\n            _likelihoods.index(likelihood)\n            for likelihood in list(self._observables_per_likelihood_no_theory_uncertainty.keys())\n        ], dtype=int)\n        self._likelihood_indices_correlated = jnp.array([\n            _likelihoods.index(likelihood)\n            for likelihood in list(self._observables_per_likelihood_correlated.keys())\n        ], dtype=int)\n\n        # add global likelihood\n        self._likelihood_indices_global = jnp.array([\n            i for i, likelihood in enumerate(_likelihoods)\n            if likelihood not in (\n                set(self.custom_likelihoods_gaussian) | set(self.custom_likelihoods_no_theory_uncertainty)\n            )\n        ], dtype=int)\n        self.likelihoods = _likelihoods + ['global']\n\n        (\n            self.constraints_no_theory_uncertainty,\n            self.constraints_no_theory_uncertainty_no_corr,\n            self.selector_matrix_no_th_unc_univariate,\n            self.selector_matrix_no_th_unc_multivariate,\n            self._indices_mvn_not_custom,\n        ) = self._get_constraints_no_theory_uncertainty(\n            self.observables_no_theory_uncertainty,\n            list(self._observables_per_likelihood_no_theory_uncertainty.values())\n        )\n\n        (\n            self.constraints_correlated_par_indep_cov,\n            self.constraints_correlated_par_dep_cov,\n            self.selector_matrix_correlated,\n        ) = self._get_constraints_correlated()\n\n        self._log_likelihood_point_function = self._get_log_likelihood_point_function()\n        self._log_likelihood_point = partial(\n            self._log_likelihood_point_function,\n            prediction_data_no_theory_uncertainty=self.prediction_data_no_theory_uncertainty,\n            prediction_data_correlated=self.prediction_data_correlated,\n            constraints_no_theory_uncertainty=self.constraints_no_theory_uncertainty,\n            constraints_correlated_par_indep_cov=self.constraints_correlated_par_indep_cov,\n            constraints_correlated_par_dep_cov=self.constraints_correlated_par_dep_cov,\n            selector_matrix_no_th_unc_univariate=self.selector_matrix_no_th_unc_univariate,\n            selector_matrix_no_th_unc_multivariate=self.selector_matrix_no_th_unc_multivariate,\n            selector_matrix_correlated=self.selector_matrix_correlated,\n            likelihood_indices_no_theory_uncertainty=self._likelihood_indices_no_theory_uncertainty,\n            likelihood_indices_correlated=self._likelihood_indices_correlated,\n            likelihood_indices_global=self._likelihood_indices_global,\n        )\n        (\n            sm_prediction_no_theory_uncertainty,\n            sm_prediction_correlated,\n            sm_log_likelihood_no_th_unc_univariate,\n            sm_log_likelihood_no_th_unc_multivariate,\n            sm_log_likelihood_correlated,\n            self.sm_log_likelihood_summed,\n            std_sm_exp_correlated_scaled,\n        ) = self._log_likelihood_point(\n            self._get_par_array({}),\n            self._reference_scale,\n            par_dep_cov=False,\n        )\n\n        self._obstable = partial(\n            self._get_obstable_function(),\n            constraints_no_theory_uncertainty_no_corr=self.constraints_no_theory_uncertainty_no_corr,\n            indices_mvn_not_custom=self._indices_mvn_not_custom,\n            exp_central_scaled=self.exp_central_scaled,\n            std_sm_exp=self.std_sm_exp,\n        )\n        (\n            sm_log_likelihood_no_th_unc_multivariate,\n            sm_log_likelihood_no_th_unc_multivariate_no_corr,\n            self.sm_log_likelihood_correlated,\n            self.sm_log_likelihood_correlated_no_corr,\n            _,\n            _,\n        ) = self._obstable(\n            sm_prediction_no_theory_uncertainty,\n            sm_prediction_correlated,\n            sm_log_likelihood_no_th_unc_multivariate,\n            sm_log_likelihood_correlated,\n            std_sm_exp_correlated_scaled,\n        )\n        self.sm_log_likelihood_no_theory_uncertainty = sm_log_likelihood_no_th_unc_univariate + sm_log_likelihood_no_th_unc_multivariate\n        self.sm_log_likelihood_no_theory_uncertainty_no_corr = sm_log_likelihood_no_th_unc_univariate + sm_log_likelihood_no_th_unc_multivariate_no_corr\n\n        combined_constraints = Measurement.get_combined_constraints(\n            self.observables_no_theory_uncertainty\n        )\n        experimental_values = {}\n        for dist_type, dist_info in combined_constraints.items():\n            observable_indices = dist_info['observable_indices']\n            mode, uncertainty = get_mode_and_uncertainty(dist_type, dist_info)\n            experimental_values.update({\n                self.observables_no_theory_uncertainty[ind]: [mode[i], uncertainty[i]]\n                for i, ind in enumerate(observable_indices)\n            })\n        self.experimental_values_no_theory_uncertainty = experimental_values\n\n        self._cache_jitted_functions = {}\n\n    @classmethod\n    def load(cls, path):\n        # load all observable sectors\n        ObservableSector.load(path)\n        # load all measurements\n        Measurement.load(path)\n        # load all theory correlations\n        TheoryCorrelations.load(path)\n        # load all experimental correlations\n        ExperimentalCorrelations.load()\n\n    def _get_observable_sectors(self, include_observable_sectors, exclude_observable_sectors):\n        if include_observable_sectors is not None and exclude_observable_sectors is not None:\n            raise ValueError(\"Please provide either `include_observable_sectors` or `exclude_observable_sectors`, not both.\")\n        available_observable_sectors = set(ObservableSector.get_all_names(eft=self.eft, basis=self.basis, custom_basis=self.custom_basis))\n        if include_observable_sectors is not None:\n            if set(include_observable_sectors)-available_observable_sectors:\n                raise ValueError(f\"Observable sectors {set(include_observable_sectors)-available_observable_sectors} provided in `include_observable_sectors` but not found in loaded observable sectors\")\n            observable_sectors = sorted(\n                include_observable_sectors\n            )\n        elif exclude_observable_sectors is not None:\n            if set(exclude_observable_sectors)-available_observable_sectors:\n                raise ValueError(f\"Observable sectors {set(exclude_observable_sectors)-available_observable_sectors} provided in `exclude_observable_sectors` but not found in loaded observable sectors\")\n            observable_sectors = sorted(\n                available_observable_sectors - set(exclude_observable_sectors)\n            )\n        else:\n            observable_sectors = sorted(available_observable_sectors)\n        if observable_sectors:\n            basis_mode = ObservableSector.get(observable_sectors[0]).basis_mode\n            if basis_mode in ['wcxf', 'custom']:\n                scales = set(\n                    ObservableSector.get(observable_sector).scale\n                    for observable_sector in observable_sectors\n                )\n                if len(scales) &gt; 1:\n                    raise ValueError(\n                        f\"Observable sectors for basis {self.custom_basis or (self.eft, self.basis)} are defined at different scales. Please use `include_observable_sectors` or `exclude_observable_sectors` to select observable sectors at the same scale.\"\n                    )\n        observable_sectors_gaussian = []\n        observable_sectors_no_theory_uncertainty = []\n        for observable_sector in observable_sectors:\n            if ObservableSector.get(observable_sector).observable_uncertainties is None:\n                observable_sectors_no_theory_uncertainty.append(observable_sector)\n            else:\n                observable_sectors_gaussian.append(observable_sector)\n        return observable_sectors_gaussian, observable_sectors_no_theory_uncertainty, basis_mode\n\n    def _get_observable_sectors_correlated(self):\n\n        # get correlations for all gaussian observable sectors\n\n        correlations_th =  []\n        correlations_exp =  []\n        for i, row_sector in enumerate(self.observable_sectors_gaussian):\n            row_th = []\n            row_exp = []\n            for j, col_sector in enumerate(self.observable_sectors_gaussian[:i+1]):\n                obs_row = ObservableSector.get(row_sector).observable_names\n                obs_col = ObservableSector.get(col_sector).observable_names\n                row_th.append(TheoryCorrelations.get_data(obs_row, obs_col))\n                row_exp.append(ExperimentalCorrelations.get_data('correlations', self.include_measurements, obs_row, obs_col))\n            correlations_th.append(row_th)\n            correlations_exp.append(row_exp)\n\n\n        # find connected components of the correlation graph\n\n        G = nx.Graph()\n        G.add_nodes_from(self.observable_sectors_gaussian)\n        for i, name_i in enumerate(self.observable_sectors_gaussian):\n            for j, name_j in enumerate(self.observable_sectors_gaussian[:i+1]):\n                if correlations_th[i][j] is not None or correlations_exp[i][j] is not None:\n                    G.add_edge(name_i, name_j)\n        components = list(nx.connected_components(G))\n        components = [sorted(list(group)) for group in components]\n        components = sorted(components, key=lambda c: self.observable_sectors_gaussian.index(c[0]))\n        observable_sectors_correlated = components\n\n\n        # get combined sm and exp standard deviations and scaled uncertainties for connected components\n\n        std_th_scaled = []\n        std_exp_scaled = []\n        std_sm_exp = []\n        exp_central_scaled = []\n        std_exp_list = []\n        for group in components:\n            sub_std_th_scaled = []\n            sub_std_exp_scaled = []\n            sub_std_sm_exp = []\n            sub_exp_central_scaled = []\n            sub_std_exp = []\n            for i, row_sector in enumerate(group):\n                obs_row = ObservableSector.get(row_sector).observable_names\n                std_exp = ExperimentalCorrelations.get_data('uncertainties', self.include_measurements, obs_row)\n                exp_central = ExperimentalCorrelations.get_data('central', self.include_measurements, obs_row)\n                std_th = ObservableSector.get(row_sector).observable_uncertainties\n                std_sm = ObservableSector.get(row_sector).observable_uncertainties_SM\n                _std_sm_exp = std_exp * np.sqrt(1 + (std_sm / std_exp)**2) # combined sm + exp uncertainty\n                sub_std_th_scaled.append(std_th/_std_sm_exp)\n                sub_std_exp_scaled.append(std_exp/_std_sm_exp)\n                sub_std_sm_exp.append(_std_sm_exp)\n                sub_exp_central_scaled.append(exp_central/_std_sm_exp)\n                sub_std_exp.append(std_exp)\n            std_th_scaled.append(sub_std_th_scaled)\n            std_exp_scaled.append(sub_std_exp_scaled)\n            std_sm_exp.append(jnp.array(np.concatenate(sub_std_sm_exp)))\n            exp_central_scaled.append(jnp.array(np.concatenate(sub_exp_central_scaled)))\n            std_exp_list.append(jnp.array(np.concatenate(sub_std_exp)))\n\n\n        # get scaled covariance matrices for connected components\n\n        cov_coeff_th_scaled = []\n        cov_exp_scaled = []\n        for k, group in enumerate(components):\n            sub_th = []\n            sub_exp = []\n            for i, row_sector in enumerate(group):\n                row_th = []\n                row_exp = []\n                for j, col_sector in enumerate(group[:i+1]):\n                    obs_row = ObservableSector.get(row_sector).observable_names\n                    obs_col = ObservableSector.get(col_sector).observable_names\n                    row_th.append(TheoryCorrelations.get_cov_scaled(\n                        self.include_measurements, obs_row, obs_col, std_th_scaled[k][i], std_th_scaled[k][j]\n                    ))\n                    row_exp.append(ExperimentalCorrelations.get_cov_scaled(\n                        self.include_measurements, obs_row, obs_col, std_exp_scaled[k][i], std_exp_scaled[k][j]\n                    ))\n                sub_th.append(row_th)\n                sub_exp.append(row_exp)\n            cov_coeff_th_scaled.append(sub_th)\n\n            n_sectors = len(sub_exp)\n            cov_exp = np.empty((n_sectors, n_sectors), dtype=object).tolist()\n            for i in range(n_sectors):\n                for j in range(n_sectors):\n                    if i &gt;= j:\n                        cov_exp[i][j] = sub_exp[i][j]\n                    else:\n                        shape = sub_exp[j][i].shape\n                        cov_exp[i][j] = np.zeros((shape[1], shape[0]))\n            cov_exp_tril = np.tril(np.block(cov_exp))\n            sub_exp = cov_exp_tril + cov_exp_tril.T - np.diag(np.diag(cov_exp_tril))\n            cov_exp_scaled.append(jnp.array(sub_exp))\n\n        return (\n            observable_sectors_correlated,\n            cov_coeff_th_scaled,\n            cov_exp_scaled,\n            exp_central_scaled,\n            std_sm_exp,\n            std_exp_list,\n        )\n\n    def _get_custom_likelihoods(self, custom_likelihoods):\n        if custom_likelihoods is None:\n            return {}, {}\n        if not isinstance(custom_likelihoods, dict) or not all([isinstance(k, str) and isinstance(v, list) for k, v in custom_likelihoods.items()]):\n            raise ValueError(\"The custom_likelihoods argument should be a dictionary with string names of custom likelihoods as keys and lists of observable names as values.\")\n\n        likelihoods_gaussian = {}\n        likelihoods_no_theory_uncertainty = {}\n\n        for name, observables in custom_likelihoods.items():\n            observables_gaussian = set()\n            observables_no_theory_uncertainty = set()\n            invalid_observables = set()\n            for observable in observables:\n                if observable in self.observables_gaussian:\n                    observables_gaussian.add(observable)\n                elif observable in self.observables_no_theory_uncertainty:\n                    observables_no_theory_uncertainty.add(observable)\n                else:\n                    invalid_observables.add(observable)\n            if invalid_observables:\n                raise ValueError(\n                    f\"Custom likelihood '{name}' contains observables not found in the loaded observable sectors: {sorted(invalid_observables)}\"\n                )\n            if observables_gaussian:\n                likelihoods_gaussian[f'custom_{name}'] = sorted(observables_gaussian)\n            if observables_no_theory_uncertainty:\n                likelihoods_no_theory_uncertainty[f'custom_{name}'] = sorted(observables_no_theory_uncertainty)\n\n        return likelihoods_gaussian, likelihoods_no_theory_uncertainty\n\n    def _get_observables_per_likelihood(self):\n\n        observables_per_likelihood_no_theory_uncertainty = {\n            observable_sector: ObservableSector.get(observable_sector).observable_names\n            for observable_sector in self.observable_sectors_no_theory_uncertainty\n        }\n\n        observables_per_likelihood_correlated = {\n            tuple(observable_sectors): self.observables_correlated[i]\n            for i, observable_sectors in enumerate(self.observable_sectors_correlated)\n            }\n\n        return observables_per_likelihood_no_theory_uncertainty, observables_per_likelihood_correlated\n\n    def _get_prediction_function_gaussian(self, observable_sectors_gaussian):\n\n        prediction_functions = [\n            ObservableSector.get(name).prediction\n            for name in observable_sectors_gaussian\n        ]\n\n        def prediction(\n            par_array: jnp.array, scale: Union[float, int, jnp.array],\n            prediction_data: List[List[jnp.array]]\n        ) -&gt; jnp.array:\n            polynomial_predictions = [jnp.empty(0)]\n            par_monomials = []\n            for prediction_function, data in zip(prediction_functions, prediction_data):\n                polynomial_prediction, par_monomial = prediction_function(\n                    par_array, scale, data\n                )\n                polynomial_predictions.append(polynomial_prediction)\n                par_monomials.append(par_monomial)\n            polynomial_predictions = jnp.concatenate(polynomial_predictions, axis=-1)\n            return polynomial_predictions, par_monomials\n\n        return prediction\n\n    def _get_prediction_function_no_theory_uncertainty(self):\n\n        prediction_functions = [\n            ObservableSector.get(name).prediction\n            for name in self.observable_sectors_no_theory_uncertainty\n        ]\n        def prediction(\n            par_array: jnp.array, scale: Union[float, int, jnp.array],\n            prediction_data: List[List[jnp.array]]\n        ) -&gt; jnp.array:\n            polynomial_predictions = [jnp.empty(0)]\n            for prediction_function, data in zip(prediction_functions, prediction_data):\n                polynomial_predictions.append(\n                    prediction_function(par_array, scale, data)[0]\n                )\n            polynomial_predictions = jnp.concatenate(polynomial_predictions, axis=-1)\n            return polynomial_predictions\n\n\n        return prediction\n\n    def _get_constraints_no_theory_uncertainty(self, observables, observable_lists_per_likelihood=None):\n\n        constraint_dict = {}\n\n        constraints = Measurement.get_constraints(\n            observables,\n            include_measurements=self.include_measurements,\n            distribution_types=[\n                'NumericalDistribution',\n                'NormalDistribution',\n                'HalfNormalDistribution',\n                'GammaDistributionPositive',\n                'MultivariateNormalDistribution',\n            ]\n        )\n\n        # numerical distribution\n        if 'NumericalDistribution' in constraints:\n            constraint_dict['NumericalDistribution'] = [\n                jnp.asarray(constraints['NumericalDistribution']['observable_indices']),\n                jnp.asarray(constraints['NumericalDistribution']['x']),\n                jnp.asarray(constraints['NumericalDistribution']['log_y']),\n            ]\n\n        # normal distribution\n        if 'NormalDistribution' in constraints:\n            constraint_dict['NormalDistribution'] = [\n                jnp.asarray(constraints['NormalDistribution']['observable_indices']),\n                jnp.asarray(constraints['NormalDistribution']['central_value']),\n                jnp.asarray(constraints['NormalDistribution']['standard_deviation']),\n            ]\n\n        # half normal distribution\n        if 'HalfNormalDistribution' in constraints:\n            constraint_dict['HalfNormalDistribution'] = [\n                jnp.asarray(constraints['HalfNormalDistribution']['observable_indices']),\n                jnp.asarray(constraints['HalfNormalDistribution']['standard_deviation']),\n            ]\n\n        # gamma distribution positive\n        if 'GammaDistributionPositive' in constraints:\n            constraint_dict['GammaDistributionPositive'] = [\n                jnp.asarray(constraints['GammaDistributionPositive']['observable_indices']),\n                jnp.asarray(constraints['GammaDistributionPositive']['a']),\n                jnp.asarray(constraints['GammaDistributionPositive']['loc']),\n                jnp.asarray(constraints['GammaDistributionPositive']['scale']),\n            ]\n\n        # MVN constraints, neglecting correlations\n        if 'MultivariateNormalDistribution' in constraints:\n            constraint_no_corr = [\n                jnp.asarray(np.concatenate(constraints['MultivariateNormalDistribution']['observable_indices'])),\n                jnp.asarray(np.concatenate(constraints['MultivariateNormalDistribution']['central_value'])),\n                jnp.asarray(np.concatenate(constraints['MultivariateNormalDistribution']['standard_deviation'])),\n            ]\n        else:\n            constraint_no_corr = None\n\n        if observable_lists_per_likelihood is not None:  # if not only correlated likelihoods\n            # selector matrix for univariate distributions\n            selector_matrix_univariate = jnp.array([\n                np.isin(observables, likelihood_observables).astype(float)\n                for likelihood_observables in observable_lists_per_likelihood\n            ])\n        else:\n            selector_matrix_univariate = jnp.zeros((0, len(observables)), dtype=float)\n\n        # multivariate normal distribution\n\n        _observable_lists_per_likelihood = observable_lists_per_likelihood or [observables]\n        # Collect all unique MVN blocks into this dict\n        unique_mvnd_blocks = {}\n\n        # For each likelihood, keep track of which MVNs it uses (by key)\n        mvnd_keys_per_likelihood = [[] for _ in _observable_lists_per_likelihood]\n\n        # Loop over all likelihood definitions\n        for i, observable_list in enumerate(_observable_lists_per_likelihood):\n\n            mvnd_block_data = Measurement.get_constraints(\n                observable_list,\n                include_measurements=self.include_measurements,\n                observables_for_indices=observables,\n                distribution_types=['MultivariateNormalDistribution'],\n            )['MultivariateNormalDistribution']\n\n            for j in range(len(mvnd_block_data['measurement_name'])):\n                mvnd_entry = {k: mvnd_block_data[k][j] for k in mvnd_block_data.keys()}\n                mvnd_key = (mvnd_entry['measurement_name'], tuple(mvnd_entry['observables']))\n                unique_mvnd_blocks[mvnd_key] = mvnd_entry\n                mvnd_keys_per_likelihood[i].append(mvnd_key)\n\n        # Final ordered list of all unique MVN blocks\n        all_mvnd_keys = list(unique_mvnd_blocks.keys())\n\n        n_likelihoods = len(mvnd_keys_per_likelihood)\n        n_contributions = len(all_mvnd_keys)\n\n        # Map MVND key to its index in all_mvnd_keys for fast lookup\n        mvnd_key_to_index = {key: i for i, key in enumerate(all_mvnd_keys)}\n\n        # Construct the logpdf input data from the unique MVNs\n        if all_mvnd_keys:\n            constraint_dict['MultivariateNormalDistribution'] = [\n                [jnp.asarray(unique_mvnd_blocks[k]['observable_indices']) for k in all_mvnd_keys],\n                [jnp.asarray(unique_mvnd_blocks[k]['central_value']) for k in all_mvnd_keys],\n                [jnp.asarray(unique_mvnd_blocks[k]['standard_deviation']) for k in all_mvnd_keys],\n                [jnp.asarray(unique_mvnd_blocks[k]['inverse_correlation']) for k in all_mvnd_keys],\n            ]\n            # Create selector matrix (n_likelihoods x n_contributions)\n            selector_matrix_multivariate = np.zeros((n_likelihoods, n_contributions))\n            for i, mvnd_keys in enumerate(mvnd_keys_per_likelihood):\n                for key in mvnd_keys:\n                    selector_matrix_multivariate[i, mvnd_key_to_index[key]] = 1.0\n            selector_matrix_multivariate = jnp.array(selector_matrix_multivariate)\n        else:\n            selector_matrix_multivariate = jnp.zeros((n_likelihoods, 1), dtype=float)\n\n        # Get indices of MVNs that contribute to non-custom likelihoods\n        n_likelihoods_not_custom = len(self.observable_sectors_no_theory_uncertainty)\n        indices_mvn_not_custom = jnp.nonzero(\n            np.sum(\n                selector_matrix_multivariate[:n_likelihoods_not_custom],\n                axis=0\n            )\n        )[0]\n\n        return (\n            constraint_dict,\n            constraint_no_corr,\n            selector_matrix_univariate,\n            selector_matrix_multivariate,\n            indices_mvn_not_custom,\n        )\n\n    def _get_constraints_correlated(self):\n\n        # constraints for correlated observable sectors with parameter dependent covariance matrix\n\n        n_correlated_likelihoods = len(self._observables_per_likelihood_correlated)\n        unique_indices_list = []\n        selector_matrix = []\n        for i, observables_correlated in enumerate(self.observables_correlated):\n            unique_observable_indices = []\n            mvn_to_likelihood_map = defaultdict(list)  # maps indices of observables in the set of correlated sectors (MVNs) to likelihoods\n            for j, observables_in_likelihood in enumerate(self._observables_per_likelihood_correlated.values()):\n                if (\n                    j == i  # this is the set of correlated sectors selected in the i loop\n                    or j &gt;= len(self.observables_correlated)  # these are the custom likelihoods\n                ):\n                    obs_indices = tuple(\n                        observables_correlated.index(observable)\n                        for observable in observables_in_likelihood\n                        if (\n                            observable in observables_correlated  # a custom likelihood might contain no observable from this set of correlated sectors\n                            and observable in self.observables_constrained  # only consider observables that are constrained\n                        )\n                    )\n                    if obs_indices:\n                        if obs_indices not in unique_observable_indices:\n                            unique_observable_indices.append(\n                                obs_indices\n                            )\n                        mvn_to_likelihood_map[obs_indices].append(j)\n\n            # build selector matrix of (n_correlated_likelihoods, n_mvns)\n            sel_matrix = np.zeros((n_correlated_likelihoods, len(unique_observable_indices)))\n            for col, indices in enumerate(unique_observable_indices):\n                rows = mvn_to_likelihood_map.get(indices, [])\n                sel_matrix[rows, col] = 1  # set the entry to 1 if the likelihood depends on this MVN based on the mvn_to_likelihood_map\n\n            unique_indices_list.append([jnp.array(indices, dtype=int) for indices in unique_observable_indices])\n            selector_matrix.append(sel_matrix)\n\n        constraints_correlated_par_dep_cov = [\n            self.cov_coeff_th_scaled,\n            self.std_sm_exp,\n            unique_indices_list,\n            self.exp_central_scaled,\n            self.cov_exp_scaled,\n        ]\n\n        # constraints for correlated observable sectors with parameter independent covariance matrix\n\n        mean = []\n        standard_deviation = []\n        inverse_correlation = []\n        for i, unique_indices in enumerate(unique_indices_list):\n            mean.append([])\n            standard_deviation.append([])\n            inverse_correlation.append([])\n            cov_exp_scaled = self.cov_exp_scaled[i]\n            cov_coeff_th_scaled = self.cov_coeff_th_scaled[i]\n            par_monomials = []\n            for name in self.observable_sectors_correlated[i]:\n                sector = ObservableSector.get(name)\n                par_monomial = np.zeros(len(sector.keys_coeff_observable))\n                par_monomial[0] = 1.0\n                par_monomials.append(par_monomial)\n            cov_obs_th_scaled = cov_coeff_to_cov_obs(par_monomials, cov_coeff_th_scaled)\n            corr = cov_obs_th_scaled + cov_exp_scaled  # actually correlation matrix as it is parameter independent and rescaled with its own diagonal\n            std_sm_exp = self.std_sm_exp[i]\n            for index_array in unique_indices:\n                index_list = list(index_array)\n                mean[i].append(\n                    jnp.asarray(\n                        np.take(\n                            self.exp_central_scaled[i]*std_sm_exp,\n                            index_list\n                        ),\n                        dtype=jnp.float64\n                    )\n                )\n                std = np.take(\n                    std_sm_exp,\n                    index_list\n                )\n                standard_deviation[i].append(\n                    jnp.asarray(\n                        std,\n                        dtype=jnp.float64\n                    )\n                )\n                c = np.take(\n                    np.take(corr, index_list, axis=0),\n                    index_list,\n                    axis=1\n                )\n                inverse_correlation[i].append(\n                    jnp.asarray(\n                        np.linalg.inv(c),\n                        dtype=jnp.float64\n                    )\n                )\n\n        constraints_correlated_par_indep_cov = [\n            unique_indices_list,\n            mean,\n            standard_deviation,\n            inverse_correlation,\n        ]\n\n        return constraints_correlated_par_indep_cov, constraints_correlated_par_dep_cov, selector_matrix\n\n    def get_negative_log_likelihood(\n            self,\n            par_list: List[Tuple[str, str]],\n            likelihood: Union[str, Tuple[str, ...]],\n            par_dep_cov: bool,\n        ):\n\n        # prepare selector matrices for included likelihoods\n        if likelihood == 'global':  # for global likelihood, select all non-custom likelihoods\n            selector_matrix_no_th_unc_univariate  = self.selector_matrix_no_th_unc_univariate[:len(self.observable_sectors_no_theory_uncertainty)]\n            selector_matrix_no_th_unc_multivariate = self.selector_matrix_no_th_unc_multivariate[:len(self.observable_sectors_no_theory_uncertainty)]\n            selector_matrix_correlated = [selector_matrix[:len(self.observable_sectors_correlated)] for selector_matrix in self.selector_matrix_correlated]\n        else:  # for a specific likelihood, select just the corresponding rows in selector matrices\n            if likelihood in self._observables_per_likelihood_no_theory_uncertainty:\n                n = list(self._observables_per_likelihood_no_theory_uncertainty).index(likelihood)\n                selector_matrix_no_th_unc_univariate = self.selector_matrix_no_th_unc_univariate[[n], :]\n                selector_matrix_no_th_unc_multivariate = self.selector_matrix_no_th_unc_multivariate[[n], :]\n            else:\n                selector_matrix_no_th_unc_univariate = None\n                selector_matrix_no_th_unc_multivariate = None\n            if likelihood in self._observables_per_likelihood_correlated:\n                n = list(self._observables_per_likelihood_correlated).index(likelihood)\n                selector_matrix_correlated = [selector_matrix[[n], :] for selector_matrix in self.selector_matrix_correlated]\n            else:\n                selector_matrix_correlated = [None for _ in self.selector_matrix_correlated]\n\n        log_likelihood_data = [\n            self.prediction_data_no_theory_uncertainty,\n            self.prediction_data_correlated,\n            self.constraints_no_theory_uncertainty,\n            self.constraints_correlated_par_indep_cov,\n            self.constraints_correlated_par_dep_cov,\n            selector_matrix_no_th_unc_univariate,\n            selector_matrix_no_th_unc_multivariate,\n            selector_matrix_correlated,\n        ]\n\n        n_parameters = len(self.parameter_basis_split_re_im)\n        par_indices = jnp.array([self.parameter_basis_split_re_im[par] for par in par_list])\n\n        def negative_log_likelihood(\n            par_array: jnp.array,\n            scale: Union[float, int, jnp.array],\n            log_likelihood_data: List,\n        ) -&gt; float:\n\n            (\n                prediction_data_no_theory_uncertainty,\n                prediction_data_correlated,\n                constraints_no_theory_uncertainty,\n                constraints_correlated_par_indep_cov,\n                constraints_correlated_par_dep_cov,\n                selector_matrix_no_th_unc_univariate,\n                selector_matrix_no_th_unc_multivariate,\n                selector_matrix_correlated,\n            ) = log_likelihood_data\n\n            par_array_full = jnp.zeros(n_parameters)\n            par_array_full = par_array_full.at[par_indices].set(par_array)\n\n            # no theory uncertainty likelihoods\n            log_likelihood_no_th_unc_summed = 0.0\n            if selector_matrix_no_th_unc_univariate is not None:\n                prediction_no_theory_uncertainty = self.prediction_function_no_theory_uncertainty(\n                    par_array_full, scale, prediction_data_no_theory_uncertainty\n                )\n                for distribution_type in constraints_no_theory_uncertainty.keys():\n                    if distribution_type == 'MultivariateNormalDistribution':\n                        selector_matrix = selector_matrix_no_th_unc_multivariate\n                    else:\n                        selector_matrix = selector_matrix_no_th_unc_univariate\n                    log_likelihood_no_th_unc_summed += jnp.sum(\n                        logL_functions_summed[distribution_type](\n                            prediction_no_theory_uncertainty,\n                            selector_matrix,\n                            *constraints_no_theory_uncertainty[distribution_type]\n                        )\n                    )\n\n            # correlated likelihoods\n            prediction_correlated = [\n                prediction_function(\n                    par_array_full, scale, prediction_data_correlated[i]\n                ) for i, prediction_function in enumerate(self.prediction_function_correlated)  # includes predictions and par_monomials\n            ]\n            n_correlated_sectors = len(selector_matrix_correlated)\n            log_likelihood_correlated_summed = 0.0\n            if par_dep_cov:\n                (cov_coeff_th_scaled,\n                 std_sm_exp,\n                 observable_indices,\n                 exp_central_scaled,\n                 cov_exp_scaled,\n                ) = constraints_correlated_par_dep_cov\n                for i in range(n_correlated_sectors):\n                    selector_matrix = selector_matrix_correlated[i]\n                    if selector_matrix is not None:\n                        predictions, par_monomials = prediction_correlated[i]\n                        cov_obs_th_scaled = cov_coeff_to_cov_obs(par_monomials, cov_coeff_th_scaled[i])\n                        log_likelihood_correlated_summed += jnp.sum(\n                            logL_correlated_sectors_summed(\n                                predictions/std_sm_exp[i],\n                                selector_matrix,\n                                observable_indices[i],\n                                exp_central_scaled[i],\n                                cov_obs_th_scaled,\n                                cov_exp_scaled[i]\n                            )\n                        )\n            else:\n                (\n                 observable_indices,\n                 mean,\n                 standard_deviation,\n                 inverse_correlation,\n                ) = constraints_correlated_par_indep_cov\n                logL_function = logL_functions_summed['MultivariateNormalDistribution']\n                for i in range(n_correlated_sectors):\n                    selector_matrix = selector_matrix_correlated[i]\n                    if selector_matrix is not None:\n                        predictions, _ = prediction_correlated[i]\n                        log_likelihood_correlated_summed += jnp.sum(\n                            logL_function(\n                                predictions,\n                                selector_matrix,\n                                observable_indices[i],\n                                mean[i],\n                                standard_deviation[i],\n                                inverse_correlation[i],\n                            )\n                        )\n            return - (log_likelihood_no_th_unc_summed + log_likelihood_correlated_summed)\n\n        return negative_log_likelihood, log_likelihood_data\n\n    def _get_log_likelihood_point_function(self):\n\n        n_likelihoods = len(self.likelihoods)\n\n        def log_likelihood_point(\n            par_array: jnp.array,\n            scale: Union[float, int, jnp.array],\n            par_dep_cov: bool,\n            prediction_data_no_theory_uncertainty: jnp.array,\n            prediction_data_correlated: jnp.array,\n            constraints_no_theory_uncertainty: Dict[str,Union[List[jnp.array],List[List[jnp.array]]]],\n            constraints_correlated_par_indep_cov: Union[List[jnp.array],List[List[jnp.array]]],\n            constraints_correlated_par_dep_cov: Union[List[jnp.array],List[List[jnp.array]]],\n            selector_matrix_no_th_unc_univariate: jnp.array,\n            selector_matrix_no_th_unc_multivariate: jnp.array,\n            selector_matrix_correlated: List[jnp.array],\n            likelihood_indices_no_theory_uncertainty: jnp.array,\n            likelihood_indices_correlated: jnp.array,\n            likelihood_indices_global: jnp.array,\n        ) -&gt; Tuple[jnp.array]:\n\n            # no theory uncertainty likelihoods and predictions\n            prediction_no_theory_uncertainty = self.prediction_function_no_theory_uncertainty(\n                par_array, scale, prediction_data_no_theory_uncertainty\n            )\n            log_likelihood_no_th_unc_univariate = jnp.zeros(len(prediction_no_theory_uncertainty))\n            log_likelihood_no_th_unc_multivariate = jnp.zeros((1, len(prediction_no_theory_uncertainty)))\n            for distribution_type in constraints_no_theory_uncertainty.keys():\n                if distribution_type == 'MultivariateNormalDistribution':\n                    log_likelihood_no_th_unc_multivariate = logL_functions[distribution_type](\n                        prediction_no_theory_uncertainty,\n                        *constraints_no_theory_uncertainty[distribution_type]\n                    )\n                else:\n                    log_likelihood_no_th_unc_univariate += logL_functions[distribution_type](\n                        prediction_no_theory_uncertainty,\n                        *constraints_no_theory_uncertainty[distribution_type]\n                    )\n\n            log_likelihood_no_theory_uncertainty_summed = (\n                selector_matrix_no_th_unc_univariate @ log_likelihood_no_th_unc_univariate\n                + selector_matrix_no_th_unc_multivariate @ jnp.sum(log_likelihood_no_th_unc_multivariate, axis=1)\n            )\n\n            # correlated likelihoods and predictions\n            prediction_correlated = [\n                prediction_function(\n                    par_array, scale, prediction_data_correlated[i]\n                ) for i, prediction_function in enumerate(self.prediction_function_correlated)  # includes predictions and par_monomials\n            ]\n            n_correlated_sectors = len(prediction_correlated)\n            log_likelihood_correlated = []\n            std_th_exp_correlated_scaled = []\n            if par_dep_cov:\n                (cov_coeff_th_scaled,\n                 std_sm_exp,\n                 observable_indices,\n                 exp_central_scaled,\n                 cov_exp_scaled,\n                ) = constraints_correlated_par_dep_cov\n                for i in range(n_correlated_sectors):\n                    predictions, par_monomials = prediction_correlated[i]\n                    cov_obs_th_scaled = cov_coeff_to_cov_obs(par_monomials, cov_coeff_th_scaled[i])\n                    std_th_exp_correlated_scaled.append(jnp.sqrt(jnp.diag(cov_obs_th_scaled) + jnp.diag(cov_exp_scaled[i])))\n                    log_likelihood_correlated.append(\n                        logL_correlated_sectors(\n                            predictions/std_sm_exp[i],\n                            observable_indices[i],\n                            exp_central_scaled[i],\n                            cov_obs_th_scaled,\n                            cov_exp_scaled[i]\n                        )\n                    )\n            else:\n                (\n                 observable_indices,\n                 mean,\n                 standard_deviation,\n                 inverse_correlation,\n                ) = constraints_correlated_par_indep_cov\n                logL_function = logL_functions['MultivariateNormalDistribution']\n                for i in range(n_correlated_sectors):\n                    predictions, _ = prediction_correlated[i]\n                    std_th_exp_correlated_scaled.append(jnp.ones_like(predictions))\n                    log_likelihood_correlated.append(\n                        logL_function(\n                            predictions,\n                            observable_indices[i],\n                            mean[i],\n                            standard_deviation[i],\n                            inverse_correlation[i],\n                        )\n                    )\n\n            n_correlated_likelihoods = len(likelihood_indices_correlated)\n            log_likelihood_correlated_summed = jnp.zeros(n_correlated_likelihoods)\n            for i in range(n_correlated_sectors):\n                logL = jnp.sum(log_likelihood_correlated[i], axis=1)\n                logL = jnp.where(jnp.isnan(logL), len(log_likelihood_correlated[i])*LOG_ZERO, logL)\n                log_likelihood_correlated_summed += selector_matrix_correlated[i] @ logL\n\n            log_likelihood_summed = jnp.zeros(n_likelihoods)\n            log_likelihood_summed = log_likelihood_summed.at[likelihood_indices_no_theory_uncertainty].add(log_likelihood_no_theory_uncertainty_summed)\n            log_likelihood_summed = log_likelihood_summed.at[likelihood_indices_correlated].add(log_likelihood_correlated_summed)\n            log_likelihood_global = jnp.sum(log_likelihood_summed[likelihood_indices_global])\n            log_likelihood_summed = log_likelihood_summed.at[-1].set(log_likelihood_global)\n            return (\n                prediction_no_theory_uncertainty,\n                prediction_correlated,\n                log_likelihood_no_th_unc_univariate,\n                log_likelihood_no_th_unc_multivariate,\n                log_likelihood_correlated,\n                log_likelihood_summed,\n                std_th_exp_correlated_scaled,\n            )\n        return jit(log_likelihood_point, static_argnames=[\"par_dep_cov\"])\n\n    def _get_obstable_function(self):\n\n        @jit\n        def obstable(\n            prediction_no_theory_uncertainty: jnp.array,\n            prediction_correlated: List[jnp.array],\n            log_likelihood_no_th_unc_multivariate: jnp.array,\n            log_likelihood_correlated: List[jnp.array],\n            std_th_exp_correlated_scaled: List[jnp.array],\n            constraints_no_theory_uncertainty_no_corr: List[jnp.array],\n            indices_mvn_not_custom: jnp.array,\n            exp_central_scaled: List[jnp.array],\n            std_sm_exp: List[jnp.array],\n        ) -&gt; Tuple[jnp.array]:\n\n            # no theory uncertainty sectors\n            # including correlations\n            log_likelihood_no_th_unc_multivariate = jnp.sum(\n                jnp.take(\n                    log_likelihood_no_th_unc_multivariate,\n                    indices_mvn_not_custom,\n                    axis=0\n                ),\n                axis=0\n            )\n\n            # neglecting correlations\n            if constraints_no_theory_uncertainty_no_corr is not None:\n                log_likelihood_no_th_unc_multivariate_no_corr = logL_functions['NormalDistribution'](\n                    prediction_no_theory_uncertainty,\n                    *constraints_no_theory_uncertainty_no_corr,\n                )\n            else:\n                log_likelihood_no_th_unc_multivariate_no_corr = jnp.zeros(len(prediction_no_theory_uncertainty))\n\n            # correlated sectors\n            # including correlations\n            log_likelihood_correlated = [log_likelihood[0] for log_likelihood in log_likelihood_correlated]\n\n            # neglecting correlations\n            log_likelihood_correlated_no_corr = []\n            exp_central_correlated = []\n            std_th_exp_correlated = []\n            n_correlated_sectors = len(prediction_correlated)\n            for i in range(n_correlated_sectors):\n                std_th_exp = std_th_exp_correlated_scaled[i] * std_sm_exp[i]\n                exp_central = exp_central_scaled[i] * std_sm_exp[i]\n                observable_indices = jnp.arange(len(prediction_correlated[i][0]))\n                log_likelihood_correlated_no_corr.append(\n                    logL_functions['NormalDistribution'](\n                        prediction_correlated[i][0],\n                        observable_indices,\n                        exp_central,\n                        std_th_exp\n                    )\n                )\n                exp_central_correlated.append(exp_central)\n                std_th_exp_correlated.append(std_th_exp)\n\n            return (\n                log_likelihood_no_th_unc_multivariate,\n                log_likelihood_no_th_unc_multivariate_no_corr,\n                log_likelihood_correlated,\n                log_likelihood_correlated_no_corr,\n                exp_central_correlated,\n                std_th_exp_correlated,\n            )\n        return obstable\n\n    def _get_parameter_basis(self):\n        if self.basis_mode == 'rgevolve':\n            parameter_basis_split_re_im = get_wc_basis(eft=self.eft, basis=self.basis, sector=None, split_re_im=True)\n            parameter_basis = get_wc_basis(eft=self.eft, basis=self.basis, sector=None, split_re_im=False)\n        elif self.basis_mode == 'wcxf':\n            parameter_basis_split_re_im = get_wc_basis_from_wcxf(eft=self.eft, basis=self.basis, sector=None, split_re_im=True)\n            parameter_basis = get_wc_basis_from_wcxf(eft=self.eft, basis=self.basis, sector=None, split_re_im=False)\n        else:\n            custom_basis = CustomBasis.get(\n                ObservableSector.get(self.observable_sectors[0]).custom_basis\n            )\n            parameter_basis_split_re_im = custom_basis.get_parameter_basis(split_re_im=True)\n            parameter_basis = custom_basis.get_parameter_basis(split_re_im=False)\n        parameter_basis_split_re_im = {par: i for i, par in enumerate(parameter_basis_split_re_im)}\n        parameter_basis = {par: i for i, par in enumerate(parameter_basis)}\n        return parameter_basis_split_re_im, parameter_basis\n\n    def _get_par_array(self, par_dict):\n        if not par_dict:\n            return jnp.zeros(len(self.parameter_basis_split_re_im))\n        elif isinstance(list(par_dict.keys())[0], tuple):\n            par_array = np.zeros(len(self.parameter_basis_split_re_im))\n            for name, value in par_dict.items():\n                if name not in self.parameter_basis_split_re_im:\n                    raise ValueError(f\"Parameter {name} not found in the parameter basis.\")\n                par_array[self.parameter_basis_split_re_im[name]] = value\n            return jnp.array(par_array)\n        else:\n            par_array = np.zeros(len(self.parameter_basis_split_re_im))\n            for name, value in par_dict.items():\n                if (name,'R') not in self.parameter_basis_split_re_im:\n                    raise ValueError(f\"Parameter {name} not found in the parameter basis.\")\n                par_array[self.parameter_basis_split_re_im[(name, 'R')]] = value.real\n                if (name, 'I') in self.parameter_basis_split_re_im:\n                    par_array[self.parameter_basis_split_re_im[(name, 'I')]] = value.imag\n            return jnp.array(par_array)\n\n    def parameter_point(self, *args, par_dep_cov: bool = False):\n        \"\"\"\n        Create a GlobalLikelihoodPoint instance.\n\n        Parameters\n        ----------\n        *args : tuple\n            Positional arguments. The method dispatches\n            based on the number and types of these arguments. Accepted input signatures:\n\n              1. `parameter_point(par_dict: dict, scale: Union[float, int], *, par_dep_cov: bool = False)`\n                - Create a GlobalLikelihoodPoint from a dictionary of parameters and a scale.\n\n              2. `parameter_point(w: wilson.Wilson, *, par_dep_cov: bool = False)`\n                - Create a GlobalLikelihoodPoint from a `wilson.Wilson` object.\n\n              3. `parameter_point(wc: wilson.wcxf.WC, *, par_dep_cov: bool = False)`\n                - Create a GlobalLikelihoodPoint from a `wilson.wcxf.WC` object.\n\n              4. `parameter_point(filename: str, *, par_dep_cov: bool = False)`\n                - Create a GlobalLikelihoodPoint from the path to a WCxf file.\n\n        par_dep_cov : bool, optional\n            If `True`, use the parameter dependent covariance matrix for the likelihood point.\n            Default is `False`.\n\n        Returns\n        -------\n        GlobalLikelihoodPoint\n            An instance of GlobalLikelihoodPoint with the specified parameters.\n        \"\"\"\n\n        if len(args) == 2:\n            par_dict, scale = args\n            if not isinstance(par_dict, dict) or not isinstance(scale, (float, int)):\n                raise ValueError(\n                    \"Invalid types of the two positional arguments. Expected a dictionary and scale.\"\n                )\n        elif len(args) == 1:\n            arg = args[0]\n            if isinstance(arg, Wilson):\n                par_dict = arg.wc.dict\n                scale = arg.wc.scale\n            elif isinstance(arg, wcxf.WC):\n                par_dict = arg.dict\n                scale = arg.scale\n            elif isinstance(arg, str):\n                with open(arg, 'r') as f:\n                    wc = wcxf.WC.load(f)\n                par_dict = wc.dict\n                scale = wc.scale\n            else:\n                raise ValueError(\n                    \"Invalid type of the positional argument. Expected a Wilson or wcxf.WC object, or a filename.\"\n                )\n        else:\n            raise ValueError(\"Invalid number of positional arguments. Expected either two (a dictionary and scale) or one (a Wilson or wcxf.WC object, or a filename).\")\n        return GlobalLikelihoodPoint(self, self._get_par_array(par_dict), scale, par_dep_cov=par_dep_cov)\n\n    def get_jitted(\n        self,\n        par_list: List[Tuple[str, str]],\n        likelihood: Union[str, Tuple[str, ...]],\n        par_dep_cov: bool = False,\n    ):\n\n        if (tuple(par_list), likelihood, par_dep_cov) not in self._cache_jitted_functions:\n            jitted_functions = GetJittedFunctions(\n                self,\n                par_list,\n                likelihood,\n                par_dep_cov,\n            )\n            self._cache_jitted_functions[(tuple(par_list), likelihood, par_dep_cov)] = jitted_functions\n        return self._cache_jitted_functions[(tuple(par_list), likelihood, par_dep_cov)]\n\n    def _get_reference_scale(self):\n        if self.basis_mode == 'rgevolve':\n            return float(reference_scale[self.eft])\n        else:\n            return ObservableSector.get(self.observable_sectors[0]).scale\n\n    def plot_data_2d(self, par_fct, scale, x_min, x_max, y_min, y_max, x_log=False, y_log=False, steps=20, par_dep_cov=False):\n        if x_log:\n            _x = jnp.logspace(x_min, x_max, steps)\n        else:\n            _x = jnp.linspace(x_min, x_max, steps)\n        if y_log:\n            _y = jnp.logspace(y_min, y_max, steps)\n        else:\n            _y = jnp.linspace(y_min, y_max, steps)\n        x, y = jnp.meshgrid(_x, _y)\n        xy = jnp.array([x, y]).reshape(2, steps**2).T\n        xy_enumerated = list(enumerate(xy))\n        if isinstance(scale, Number):\n            scale_fct = partial(_scale_fct_fixed, scale=scale)\n        else:\n            scale_fct = scale\n        ll = partial(_log_likelihood_2d, gl=self, par_fct=par_fct, scale_fct=scale_fct, par_dep_cov=par_dep_cov)\n        ll_dict_list_enumerated = map(ll, xy_enumerated)  # no multiprocessing for now\n        ll_dict_list = [\n            ll_dict[1] for ll_dict in\n            sorted(ll_dict_list_enumerated, key=itemgetter(0))\n        ]\n        plotdata = {}\n        keys = ll_dict_list[0].keys()  # look at first dict to fix keys\n        for k in keys:\n            z = -2 * np.array([ll_dict[k] for ll_dict in ll_dict_list]).reshape((steps, steps))\n            plotdata[k] = {'x': x, 'y': y, 'z': z}\n        return plotdata\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood.parameter_point","title":"<code>parameter_point(*args, par_dep_cov=False)</code>","text":"<p>Create a GlobalLikelihoodPoint instance.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>Positional arguments. The method dispatches based on the number and types of these arguments. Accepted input signatures:</p> <ol> <li> <p><code>parameter_point(par_dict: dict, scale: Union[float, int], *, par_dep_cov: bool = False)</code></p> <ul> <li>Create a GlobalLikelihoodPoint from a dictionary of parameters and a scale.</li> </ul> </li> <li> <p><code>parameter_point(w: wilson.Wilson, *, par_dep_cov: bool = False)</code></p> <ul> <li>Create a GlobalLikelihoodPoint from a <code>wilson.Wilson</code> object.</li> </ul> </li> <li> <p><code>parameter_point(wc: wilson.wcxf.WC, *, par_dep_cov: bool = False)</code></p> <ul> <li>Create a GlobalLikelihoodPoint from a <code>wilson.wcxf.WC</code> object.</li> </ul> </li> <li> <p><code>parameter_point(filename: str, *, par_dep_cov: bool = False)</code></p> <ul> <li>Create a GlobalLikelihoodPoint from the path to a WCxf file.</li> </ul> </li> </ol> <code>()</code> <code>par_dep_cov</code> <code>bool</code> <p>If <code>True</code>, use the parameter dependent covariance matrix for the likelihood point. Default is <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>GlobalLikelihoodPoint</code> <p>An instance of GlobalLikelihoodPoint with the specified parameters.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def parameter_point(self, *args, par_dep_cov: bool = False):\n    \"\"\"\n    Create a GlobalLikelihoodPoint instance.\n\n    Parameters\n    ----------\n    *args : tuple\n        Positional arguments. The method dispatches\n        based on the number and types of these arguments. Accepted input signatures:\n\n          1. `parameter_point(par_dict: dict, scale: Union[float, int], *, par_dep_cov: bool = False)`\n            - Create a GlobalLikelihoodPoint from a dictionary of parameters and a scale.\n\n          2. `parameter_point(w: wilson.Wilson, *, par_dep_cov: bool = False)`\n            - Create a GlobalLikelihoodPoint from a `wilson.Wilson` object.\n\n          3. `parameter_point(wc: wilson.wcxf.WC, *, par_dep_cov: bool = False)`\n            - Create a GlobalLikelihoodPoint from a `wilson.wcxf.WC` object.\n\n          4. `parameter_point(filename: str, *, par_dep_cov: bool = False)`\n            - Create a GlobalLikelihoodPoint from the path to a WCxf file.\n\n    par_dep_cov : bool, optional\n        If `True`, use the parameter dependent covariance matrix for the likelihood point.\n        Default is `False`.\n\n    Returns\n    -------\n    GlobalLikelihoodPoint\n        An instance of GlobalLikelihoodPoint with the specified parameters.\n    \"\"\"\n\n    if len(args) == 2:\n        par_dict, scale = args\n        if not isinstance(par_dict, dict) or not isinstance(scale, (float, int)):\n            raise ValueError(\n                \"Invalid types of the two positional arguments. Expected a dictionary and scale.\"\n            )\n    elif len(args) == 1:\n        arg = args[0]\n        if isinstance(arg, Wilson):\n            par_dict = arg.wc.dict\n            scale = arg.wc.scale\n        elif isinstance(arg, wcxf.WC):\n            par_dict = arg.dict\n            scale = arg.scale\n        elif isinstance(arg, str):\n            with open(arg, 'r') as f:\n                wc = wcxf.WC.load(f)\n            par_dict = wc.dict\n            scale = wc.scale\n        else:\n            raise ValueError(\n                \"Invalid type of the positional argument. Expected a Wilson or wcxf.WC object, or a filename.\"\n            )\n    else:\n        raise ValueError(\"Invalid number of positional arguments. Expected either two (a dictionary and scale) or one (a Wilson or wcxf.WC object, or a filename).\")\n    return GlobalLikelihoodPoint(self, self._get_par_array(par_dict), scale, par_dep_cov=par_dep_cov)\n</code></pre>"},{"location":"jelli/core/global_likelihood_point/","title":"jelli.core.global_likelihood_point","text":""},{"location":"jelli/core/measurement/","title":"jelli.core.measurement","text":""},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement","title":"<code>Measurement</code>","text":"<p>Class to store measurements and constraints on observables.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the measurement.</p> required <code>constraints</code> <code>list of dict</code> <p>List of constraints on observables. Each constraint is a dictionary with the following keys:</p> <ul> <li> <p><code>type : str</code>     Type of the distribution. Can be <code>NormalDistribution</code>, <code>HalfNormalDistribution</code>, <code>GammaDistributionPositive</code>, <code>NumericalDistribution</code>, <code>MultivariateNormalDistribution</code>.</p> </li> <li> <p><code>observables : list containing str</code>     List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a \\(q^2\\) value or \\(q^2\\) range).</p> </li> <li> <p><code>parameters : dict</code>     Parameters of the distribution. The keys depend on the type of the distribution as follows:</p> <ul> <li><code>NormalDistribution</code>: <code>central_value</code> (float), <code>standard_deviation</code> (float)</li> <li><code>HalfNormalDistribution</code>: <code>central_value</code> (float), <code>standard_deviation</code> (float)</li> <li><code>GammaDistributionPositive</code>: <code>a</code> (float), <code>loc</code> (float), <code>scale</code> (float)</li> <li><code>NumericalDistribution</code>: <code>x</code> (list of float), <code>y</code> (list of float)</li> <li><code>MultivariateNormalDistribution</code>: <code>central_value</code> (list of float), <code>covariance</code> (list of list of float)</li> </ul> </li> </ul> required <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the measurement.</p> <code>constraints</code> <code>list of dict</code> <p>List of constraints on observables. Each constraint is a dictionary with the following keys:</p> <ul> <li> <p><code>observables : list containing str</code>     List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a \\(q^2\\) value or \\(q^2\\) range).</p> </li> <li> <p><code>distribution_type : str</code>     Type of the distribution. Can be <code>NormalDistribution</code>, <code>HalfNormalDistribution</code>, <code>GammaDistributionPositive</code>, <code>NumericalDistribution</code>, <code>MultivariateNormalDistribution</code>.</p> </li> <li> <p><code>parameters : dict</code>     Parameters of the distribution. The keys depend on the type of the distribution as follows:</p> <ul> <li><code>NormalDistribution</code>: <code>central_value</code> (float), <code>standard_deviation</code> (float)</li> <li><code>HalfNormalDistribution</code>: <code>central_value</code> (float), <code>standard_deviation</code> (float)</li> <li><code>GammaDistributionPositive</code>: <code>a</code> (float), <code>loc</code> (float), <code>scale</code> (float)</li> <li><code>NumericalDistribution</code>: <code>x</code> (list of float), <code>y</code> (list of float)</li> <li><code>MultivariateNormalDistribution</code>: <code>central_value</code> (list of float), <code>covariance</code> (list of list of float)</li> </ul> </li> </ul> <code>constrained_observables</code> <code>set</code> <p>Set of observables that the measurement constrains</p> <p>Methods:</p> Name Description <code>get_all_measurements</code> <p>Return all measurements.</p> <code>get_all_observables</code> <p>Return all observables.</p> <code>get_measurements</code> <p>Return measurements that constrain the specified observables.</p> <code>get_constraints</code> <p>Return constraints on the specified observables.</p> <code>get_combined_constraints</code> <p>Return combined constraints on the specified observables.</p> <code>load</code> <p>Load measurements from a json file or a directory containing json files</p> <code>unload</code> <p>Unload measurements.</p> <code>clear</code> <p>Clear all measurements.</p> <p>Examples:</p> <p>Load measurements from a json file:</p> <pre><code>&gt;&gt;&gt; Measurement.load('measurements.json')\n</code></pre> <p>Get all measurements:</p> <pre><code>&gt;&gt;&gt; Measurement.get_all_measurements()\n{'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n</code></pre> <p>Get all observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_all_observables()\n{'observable1', 'observable2', ...}\n</code></pre> <p>Get measurements that contain the specified observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_measurements({'observable1', 'observable2'})\n{'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n</code></pre> <p>Get constraints on the specified observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_constraints({'observable1', 'observable2'})\n{'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n</code></pre> <p>Get combined constraints on the specified observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_combined_constraints({'observable1', 'observable2'})\n{'NormalDistribution': {'measurement_name': ['measurement1'], 'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n</code></pre> <p>Unload measurements:</p> <pre><code>&gt;&gt;&gt; Measurement.unload(['measurement1', 'measurement2'])\n</code></pre> <p>Clear all measurements:</p> <pre><code>&gt;&gt;&gt; Measurement.clear()\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>class Measurement:\n    '''\n    Class to store measurements and constraints on observables.\n\n    Parameters\n    ----------\n    name : str\n        Name of the measurement.\n    constraints : list of dict\n        List of constraints on observables. Each constraint is a dictionary with the following keys:\n\n          - `type : str`\n            Type of the distribution. Can be `NormalDistribution`, `HalfNormalDistribution`, `GammaDistributionPositive`, `NumericalDistribution`, `MultivariateNormalDistribution`.\n\n          - `observables : list containing str`\n            List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a $q^2$ value or $q^2$ range).\n\n          - `parameters : dict`\n            Parameters of the distribution. The keys depend on the type of the distribution as follows:\n              - `NormalDistribution`: `central_value` (float), `standard_deviation` (float)\n              - `HalfNormalDistribution`: `central_value` (float), `standard_deviation` (float)\n              - `GammaDistributionPositive`: `a` (float), `loc` (float), `scale` (float)\n              - `NumericalDistribution`: `x` (list of float), `y` (list of float)\n              - `MultivariateNormalDistribution`: `central_value` (list of float), `covariance` (list of list of float)\n\n    Attributes\n    ----------\n    name : str\n        Name of the measurement.\n    constraints : list of dict\n        List of constraints on observables. Each constraint is a dictionary with the following keys:\n\n          - `observables : list containing str`\n            List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a $q^2$ value or $q^2$ range).\n\n          - `distribution_type : str`\n            Type of the distribution. Can be `NormalDistribution`, `HalfNormalDistribution`, `GammaDistributionPositive`, `NumericalDistribution`, `MultivariateNormalDistribution`.\n\n          - `parameters : dict`\n            Parameters of the distribution. The keys depend on the type of the distribution as follows:\n              - `NormalDistribution`: `central_value` (float), `standard_deviation` (float)\n              - `HalfNormalDistribution`: `central_value` (float), `standard_deviation` (float)\n              - `GammaDistributionPositive`: `a` (float), `loc` (float), `scale` (float)\n              - `NumericalDistribution`: `x` (list of float), `y` (list of float)\n              - `MultivariateNormalDistribution`: `central_value` (list of float), `covariance` (list of list of float)\n    constrained_observables : set\n        Set of observables that the measurement constrains\n\n    Methods\n    -------\n    get_all_measurements()\n        Return all measurements.\n    get_all_observables()\n        Return all observables.\n    get_measurements(observables)\n        Return measurements that constrain the specified observables.\n    get_constraints(observables)\n        Return constraints on the specified observables.\n    get_combined_constraints(observables)\n        Return combined constraints on the specified observables.\n    load(path)\n        Load measurements from a json file or a directory containing json files\n    unload(measurement_names)\n        Unload measurements.\n    clear()\n        Clear all measurements.\n\n    Examples\n    --------\n    Load measurements from a json file:\n\n    &gt;&gt;&gt; Measurement.load('measurements.json')\n\n    Get all measurements:\n\n    &gt;&gt;&gt; Measurement.get_all_measurements()\n    {'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n\n    Get all observables:\n\n    &gt;&gt;&gt; Measurement.get_all_observables()\n    {'observable1', 'observable2', ...}\n\n    Get measurements that contain the specified observables:\n\n    &gt;&gt;&gt; Measurement.get_measurements({'observable1', 'observable2'})\n    {'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n\n    Get constraints on the specified observables:\n\n    &gt;&gt;&gt; Measurement.get_constraints({'observable1', 'observable2'})\n    {'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n\n    Get combined constraints on the specified observables:\n\n    &gt;&gt;&gt; Measurement.get_combined_constraints({'observable1', 'observable2'})\n    {'NormalDistribution': {'measurement_name': ['measurement1'], 'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n\n    Unload measurements:\n\n    &gt;&gt;&gt; Measurement.unload(['measurement1', 'measurement2'])\n\n    Clear all measurements:\n\n    &gt;&gt;&gt; Measurement.clear()\n    '''\n\n    _measurements: Dict[str, 'Measurement'] = {}  # Class attribute to store all measurements\n    _observable_to_measurements: DefaultDict[str, Set[str]] = defaultdict(set)  # Class attribute to map observables to measurements\n    _pdfxf_versions = ['1.0'] # List of supported versions of the pdfxf JSON schema\n\n    def __init__(self, name: str, constraints: List[dict]):\n        '''\n        Initialize a Measurement object.\n\n        Parameters\n        ----------\n        name : str\n            Name of the measurement.\n        constraints : list of dict\n            List of constraints on observables. Each constraint is a dictionary with the following keys:\n\n              - `type : str`\n                Type of the distribution. Can be `NormalDistribution`, `HalfNormalDistribution`, `GammaDistributionPositive`, `NumericalDistribution`, `MultivariateNormalDistribution`.\n\n              - `observables : list containing str`\n                List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a $q^2$ value or $q^2$ range).\n\n              - `parameters : dict`\n                Parameters of the distribution. The keys depend on the type of the distribution as follows:\n                  - `NormalDistribution`: `central_value` (float), `standard_deviation` (float)\n                  - `HalfNormalDistribution`: `central_value` (float), `standard_deviation` (float)\n                  - `GammaDistributionPositive`: `a` (float), `loc` (float), `scale` (float)\n                  - `NumericalDistribution`: `x` (list of float), `y` (list of float)\n                  - `MultivariateNormalDistribution`: `central_value` (list of float), `covariance` (list of list of float)\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Initialize a Measurement object:\n\n        &gt;&gt;&gt; Measurement('measurement1', [{'type': 'NormalDistribution', 'observables': ['observable1'], 'parameters': {'central_value': 0.0, 'standard_deviation': 1.0}}])\n        '''\n        self.name: str = name\n        self.constraints: list[dict] = []\n        for constraint in constraints:\n\n            # Convert list of observable names to numpy array containing strings\n            constraint['observables'] = np.array(constraint['observables'], dtype=object)\n\n            # Add measurement name to `_observable_to_measurements` class attribute\n            for observable in constraint['observables']:\n                self._observable_to_measurements[observable].add(name)\n\n            # Add constraint to `constraints` attribute of the Measurement object\n            self.constraints.append(self._define_constraint(**constraint))\n\n        # Add set of observables that the measurement constrains to `constrained_observables` attribute of the Measurement object\n        self.constrained_observables = set(chain.from_iterable(\n            constraint['observables'] for constraint in self.constraints\n        ))\n\n        # Add measurement to `_measurements` class attribute\n        self._measurements[name] = self\n\n    @staticmethod\n    def _define_constraint(observables: np.ndarray, distribution_type: str, **parameters: dict) -&gt; dict:\n\n        # Convert GeneralGammaDistributionPositive to NumericalDistribution\n        if distribution_type == 'GeneralGammaDistributionPositive':\n            distribution_type, parameters = convert_GeneralGammaDistributionPositive(**parameters)\n\n        # Convert lists to numpy arrays for numerical distributions,\n        # normalize PDF to 1, and add log PDF\n        elif distribution_type == 'NumericalDistribution':\n            x = np.array(parameters['x'])\n            y = np.array(parameters['y'])\n            y = np.maximum(0, y)  # make sure PDF is positive\n            y = y /  np.trapz(y, x=x)  # normalize PDF to 1\n            # ignore warning from log(0)=-np.inf\n            with np.errstate(divide='ignore', invalid='ignore'):\n                log_y = np.log(y)\n            # replace -np.inf with a large negative number\n            log_y[np.isneginf(log_y)] = LOG_ZERO\n            parameters['x'] = x\n            parameters['y'] = y\n            parameters['log_y'] = log_y\n\n        # Convert lists to numpy arrays for multivariate normal distribution\n        elif distribution_type == 'MultivariateNormalDistribution':\n            parameters['standard_deviation'] = np.array(parameters['standard_deviation'])\n            parameters['correlation'] = np.array(parameters['correlation'])\n\n        return {'observables': observables, 'distribution_type': distribution_type, 'parameters': parameters}\n\n    def __repr__(self):\n        return f'&lt;Measurement {self.name} constraining {self.constrained_observables}&gt;'\n\n    def __str__(self):\n        return f'Measurement {self.name} constraining {self.constrained_observables}'\n\n    @classmethod\n    def get_all_measurements(cls):\n        '''\n        Return all measurements.\n\n        Returns\n        -------\n        dict\n            Dictionary containing all measurements.\n\n        Examples\n        --------\n        Get all measurements:\n\n        &gt;&gt;&gt; Measurement.get_all_measurements()\n        {'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n        '''\n        return cls._measurements\n\n    @classmethod\n    def get_all_observables(cls):\n        '''\n        Return all observables.\n\n        Returns\n        -------\n        set\n            Set containing all observables.\n\n        Examples\n        --------\n        Get all observables:\n\n        &gt;&gt;&gt; Measurement.get_all_observables()\n        {'observable1', 'observable2', ...}\n        '''\n        return set(cls._observable_to_measurements.keys())\n\n    @classmethod\n    def get_measurements(\n        cls,\n        observables: Union[List[str], np.ndarray],\n        include_measurements: Optional[List[str]] = None,\n        exclude_measurements: Optional[List[str]] = None,\n    ) -&gt; Dict[str, 'Measurement']:\n        '''\n        Return measurements that constrain the specified observables.\n\n        Parameters\n        ----------\n        observables : list or array of str\n            Observables to constrain.\n        include_measurements : list of str, optional\n            A list of measurements to include. If `None`, include all measurements.\n        exclude_measurements : list of str, optional\n            A list of measurements to exclude. If `None`, exclude no measurements.\n\n        Returns\n        -------\n        dict\n            Dictionary containing measurements that constrain the specified observables.\n\n        Examples\n        --------\n        Get measurements that constrain the specified observables:\n\n        &gt;&gt;&gt; Measurement.get_measurements(['observable1', 'observable2'])\n        {'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n        '''\n        if include_measurements is not None and exclude_measurements is not None:\n            raise ValueError(\"Please provide either `include_measurements` or `exclude_measurements`, not both.\")\n        measurement_names = set(chain.from_iterable(\n            cls._observable_to_measurements.get(observable, set())\n            for observable in observables\n        ))\n        all_measurements = set(cls.get_all_measurements())\n        if include_measurements is not None:\n            if set(include_measurements) - all_measurements:\n                raise ValueError(f\"Measurements {set(include_measurements) - all_measurements} provided in `include_measurements` not found in loaded measurements.\")\n            measurement_names = set(include_measurements) &amp; measurement_names\n        elif exclude_measurements is not None:\n            if set(exclude_measurements) - all_measurements:\n                raise ValueError(f\"Measurements {set(exclude_measurements) - all_measurements} provided in `exclude_measurements` not found in loaded measurements.\")\n            measurement_names = measurement_names - set(exclude_measurements)\n        return {name: cls._measurements[name] for name in measurement_names}\n\n    @classmethod\n    def get_constraints(\n        cls,\n        observables: Union[List[str], np.ndarray],\n        observables_for_indices: Union[List[str], np.ndarray] = None,\n        distribution_types: Optional[List[str]] = None,\n        include_measurements: Optional[List[str]] = None,\n        exclude_measurements: Optional[List[str]] = None,\n    ) -&gt; Dict[str, Dict[str, np.ndarray]]:\n        '''\n        Return constraints on the specified observables.\n\n        Parameters\n        ----------\n        observables : list or array of str\n            Observables to constrain.\n        observables_for_indices : list or array of str, optional\n            Observables to create indices for. If `None`, use the same observables as `observables`.\n        distribution_types : list of str, optional\n            Types of distributions to include. If `None`, include all distributions.\n        include_measurements : list of str, optional\n            A list of measurements to include. If `None`, include all measurements.\n        exclude_measurements : list of str, optional\n            A list of measurements to exclude. If `None`, exclude no measurements.\n\n        Returns\n        -------\n        dict\n            Dictionary containing constraints on the specified observables.\n\n        Examples\n        --------\n        Get constraints on the specified observables:\n\n        &gt;&gt;&gt; Measurement.get_constraints(['observable1', 'observable2'])\n        {'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n\n        Get constraints on the specified observables with specific distribution types:\n\n        &gt;&gt;&gt; Measurement.get_constraints(['observable1', 'observable2'], ['NormalDistribution', 'MultivariateNormalDistribution'])\n        {'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, 'MultivariateNormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'covariance': [[1.0, 0.0], [0.0, 1.0]], 'inverse_covariance': [[1.0, 0.0], [0.0, 1.0]]}}\n        '''\n        if not isinstance(observables, (list, tuple, np.ndarray)):\n            raise ValueError('observables must be a list, tuple, or array')\n        if isinstance(observables, np.ndarray):\n            observables = observables.tolist()\n        if observables_for_indices is None:\n            observables_for_indices = observables\n        else:\n            if not isinstance(observables_for_indices, (list, tuple, np.ndarray)):\n                raise ValueError('observables_for_indices must be a list, tuple, or array')\n            if isinstance(observables_for_indices, np.ndarray):\n                observables_for_indices = observables_for_indices.tolist()\n        measurements = cls.get_measurements(observables, include_measurements, exclude_measurements)\n        observables_set = set(observables)\n        constraints = defaultdict(lambda: defaultdict(list))\n        for measurement_name, measurement in measurements.items():\n            for constraint in measurement.constraints:\n                selected_observables = set(constraint['observables']) &amp; observables_set\n                if selected_observables:\n                    distribution_type = constraint['distribution_type']\n                    if distribution_types is not None and distribution_type not in distribution_types:\n                        continue\n                    if distribution_type == 'MultivariateNormalDistribution':\n\n                        # Boolean mask for order-preserving selection\n                        selected_observables_array = np.array(list(selected_observables), dtype=object)\n                        mask = np.isin(constraint['observables'], selected_observables_array)\n\n                        # Skip if no matches\n                        if not np.any(mask):\n                            continue\n\n                        # Select entries using the boolean mask\n                        constraint_observables = constraint['observables'][mask]\n                        constraint_central_value = np.array(constraint['parameters']['central_value'])[mask]\n                        constraint_standard_deviation = np.array(constraint['parameters']['standard_deviation'])[mask]\n                        constraint_correlation = np.array(constraint['parameters']['correlation'])[mask][:, mask]\n                        observable_indices = np.array([observables_for_indices.index(obs) for obs in constraint_observables])\n\n                        if np.sum(mask) == 1: # Univariate normal distribution\n                            constraints['NormalDistribution']['measurement_name'].append(measurement_name)\n                            constraints['NormalDistribution']['observables'].extend(constraint_observables)\n                            constraints['NormalDistribution']['observable_indices'].extend(observable_indices)\n                            constraints['NormalDistribution']['central_value'].extend(constraint_central_value)\n                            constraints['NormalDistribution']['standard_deviation'].extend(constraint_standard_deviation)\n                        else: # Multivariate normal distribution\n                            constraints[distribution_type]['measurement_name'].append(measurement_name)\n                            constraints[distribution_type]['observables'].append(\n                                np.asarray(constraint_observables, dtype=object)\n                            )\n                            constraints[distribution_type]['observable_indices'].append(\n                                np.asarray(observable_indices, dtype=int)\n                            )\n                            constraints[distribution_type]['central_value'].append(\n                                np.asarray(constraint_central_value)\n                            )\n                            constraints[distribution_type]['standard_deviation'].append(\n                                np.asarray(constraint_standard_deviation)\n                            )\n                            constraints[distribution_type]['inverse_correlation'].append(\n                                np.linalg.inv(constraint_correlation)\n                            )\n                            n = len(constraint_observables)\n                            logdet_corr = np.linalg.slogdet(constraint_correlation)[1]\n                            logprod_std2 = 2 * np.sum(np.log(constraint_standard_deviation))\n                            constraints[distribution_type]['logpdf_normalization_per_observable'].append(\n                                -0.5 * ( (logdet_corr + logprod_std2) / n + np.log(2 * np.pi) )\n                            )\n                    else:\n                        constraints[distribution_type]['measurement_name'].append(measurement_name)\n                        observable_indices = [observables_for_indices.index(obs) for obs in constraint['observables']]\n                        constraints[distribution_type]['observables'].extend(constraint['observables'])\n                        constraints[distribution_type]['observable_indices'].extend(observable_indices)\n                        for key in constraint['parameters']:\n                            constraints[distribution_type][key].append(constraint['parameters'][key])\n        for distribution_type in constraints:\n\n            # Pad arrays to the same length for numerical distributions\n            if distribution_type == 'NumericalDistribution':\n                constraints[distribution_type]['x'] = pad_arrays(constraints[distribution_type]['x'])\n                constraints[distribution_type]['y'] = pad_arrays(constraints[distribution_type]['y'])\n                constraints[distribution_type]['log_y'] = pad_arrays(constraints[distribution_type]['log_y'])\n\n            # Convert lists to numpy arrays\n            if distribution_type == 'MultivariateNormalDistribution':\n                for key in constraints[distribution_type]:\n                    nparray = np.empty(len(constraints[distribution_type][key]), dtype=object)\n                    nparray[:] = constraints[distribution_type][key]\n                    constraints[distribution_type][key] = nparray\n            else:\n                for key in constraints[distribution_type]:\n                    if key == 'observable_indices':\n                        dtype = int\n                    elif key == 'observables':\n                        dtype = object\n                    else:\n                        dtype = None\n                    constraints[distribution_type][key] = np.asarray(\n                        constraints[distribution_type][key],\n                        dtype=dtype\n                    )\n        return constraints\n\n    def combine_constraints(\n            constraints_list: List[Dict[str, Dict[str, np.ndarray]]],\n    ) -&gt; Dict[str, Dict[str, np.ndarray]]:\n        '''\n        Combine the constraints provided in the list of constraints, where each element of the list is a dictionary of constraints on a single observable.\n\n        Normal distributions are combined analytically, while other distributions are combined numerically.\n\n        Parameters\n        ----------\n        constraints_list : list of dict\n            List of constraints to combine, one constraints dictionary per observable.\n\n        Returns\n        -------\n        dict\n            Dictionary containing combined constraints.\n\n        Examples\n        --------\n        Combine two constraints on two observables:\n        &gt;&gt;&gt; Measurement.combine_constraints([\n        ...     {'NormalDistribution': {'measurement_name': ['measurement1', 'measurement2'], 'observables': ['observable1', 'observable1'], 'observable_indices': np.array([0, 0]), 'central_value': np.array([1.0, 1.2]), 'standard_deviation': np.array([0.2, 0.3])}},\n        ...     {'NormalDistribution': {'measurement_name': ['measurement3', 'measurement4'], 'observables': ['observable2', 'observable2'], 'observable_indices': np.array([1, 1]), 'central_value': np.array([2.0, 2.5]), 'standard_deviation': np.array([0.5, 0.7])}},\n        ... ])\n        {'NormalDistribution':\n            {\n                'measurement_name': array(['measurement1, measurement2', 'measurement3, measurement4']),\n                'observables': array(['observable1', 'observable2']),\n                'observable_indices': array([0, 1]),\n                'central_value': array([1.06153846, 2.16891892]),\n                'standard_deviation': array([0.16641006, 0.40686674])\n            },\n        }\n        '''\n\n        combined_constraints = defaultdict(lambda: defaultdict(list))\n        for constraints in constraints_list:\n            # handle normal distributions\n            if 'NormalDistribution' in constraints:\n                constraints['NormalDistribution'] = combine_normal_distributions(**constraints['NormalDistribution'])\n\n            if len(constraints) &gt; 1 or len(next(iter(constraints.values()))['measurement_name']) &gt; 1:\n                numerical_distribution = combine_distributions_numerically(constraints)\n                for key, value in numerical_distribution.items():\n                    combined_constraints['NumericalDistribution'][key].append(value)\n            else:\n                dist_type, dist_info = next(iter(constraints.items()))\n                for key, value in dist_info.items():\n                    if dist_type == 'NumericalDistribution' and key in ['x', 'y', 'log_y']:\n                        value = np.squeeze(value)\n                    combined_constraints[dist_type][key].append(value)\n\n        for dist_type, dist_info in combined_constraints.items():\n            for key, value_list in dist_info.items():\n                if dist_type == 'NumericalDistribution' and key in ['x', 'y', 'log_y']:\n                    combined_constraints[dist_type][key] = pad_arrays(value_list)\n                else:\n                    combined_constraints[dist_type][key] = np.concatenate(value_list, axis=0)\n        return combined_constraints\n\n    @classmethod\n    def get_combined_constraints(\n        cls,\n        observables: Union[List[str], np.ndarray],\n        ) -&gt; Dict[str, Dict[str, np.ndarray]]:\n        '''\n        Return combined constraints on the specified observables.\n\n        Parameters\n        ----------\n        observables : list or array of str\n            Observables to combine constraints for.\n\n        Returns\n        -------\n        dict\n            Dictionary containing combined constraints on the specified observables.\n\n        Examples\n        --------\n        Get combined constraints on the specified observables:\n\n        &gt;&gt;&gt; Measurement.get_combined_constraints(['observable1', 'observable2'])\n        {'NormalDistribution': {'measurement_name': ['measurement1'], 'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n        '''\n\n        return cls.combine_constraints(\n            [cls.get_constraints([observable], observables) for observable in observables]\n        )\n\n    @classmethod\n    def _load_file(cls, path: str) -&gt; None:\n        with open(path, 'r') as f:\n            json_data = json.load(f)\n        schema_name, schema_version = get_json_schema(json_data)\n        if schema_name == 'pdfxf' and schema_version in cls._pdfxf_versions:\n            del json_data['$schema']\n            for name, constraints in json_data.items():\n                cls(name, constraints)\n\n    @classmethod\n    def load(cls, path: str) -&gt; None:\n        '''\n        Load measurements from a json file or a directory containing json files.\n\n        Parameters\n        ----------\n        path : str\n            Path to a json file or a directory containing json files.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Load measurements from a json file:\n\n        &gt;&gt;&gt; Measurement.load('./measurements.json')\n\n        Load measurements from a directory containing json files:\n\n        &gt;&gt;&gt; Measurement.load('./measurements/')\n        '''\n        # load all json files in the directory\n        if os.path.isdir(path):\n            for file in os.listdir(path):\n                if file.endswith('.json'):\n                    cls._load_file(os.path.join(path, file))\n        # load single json file\n        else:\n            cls._load_file(path)\n\n    @classmethod\n    def unload(cls, measurement_names: List[str]) -&gt; None:\n        '''\n        Unload measurements.\n\n        Parameters\n        ----------\n        measurement_names : list of str\n            Names of the measurements to unload.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Unload measurements:\n\n        &gt;&gt;&gt; Measurement.unload(['measurement1', 'measurement2'])\n        '''\n        for name in measurement_names:\n            measurement = cls._measurements.pop(name, None)\n            if measurement is not None:\n                for constraint in measurement.constraints:\n                    for observable in constraint['observables']:\n                        cls._observable_to_measurements[observable].remove(name)\n                        if not cls._observable_to_measurements[observable]:\n                            del cls._observable_to_measurements[observable]\n\n    @classmethod\n    def clear(cls) -&gt; None:\n        '''\n        Clear all measurements.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Clear all measurements:\n\n        &gt;&gt;&gt; Measurement.clear()\n        '''\n        cls._measurements.clear()\n        cls._observable_to_measurements.clear()\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.__init__","title":"<code>__init__(name, constraints)</code>","text":"<p>Initialize a Measurement object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the measurement.</p> required <code>constraints</code> <code>list of dict</code> <p>List of constraints on observables. Each constraint is a dictionary with the following keys:</p> <ul> <li> <p><code>type : str</code>     Type of the distribution. Can be <code>NormalDistribution</code>, <code>HalfNormalDistribution</code>, <code>GammaDistributionPositive</code>, <code>NumericalDistribution</code>, <code>MultivariateNormalDistribution</code>.</p> </li> <li> <p><code>observables : list containing str</code>     List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a \\(q^2\\) value or \\(q^2\\) range).</p> </li> <li> <p><code>parameters : dict</code>     Parameters of the distribution. The keys depend on the type of the distribution as follows:</p> <ul> <li><code>NormalDistribution</code>: <code>central_value</code> (float), <code>standard_deviation</code> (float)</li> <li><code>HalfNormalDistribution</code>: <code>central_value</code> (float), <code>standard_deviation</code> (float)</li> <li><code>GammaDistributionPositive</code>: <code>a</code> (float), <code>loc</code> (float), <code>scale</code> (float)</li> <li><code>NumericalDistribution</code>: <code>x</code> (list of float), <code>y</code> (list of float)</li> <li><code>MultivariateNormalDistribution</code>: <code>central_value</code> (list of float), <code>covariance</code> (list of list of float)</li> </ul> </li> </ul> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Initialize a Measurement object:</p> <pre><code>&gt;&gt;&gt; Measurement('measurement1', [{'type': 'NormalDistribution', 'observables': ['observable1'], 'parameters': {'central_value': 0.0, 'standard_deviation': 1.0}}])\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>def __init__(self, name: str, constraints: List[dict]):\n    '''\n    Initialize a Measurement object.\n\n    Parameters\n    ----------\n    name : str\n        Name of the measurement.\n    constraints : list of dict\n        List of constraints on observables. Each constraint is a dictionary with the following keys:\n\n          - `type : str`\n            Type of the distribution. Can be `NormalDistribution`, `HalfNormalDistribution`, `GammaDistributionPositive`, `NumericalDistribution`, `MultivariateNormalDistribution`.\n\n          - `observables : list containing str`\n            List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a $q^2$ value or $q^2$ range).\n\n          - `parameters : dict`\n            Parameters of the distribution. The keys depend on the type of the distribution as follows:\n              - `NormalDistribution`: `central_value` (float), `standard_deviation` (float)\n              - `HalfNormalDistribution`: `central_value` (float), `standard_deviation` (float)\n              - `GammaDistributionPositive`: `a` (float), `loc` (float), `scale` (float)\n              - `NumericalDistribution`: `x` (list of float), `y` (list of float)\n              - `MultivariateNormalDistribution`: `central_value` (list of float), `covariance` (list of list of float)\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Initialize a Measurement object:\n\n    &gt;&gt;&gt; Measurement('measurement1', [{'type': 'NormalDistribution', 'observables': ['observable1'], 'parameters': {'central_value': 0.0, 'standard_deviation': 1.0}}])\n    '''\n    self.name: str = name\n    self.constraints: list[dict] = []\n    for constraint in constraints:\n\n        # Convert list of observable names to numpy array containing strings\n        constraint['observables'] = np.array(constraint['observables'], dtype=object)\n\n        # Add measurement name to `_observable_to_measurements` class attribute\n        for observable in constraint['observables']:\n            self._observable_to_measurements[observable].add(name)\n\n        # Add constraint to `constraints` attribute of the Measurement object\n        self.constraints.append(self._define_constraint(**constraint))\n\n    # Add set of observables that the measurement constrains to `constrained_observables` attribute of the Measurement object\n    self.constrained_observables = set(chain.from_iterable(\n        constraint['observables'] for constraint in self.constraints\n    ))\n\n    # Add measurement to `_measurements` class attribute\n    self._measurements[name] = self\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clear all measurements.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Clear all measurements:</p> <pre><code>&gt;&gt;&gt; Measurement.clear()\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef clear(cls) -&gt; None:\n    '''\n    Clear all measurements.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Clear all measurements:\n\n    &gt;&gt;&gt; Measurement.clear()\n    '''\n    cls._measurements.clear()\n    cls._observable_to_measurements.clear()\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.combine_constraints","title":"<code>combine_constraints(constraints_list)</code>","text":"<p>Combine the constraints provided in the list of constraints, where each element of the list is a dictionary of constraints on a single observable.</p> <p>Normal distributions are combined analytically, while other distributions are combined numerically.</p> <p>Parameters:</p> Name Type Description Default <code>constraints_list</code> <code>list of dict</code> <p>List of constraints to combine, one constraints dictionary per observable.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing combined constraints.</p> <p>Examples:</p> <p>Combine two constraints on two observables:</p> <pre><code>&gt;&gt;&gt; Measurement.combine_constraints([\n...     {'NormalDistribution': {'measurement_name': ['measurement1', 'measurement2'], 'observables': ['observable1', 'observable1'], 'observable_indices': np.array([0, 0]), 'central_value': np.array([1.0, 1.2]), 'standard_deviation': np.array([0.2, 0.3])}},\n...     {'NormalDistribution': {'measurement_name': ['measurement3', 'measurement4'], 'observables': ['observable2', 'observable2'], 'observable_indices': np.array([1, 1]), 'central_value': np.array([2.0, 2.5]), 'standard_deviation': np.array([0.5, 0.7])}},\n... ])\n{'NormalDistribution':\n    {\n        'measurement_name': array(['measurement1, measurement2', 'measurement3, measurement4']),\n        'observables': array(['observable1', 'observable2']),\n        'observable_indices': array([0, 1]),\n        'central_value': array([1.06153846, 2.16891892]),\n        'standard_deviation': array([0.16641006, 0.40686674])\n    },\n}\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>def combine_constraints(\n        constraints_list: List[Dict[str, Dict[str, np.ndarray]]],\n) -&gt; Dict[str, Dict[str, np.ndarray]]:\n    '''\n    Combine the constraints provided in the list of constraints, where each element of the list is a dictionary of constraints on a single observable.\n\n    Normal distributions are combined analytically, while other distributions are combined numerically.\n\n    Parameters\n    ----------\n    constraints_list : list of dict\n        List of constraints to combine, one constraints dictionary per observable.\n\n    Returns\n    -------\n    dict\n        Dictionary containing combined constraints.\n\n    Examples\n    --------\n    Combine two constraints on two observables:\n    &gt;&gt;&gt; Measurement.combine_constraints([\n    ...     {'NormalDistribution': {'measurement_name': ['measurement1', 'measurement2'], 'observables': ['observable1', 'observable1'], 'observable_indices': np.array([0, 0]), 'central_value': np.array([1.0, 1.2]), 'standard_deviation': np.array([0.2, 0.3])}},\n    ...     {'NormalDistribution': {'measurement_name': ['measurement3', 'measurement4'], 'observables': ['observable2', 'observable2'], 'observable_indices': np.array([1, 1]), 'central_value': np.array([2.0, 2.5]), 'standard_deviation': np.array([0.5, 0.7])}},\n    ... ])\n    {'NormalDistribution':\n        {\n            'measurement_name': array(['measurement1, measurement2', 'measurement3, measurement4']),\n            'observables': array(['observable1', 'observable2']),\n            'observable_indices': array([0, 1]),\n            'central_value': array([1.06153846, 2.16891892]),\n            'standard_deviation': array([0.16641006, 0.40686674])\n        },\n    }\n    '''\n\n    combined_constraints = defaultdict(lambda: defaultdict(list))\n    for constraints in constraints_list:\n        # handle normal distributions\n        if 'NormalDistribution' in constraints:\n            constraints['NormalDistribution'] = combine_normal_distributions(**constraints['NormalDistribution'])\n\n        if len(constraints) &gt; 1 or len(next(iter(constraints.values()))['measurement_name']) &gt; 1:\n            numerical_distribution = combine_distributions_numerically(constraints)\n            for key, value in numerical_distribution.items():\n                combined_constraints['NumericalDistribution'][key].append(value)\n        else:\n            dist_type, dist_info = next(iter(constraints.items()))\n            for key, value in dist_info.items():\n                if dist_type == 'NumericalDistribution' and key in ['x', 'y', 'log_y']:\n                    value = np.squeeze(value)\n                combined_constraints[dist_type][key].append(value)\n\n    for dist_type, dist_info in combined_constraints.items():\n        for key, value_list in dist_info.items():\n            if dist_type == 'NumericalDistribution' and key in ['x', 'y', 'log_y']:\n                combined_constraints[dist_type][key] = pad_arrays(value_list)\n            else:\n                combined_constraints[dist_type][key] = np.concatenate(value_list, axis=0)\n    return combined_constraints\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.get_all_measurements","title":"<code>get_all_measurements()</code>  <code>classmethod</code>","text":"<p>Return all measurements.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing all measurements.</p> <p>Examples:</p> <p>Get all measurements:</p> <pre><code>&gt;&gt;&gt; Measurement.get_all_measurements()\n{'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef get_all_measurements(cls):\n    '''\n    Return all measurements.\n\n    Returns\n    -------\n    dict\n        Dictionary containing all measurements.\n\n    Examples\n    --------\n    Get all measurements:\n\n    &gt;&gt;&gt; Measurement.get_all_measurements()\n    {'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n    '''\n    return cls._measurements\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.get_all_observables","title":"<code>get_all_observables()</code>  <code>classmethod</code>","text":"<p>Return all observables.</p> <p>Returns:</p> Type Description <code>set</code> <p>Set containing all observables.</p> <p>Examples:</p> <p>Get all observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_all_observables()\n{'observable1', 'observable2', ...}\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef get_all_observables(cls):\n    '''\n    Return all observables.\n\n    Returns\n    -------\n    set\n        Set containing all observables.\n\n    Examples\n    --------\n    Get all observables:\n\n    &gt;&gt;&gt; Measurement.get_all_observables()\n    {'observable1', 'observable2', ...}\n    '''\n    return set(cls._observable_to_measurements.keys())\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.get_combined_constraints","title":"<code>get_combined_constraints(observables)</code>  <code>classmethod</code>","text":"<p>Return combined constraints on the specified observables.</p> <p>Parameters:</p> Name Type Description Default <code>observables</code> <code>list or array of str</code> <p>Observables to combine constraints for.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing combined constraints on the specified observables.</p> <p>Examples:</p> <p>Get combined constraints on the specified observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_combined_constraints(['observable1', 'observable2'])\n{'NormalDistribution': {'measurement_name': ['measurement1'], 'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef get_combined_constraints(\n    cls,\n    observables: Union[List[str], np.ndarray],\n    ) -&gt; Dict[str, Dict[str, np.ndarray]]:\n    '''\n    Return combined constraints on the specified observables.\n\n    Parameters\n    ----------\n    observables : list or array of str\n        Observables to combine constraints for.\n\n    Returns\n    -------\n    dict\n        Dictionary containing combined constraints on the specified observables.\n\n    Examples\n    --------\n    Get combined constraints on the specified observables:\n\n    &gt;&gt;&gt; Measurement.get_combined_constraints(['observable1', 'observable2'])\n    {'NormalDistribution': {'measurement_name': ['measurement1'], 'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n    '''\n\n    return cls.combine_constraints(\n        [cls.get_constraints([observable], observables) for observable in observables]\n    )\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.get_constraints","title":"<code>get_constraints(observables, observables_for_indices=None, distribution_types=None, include_measurements=None, exclude_measurements=None)</code>  <code>classmethod</code>","text":"<p>Return constraints on the specified observables.</p> <p>Parameters:</p> Name Type Description Default <code>observables</code> <code>list or array of str</code> <p>Observables to constrain.</p> required <code>observables_for_indices</code> <code>list or array of str</code> <p>Observables to create indices for. If <code>None</code>, use the same observables as <code>observables</code>.</p> <code>None</code> <code>distribution_types</code> <code>list of str</code> <p>Types of distributions to include. If <code>None</code>, include all distributions.</p> <code>None</code> <code>include_measurements</code> <code>list of str</code> <p>A list of measurements to include. If <code>None</code>, include all measurements.</p> <code>None</code> <code>exclude_measurements</code> <code>list of str</code> <p>A list of measurements to exclude. If <code>None</code>, exclude no measurements.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing constraints on the specified observables.</p> <p>Examples:</p> <p>Get constraints on the specified observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_constraints(['observable1', 'observable2'])\n{'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n</code></pre> <p>Get constraints on the specified observables with specific distribution types:</p> <pre><code>&gt;&gt;&gt; Measurement.get_constraints(['observable1', 'observable2'], ['NormalDistribution', 'MultivariateNormalDistribution'])\n{'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, 'MultivariateNormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'covariance': [[1.0, 0.0], [0.0, 1.0]], 'inverse_covariance': [[1.0, 0.0], [0.0, 1.0]]}}\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef get_constraints(\n    cls,\n    observables: Union[List[str], np.ndarray],\n    observables_for_indices: Union[List[str], np.ndarray] = None,\n    distribution_types: Optional[List[str]] = None,\n    include_measurements: Optional[List[str]] = None,\n    exclude_measurements: Optional[List[str]] = None,\n) -&gt; Dict[str, Dict[str, np.ndarray]]:\n    '''\n    Return constraints on the specified observables.\n\n    Parameters\n    ----------\n    observables : list or array of str\n        Observables to constrain.\n    observables_for_indices : list or array of str, optional\n        Observables to create indices for. If `None`, use the same observables as `observables`.\n    distribution_types : list of str, optional\n        Types of distributions to include. If `None`, include all distributions.\n    include_measurements : list of str, optional\n        A list of measurements to include. If `None`, include all measurements.\n    exclude_measurements : list of str, optional\n        A list of measurements to exclude. If `None`, exclude no measurements.\n\n    Returns\n    -------\n    dict\n        Dictionary containing constraints on the specified observables.\n\n    Examples\n    --------\n    Get constraints on the specified observables:\n\n    &gt;&gt;&gt; Measurement.get_constraints(['observable1', 'observable2'])\n    {'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n\n    Get constraints on the specified observables with specific distribution types:\n\n    &gt;&gt;&gt; Measurement.get_constraints(['observable1', 'observable2'], ['NormalDistribution', 'MultivariateNormalDistribution'])\n    {'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, 'MultivariateNormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'covariance': [[1.0, 0.0], [0.0, 1.0]], 'inverse_covariance': [[1.0, 0.0], [0.0, 1.0]]}}\n    '''\n    if not isinstance(observables, (list, tuple, np.ndarray)):\n        raise ValueError('observables must be a list, tuple, or array')\n    if isinstance(observables, np.ndarray):\n        observables = observables.tolist()\n    if observables_for_indices is None:\n        observables_for_indices = observables\n    else:\n        if not isinstance(observables_for_indices, (list, tuple, np.ndarray)):\n            raise ValueError('observables_for_indices must be a list, tuple, or array')\n        if isinstance(observables_for_indices, np.ndarray):\n            observables_for_indices = observables_for_indices.tolist()\n    measurements = cls.get_measurements(observables, include_measurements, exclude_measurements)\n    observables_set = set(observables)\n    constraints = defaultdict(lambda: defaultdict(list))\n    for measurement_name, measurement in measurements.items():\n        for constraint in measurement.constraints:\n            selected_observables = set(constraint['observables']) &amp; observables_set\n            if selected_observables:\n                distribution_type = constraint['distribution_type']\n                if distribution_types is not None and distribution_type not in distribution_types:\n                    continue\n                if distribution_type == 'MultivariateNormalDistribution':\n\n                    # Boolean mask for order-preserving selection\n                    selected_observables_array = np.array(list(selected_observables), dtype=object)\n                    mask = np.isin(constraint['observables'], selected_observables_array)\n\n                    # Skip if no matches\n                    if not np.any(mask):\n                        continue\n\n                    # Select entries using the boolean mask\n                    constraint_observables = constraint['observables'][mask]\n                    constraint_central_value = np.array(constraint['parameters']['central_value'])[mask]\n                    constraint_standard_deviation = np.array(constraint['parameters']['standard_deviation'])[mask]\n                    constraint_correlation = np.array(constraint['parameters']['correlation'])[mask][:, mask]\n                    observable_indices = np.array([observables_for_indices.index(obs) for obs in constraint_observables])\n\n                    if np.sum(mask) == 1: # Univariate normal distribution\n                        constraints['NormalDistribution']['measurement_name'].append(measurement_name)\n                        constraints['NormalDistribution']['observables'].extend(constraint_observables)\n                        constraints['NormalDistribution']['observable_indices'].extend(observable_indices)\n                        constraints['NormalDistribution']['central_value'].extend(constraint_central_value)\n                        constraints['NormalDistribution']['standard_deviation'].extend(constraint_standard_deviation)\n                    else: # Multivariate normal distribution\n                        constraints[distribution_type]['measurement_name'].append(measurement_name)\n                        constraints[distribution_type]['observables'].append(\n                            np.asarray(constraint_observables, dtype=object)\n                        )\n                        constraints[distribution_type]['observable_indices'].append(\n                            np.asarray(observable_indices, dtype=int)\n                        )\n                        constraints[distribution_type]['central_value'].append(\n                            np.asarray(constraint_central_value)\n                        )\n                        constraints[distribution_type]['standard_deviation'].append(\n                            np.asarray(constraint_standard_deviation)\n                        )\n                        constraints[distribution_type]['inverse_correlation'].append(\n                            np.linalg.inv(constraint_correlation)\n                        )\n                        n = len(constraint_observables)\n                        logdet_corr = np.linalg.slogdet(constraint_correlation)[1]\n                        logprod_std2 = 2 * np.sum(np.log(constraint_standard_deviation))\n                        constraints[distribution_type]['logpdf_normalization_per_observable'].append(\n                            -0.5 * ( (logdet_corr + logprod_std2) / n + np.log(2 * np.pi) )\n                        )\n                else:\n                    constraints[distribution_type]['measurement_name'].append(measurement_name)\n                    observable_indices = [observables_for_indices.index(obs) for obs in constraint['observables']]\n                    constraints[distribution_type]['observables'].extend(constraint['observables'])\n                    constraints[distribution_type]['observable_indices'].extend(observable_indices)\n                    for key in constraint['parameters']:\n                        constraints[distribution_type][key].append(constraint['parameters'][key])\n    for distribution_type in constraints:\n\n        # Pad arrays to the same length for numerical distributions\n        if distribution_type == 'NumericalDistribution':\n            constraints[distribution_type]['x'] = pad_arrays(constraints[distribution_type]['x'])\n            constraints[distribution_type]['y'] = pad_arrays(constraints[distribution_type]['y'])\n            constraints[distribution_type]['log_y'] = pad_arrays(constraints[distribution_type]['log_y'])\n\n        # Convert lists to numpy arrays\n        if distribution_type == 'MultivariateNormalDistribution':\n            for key in constraints[distribution_type]:\n                nparray = np.empty(len(constraints[distribution_type][key]), dtype=object)\n                nparray[:] = constraints[distribution_type][key]\n                constraints[distribution_type][key] = nparray\n        else:\n            for key in constraints[distribution_type]:\n                if key == 'observable_indices':\n                    dtype = int\n                elif key == 'observables':\n                    dtype = object\n                else:\n                    dtype = None\n                constraints[distribution_type][key] = np.asarray(\n                    constraints[distribution_type][key],\n                    dtype=dtype\n                )\n    return constraints\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.get_measurements","title":"<code>get_measurements(observables, include_measurements=None, exclude_measurements=None)</code>  <code>classmethod</code>","text":"<p>Return measurements that constrain the specified observables.</p> <p>Parameters:</p> Name Type Description Default <code>observables</code> <code>list or array of str</code> <p>Observables to constrain.</p> required <code>include_measurements</code> <code>list of str</code> <p>A list of measurements to include. If <code>None</code>, include all measurements.</p> <code>None</code> <code>exclude_measurements</code> <code>list of str</code> <p>A list of measurements to exclude. If <code>None</code>, exclude no measurements.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing measurements that constrain the specified observables.</p> <p>Examples:</p> <p>Get measurements that constrain the specified observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_measurements(['observable1', 'observable2'])\n{'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef get_measurements(\n    cls,\n    observables: Union[List[str], np.ndarray],\n    include_measurements: Optional[List[str]] = None,\n    exclude_measurements: Optional[List[str]] = None,\n) -&gt; Dict[str, 'Measurement']:\n    '''\n    Return measurements that constrain the specified observables.\n\n    Parameters\n    ----------\n    observables : list or array of str\n        Observables to constrain.\n    include_measurements : list of str, optional\n        A list of measurements to include. If `None`, include all measurements.\n    exclude_measurements : list of str, optional\n        A list of measurements to exclude. If `None`, exclude no measurements.\n\n    Returns\n    -------\n    dict\n        Dictionary containing measurements that constrain the specified observables.\n\n    Examples\n    --------\n    Get measurements that constrain the specified observables:\n\n    &gt;&gt;&gt; Measurement.get_measurements(['observable1', 'observable2'])\n    {'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n    '''\n    if include_measurements is not None and exclude_measurements is not None:\n        raise ValueError(\"Please provide either `include_measurements` or `exclude_measurements`, not both.\")\n    measurement_names = set(chain.from_iterable(\n        cls._observable_to_measurements.get(observable, set())\n        for observable in observables\n    ))\n    all_measurements = set(cls.get_all_measurements())\n    if include_measurements is not None:\n        if set(include_measurements) - all_measurements:\n            raise ValueError(f\"Measurements {set(include_measurements) - all_measurements} provided in `include_measurements` not found in loaded measurements.\")\n        measurement_names = set(include_measurements) &amp; measurement_names\n    elif exclude_measurements is not None:\n        if set(exclude_measurements) - all_measurements:\n            raise ValueError(f\"Measurements {set(exclude_measurements) - all_measurements} provided in `exclude_measurements` not found in loaded measurements.\")\n        measurement_names = measurement_names - set(exclude_measurements)\n    return {name: cls._measurements[name] for name in measurement_names}\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load measurements from a json file or a directory containing json files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a json file or a directory containing json files.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Load measurements from a json file:</p> <pre><code>&gt;&gt;&gt; Measurement.load('./measurements.json')\n</code></pre> <p>Load measurements from a directory containing json files:</p> <pre><code>&gt;&gt;&gt; Measurement.load('./measurements/')\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; None:\n    '''\n    Load measurements from a json file or a directory containing json files.\n\n    Parameters\n    ----------\n    path : str\n        Path to a json file or a directory containing json files.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Load measurements from a json file:\n\n    &gt;&gt;&gt; Measurement.load('./measurements.json')\n\n    Load measurements from a directory containing json files:\n\n    &gt;&gt;&gt; Measurement.load('./measurements/')\n    '''\n    # load all json files in the directory\n    if os.path.isdir(path):\n        for file in os.listdir(path):\n            if file.endswith('.json'):\n                cls._load_file(os.path.join(path, file))\n    # load single json file\n    else:\n        cls._load_file(path)\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.unload","title":"<code>unload(measurement_names)</code>  <code>classmethod</code>","text":"<p>Unload measurements.</p> <p>Parameters:</p> Name Type Description Default <code>measurement_names</code> <code>list of str</code> <p>Names of the measurements to unload.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Unload measurements:</p> <pre><code>&gt;&gt;&gt; Measurement.unload(['measurement1', 'measurement2'])\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef unload(cls, measurement_names: List[str]) -&gt; None:\n    '''\n    Unload measurements.\n\n    Parameters\n    ----------\n    measurement_names : list of str\n        Names of the measurements to unload.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Unload measurements:\n\n    &gt;&gt;&gt; Measurement.unload(['measurement1', 'measurement2'])\n    '''\n    for name in measurement_names:\n        measurement = cls._measurements.pop(name, None)\n        if measurement is not None:\n            for constraint in measurement.constraints:\n                for observable in constraint['observables']:\n                    cls._observable_to_measurements[observable].remove(name)\n                    if not cls._observable_to_measurements[observable]:\n                        del cls._observable_to_measurements[observable]\n</code></pre>"},{"location":"jelli/core/observable_sector/","title":"jelli.core.observable_sector","text":""},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector","title":"<code>ObservableSector</code>","text":"<p>A class to represent an observable sector.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the observable sector.</p> required <code>json_data</code> <code>dict</code> <p>The JSON data containing the metadata and data of the observable sector.</p> required <p>Attributes:</p> Name Type Description <code>metadata</code> <code>dict</code> <p>The metadata of the observable sector.</p> <code>data</code> <code>dict</code> <p>The data of the observable sector.</p> <code>observable_names</code> <code>list</code> <p>The names of the observables.</p> <code>polynomial_names</code> <code>list</code> <p>The names of the polynomial coefficients.</p> <code>observable_expressions</code> <code>list</code> <p>The expressions of the observables in terms of the polynomial coefficients.</p> <code>observable_central</code> <code>array</code> <p>The central values of the polynomial coefficients of the observables (possibly an expansion to second order in the parameters).</p> <code>observable_uncertainties</code> <code>array</code> <p>The uncertainties of the polynomial coefficients of the observables (possibly an expansion to second order in the parameters).</p> <code>observable_uncertainties_SM</code> <code>array</code> <p>The uncertainties of the polynomial coefficients of the observables for the SM entry.</p> <code>polynomial_central</code> <code>array</code> <p>The central values of the polynomial coefficients.</p> <code>eft</code> <code>str</code> <p>The EFT of the observable sector.</p> <code>basis</code> <code>str</code> <p>The EFT basis of the observable sector.</p> <code>scale</code> <code>(float, int)</code> <p>The renormalization scale of the observable sector.</p> <code>sectors</code> <code>list</code> <p>The EFT sectors of the observable sector.</p> <code>parameters</code> <code>list</code> <p>The parameters of the observable sector.</p> <code>keys_pars_by_sectors</code> <code>list</code> <p>The keys of the parameters by sector.</p> <code>keys_pars</code> <code>list</code> <p>The keys of the parameters.</p> <code>sector_indices</code> <code>dict</code> <p>The indices of parameters from EFT sectors in the full parameter basis.</p> <code>evolution_matrices</code> <code>dict</code> <p>The Renormalization Group evolution matrices.</p> <code>construct_par_monomials_observable</code> <code>function</code> <p>The function that constructs the parameter monomials from the parameter array, for the polynomial coefficients of the observables (possibly corresponding to an expansion to second order in the parameters).</p> <code>construct_par_monomials_polynomial</code> <code>function</code> <p>The function that constructs the parameter monomials from the parameter array, for the polynomial coefficients.</p> <code>observable_expression_functions</code> <code>list</code> <p>The functions that evaluate the observable expressions in terms of the polynomial predictions.</p> <code>prediction</code> <code>function</code> <p>The function that makes a prediction for the observable sector.</p> <p>Methods:</p> Name Description <code>get_prediction_data</code> <p>Get the data needed to make a prediction for a given EFT and basis.</p> <code>_get_evolution_matrices</code> <p>Get the Renormalization Group evolution matrices for a given EFT and basis.</p> <code>_get_prediction_function</code> <p>Get the function that makes a prediction for the observable sector.</p> <code>_get_construct_par_monomials</code> <p>Get the function that constructs the parameter monomials from the parameter array.</p> <code>_get_observable_expression_function</code> <p>Get the function that evaluates a given observable expression in terms of the polynomial predictions.</p> <code>get_class_prediction_data</code> <p>Get the data needed to make a prediction for a list of observable sectors.</p> <code>get_class_prediction_function</code> <p>Get the function that makes a prediction for a list of observable sectors.</p> <code>get_all_names</code> <p>Get the names of all observable sectors.</p> <code>get</code> <p>Get an observable sector by name.</p> <code>get_all</code> <p>Get all observable sectors.</p> <p>Examples:</p> <p>Initialize an observable sector:</p> <pre><code>&gt;&gt;&gt; ObservableSector(json_data)\n</code></pre> <p>Load an observable sector from a json file:</p> <pre><code>&gt;&gt;&gt; ObservableSector.load('./observable_sector.json')\n</code></pre> <p>Load all observable sectors from a directory containing json files:</p> <pre><code>&gt;&gt;&gt; ObservableSector.load('./observable_sectors/')\n</code></pre> <p>Get the prediction data for the observable sector:</p> <pre><code>&gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n</code></pre> <p>Make a prediction for the observable sector:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> <p>Get an observable sector by name:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get('observable_sector_1')\n</code></pre> <p>Get all observable sectors:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all()\n</code></pre> <p>Get the names of all observable sectors:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names()\n</code></pre> <p>Get the names of all observable sectors that can provide predictions in the <code>SMEFT</code> basis <code>Warsaw</code>:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names('SMEFT', 'Warsaw')\n</code></pre> <p>Get the names of all observable sectors that can provide predictions in the <code>WET</code> basis <code>flavio</code>:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names('WET', 'flavio')\n</code></pre> <p>Get the names of all observable sectors that can provide predictions in the custom basis <code>custom_basis</code>:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names(custom_basis='custom_basis')\n</code></pre> <p>Get the prediction data for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n</code></pre> <p>Get the prediction function for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n</code></pre> <p>Make a prediction for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n&gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>class ObservableSector:\n    '''\n    A class to represent an observable sector.\n\n    Parameters\n    ----------\n    name : str\n        The name of the observable sector.\n    json_data : dict\n        The JSON data containing the metadata and data of the observable sector.\n\n    Attributes\n    ----------\n    metadata : dict\n        The metadata of the observable sector.\n    data : dict\n        The data of the observable sector.\n    observable_names : list\n        The names of the observables.\n    polynomial_names : list\n        The names of the polynomial coefficients.\n    observable_expressions : list\n        The expressions of the observables in terms of the polynomial coefficients.\n    observable_central : jnp.array\n        The central values of the polynomial coefficients of the observables (possibly an expansion to second order in the parameters).\n    observable_uncertainties : jnp.array\n        The uncertainties of the polynomial coefficients of the observables (possibly an expansion to second order in the parameters).\n    observable_uncertainties_SM : jnp.array\n        The uncertainties of the polynomial coefficients of the observables for the SM entry.\n    polynomial_central : jnp.array\n        The central values of the polynomial coefficients.\n    eft : str\n        The EFT of the observable sector.\n    basis : str\n        The EFT basis of the observable sector.\n    scale : float, int\n        The renormalization scale of the observable sector.\n    sectors : list\n        The EFT sectors of the observable sector.\n    parameters : list\n        The parameters of the observable sector.\n    keys_pars_by_sectors : list\n        The keys of the parameters by sector.\n    keys_pars : list\n        The keys of the parameters.\n    sector_indices : dict\n        The indices of parameters from EFT sectors in the full parameter basis.\n    evolution_matrices : dict\n        The Renormalization Group evolution matrices.\n    construct_par_monomials_observable : function\n        The function that constructs the parameter monomials from the parameter array, for the polynomial coefficients of the observables (possibly corresponding to an expansion to second order in the parameters).\n    construct_par_monomials_polynomial : function\n        The function that constructs the parameter monomials from the parameter array, for the polynomial coefficients.\n    observable_expression_functions : list\n        The functions that evaluate the observable expressions in terms of the polynomial predictions.\n    prediction : function\n        The function that makes a prediction for the observable sector.\n\n    Methods\n    -------\n    get_prediction_data(eft, basis)\n        Get the data needed to make a prediction for a given EFT and basis.\n    _get_evolution_matrices(eft, basis)\n        Get the Renormalization Group evolution matrices for a given EFT and basis.\n    _get_prediction_function()\n        Get the function that makes a prediction for the observable sector.\n    _get_construct_par_monomials(keys_coeff)\n        Get the function that constructs the parameter monomials from the parameter array.\n    _get_observable_expression_function(i)\n        Get the function that evaluates a given observable expression in terms of the polynomial predictions.\n    get_class_prediction_data(eft, basis, observable_sector_names)\n        Get the data needed to make a prediction for a list of observable sectors.\n    get_class_prediction_function(observable_sector_names)\n        Get the function that makes a prediction for a list of observable sectors.\n    get_all_names(eft)\n        Get the names of all observable sectors.\n    get(name)\n        Get an observable sector by name.\n    get_all()\n        Get all observable sectors.\n\n    Examples\n    --------\n    Initialize an observable sector:\n\n    &gt;&gt;&gt; ObservableSector(json_data)\n\n    Load an observable sector from a json file:\n\n    &gt;&gt;&gt; ObservableSector.load('./observable_sector.json')\n\n    Load all observable sectors from a directory containing json files:\n\n    &gt;&gt;&gt; ObservableSector.load('./observable_sectors/')\n\n    Get the prediction data for the observable sector:\n\n    &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n\n    Make a prediction for the observable sector:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n    Get an observable sector by name:\n\n    &gt;&gt;&gt; ObservableSector.get('observable_sector_1')\n\n    Get all observable sectors:\n\n    &gt;&gt;&gt; ObservableSector.get_all()\n\n    Get the names of all observable sectors:\n    &gt;&gt;&gt; ObservableSector.get_all_names()\n\n    Get the names of all observable sectors that can provide predictions in the `SMEFT` basis `Warsaw`:\n    &gt;&gt;&gt; ObservableSector.get_all_names('SMEFT', 'Warsaw')\n\n    Get the names of all observable sectors that can provide predictions in the `WET` basis `flavio`:\n    &gt;&gt;&gt; ObservableSector.get_all_names('WET', 'flavio')\n\n    Get the names of all observable sectors that can provide predictions in the custom basis `custom_basis`:\n    &gt;&gt;&gt; ObservableSector.get_all_names(custom_basis='custom_basis')\n\n    Get the prediction data for a list of observable sectors:\n\n    &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n\n    Get the prediction function for a list of observable sectors:\n\n    &gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n\n    Make a prediction for a list of observable sectors:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n    &gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n    '''\n\n    _observable_sectors: Dict[str, 'ObservableSector'] = {}  # Class attribute to store all observable sectors\n    _popxf_versions = ['1.0'] # List of supported versions of the popxf JSON schema\n\n    def __init__(self, name: str,  json_data: Dict[str, Any])-&gt; None:\n        '''\n        Initialize an observable sector.\n\n        Parameters\n        ----------\n        name : str\n            The name of the observable sector.\n        json_data : dict\n            The JSON data containing the metadata and data of the observable sector.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Initialize an observable sector:\n\n        &gt;&gt;&gt; ObservableSector(json_data)\n        '''\n        self.name = name\n        self.metadata = json_data['metadata']\n        self.data = {\n            k: {ast.literal_eval(kk): vv for kk, vv in v.items()}\n            for k, v in json_data['data'].items()\n        }\n\n        self.observable_names = self.metadata['observable_names']\n        self.polynomial_names = self.metadata.get('polynomial_names', None)\n        self.observable_expressions = self.metadata.get('observable_expressions', None)\n\n        observable_central = self.data.get('observable_central')\n        self.keys_coeff_observable = sorted(observable_central.keys())\n        self.observable_central = jnp.array(np.array([observable_central[k] for k in self.keys_coeff_observable]))\n\n        observable_uncertainties = self.data.get('observable_uncertainties', None)\n        self.observable_uncertainties = np.array([observable_uncertainties[k] for k in self.keys_coeff_observable]) if observable_uncertainties else None\n        self.observable_uncertainties_SM = self.observable_uncertainties[0].copy() if observable_uncertainties else None\n\n        polynomial_central = self.data.get('polynomial_central', None)\n        self.keys_coeff_polynomial = sorted(polynomial_central.keys()) if polynomial_central else None\n        self.polynomial_central = jnp.array(np.array([polynomial_central[k] for k in self.keys_coeff_polynomial])) if self.keys_coeff_polynomial else None\n\n        self.parameters = self.metadata['parameters']\n        self.scale = self.metadata['scale']\n        if isinstance(self.scale, list):\n            raise NotImplementedError(\n                f\"The current version of ObservableSector does not support lists of scales.\\n\"\n                f\"Please use a single scale for {self.name}.\\n\"\n            )\n        wcxf = self.metadata['basis'].get('wcxf')\n        if wcxf:\n            self.eft = wcxf['eft']\n            self.basis = wcxf['basis']\n            self.custom_basis = None\n            self.sectors = sorted(chain.from_iterable(\n                supersectors.get(sector, [sector])\n                for sector in wcxf['sectors']\n                ))\n            if self.basis in bases_installed.get(self.eft, []):\n                self.basis_mode = 'rgevolve'\n                _parameter_basis = {sector: get_wc_basis(eft=self.eft, basis=self.basis, sector=sector)\n                                    for sector in self.sectors}\n            else:\n                self.basis_mode = 'wcxf'\n                _parameter_basis = {sector: get_wc_basis_from_wcxf(eft=self.eft, basis=self.basis, sector=sector)\n                                    for sector in self.sectors}\n                if self.basis in bases_available.get(self.eft, []):\n                    warnings.warn(\n                        f\"\\nRG evolution matrices for the {self.basis} basis in {self.eft} are not installed. \"\n                        f\"Falling back to fixed-scale mode: predictions will only be available at the fixed scale of {self.scale} GeV. \"\n                        f\"You can enable RG evolution by installing the corresponding module:\\n\"\n                        f\"    pip install rgevolve.{normalize(self.eft)}.{normalize(self.basis)}\",\n                        UserWarning,\n                        stacklevel=2\n                        )\n                else:\n                    warnings.warn(\n                        f\"\\nRG evolution matrices for the {self.basis} basis in {self.eft} are not available. \"\n                        f\"Falling back to fixed-scale mode: predictions will only be available at the fixed scale of {self.scale} GeV.\",\n                        UserWarning,\n                        stacklevel=2\n                        )\n            self.keys_pars_by_sectors = [\n                tuple(\n                    par_name\n                    for par_name in _parameter_basis[sector]\n                    if par_name[0] in self.parameters\n                ) for sector in self.sectors\n                ]\n\n            if self.basis_mode == 'rgevolve':\n                self.sector_indices = {\n                    eft: {\n                        basis: get_sector_indices(\n                            eft, basis,\n                            sectors = (\n                                sorted({matching_sectors[sector] for sector in self.sectors})\n                                if eft == 'SMEFT' and self.eft != 'SMEFT' else self.sectors\n                            )\n                        )\n                        for basis in bases_installed.get(eft, [])\n                    } for eft in efts_available.get(self.eft, [])\n                }\n                self.evolution_matrices = {\n                    eft: {\n                        basis: self._get_evolution_matrices(eft, basis)\n                        for basis in bases_installed.get(eft, [])\n                    } for eft in efts_available.get(self.eft, [])\n                }\n            else:\n                self.sector_indices = {\n                    self.eft: {\n                        self.basis: get_sector_indices_from_wcxf(\n                            self.eft, self.basis, self.sectors\n                        )\n                    }\n                }\n                shapes_in = [len(get_wc_basis_from_wcxf(self.eft, self.basis, sector)) for sector in self.sectors]\n                shapes_out = [len(keys_pars) for keys_pars in self.keys_pars_by_sectors]\n                self.evolution_matrices = {\n                    self.eft: {\n                        self.basis: self._get_unit_evolution_matrices(\n                            shapes_in, shapes_out, 1\n                        )\n                    }\n                }\n        else:\n            name = self.metadata['basis']['custom']['name']\n            custom_basis = CustomBasis.get(name)\n            if custom_basis:\n                self.eft = None\n                self.basis = None\n                self.custom_basis = name\n                self.sectors = [None]\n                _parameter_basis = custom_basis.get_parameter_basis()\n                self.basis_mode = 'custom'\n                self.keys_pars_by_sectors = [\n                    tuple(\n                        parameter_name\n                        for parameter_name in _parameter_basis\n                        if parameter_name[0] in self.parameters\n                    )\n                    ]\n                self.sector_indices = {\n                    None: {\n                        None: np.arange(len(_parameter_basis))\n                    }\n                }\n                shapes_in = [len(_parameter_basis)]\n                shapes_out = [len(self.keys_pars_by_sectors[0])]\n                self.evolution_matrices = {\n                    None: {\n                        None: self._get_unit_evolution_matrices(\n                            shapes_in, shapes_out, 1\n                        )\n                    }\n                }\n                warnings.warn(\n                    f\"\\nRG evolution matrices for the custom basis {self.custom_basis} are not available. \"\n                    f\"Falling back to fixed-scale mode: predictions will only be available at the fixed scale of {self.scale} GeV.\",\n                    UserWarning,\n                    stacklevel=2\n                )\n            else:\n                raise ValueError(\n                    f\"Basis {name} not found in CustomBasis. Please define it using `CustomBasis({name}, parameters)`.\"\n                )\n\n        self.keys_pars = [('', 'R')] + list(chain.from_iterable(self.keys_pars_by_sectors))\n\n        self.construct_par_monomials_observable = self._get_construct_par_monomials(self.keys_coeff_observable)\n        self.construct_par_monomials_polynomial = self._get_construct_par_monomials(self.keys_coeff_polynomial)\n\n        self.observable_expression_functions = [\n            self._get_observable_expression_function(i)\n            for i in range(len(self.observable_names))\n        ] if self.observable_expressions else None\n\n        self.prediction = self._get_prediction_function()\n\n        # Add observable sector to `_observable_sectors` class attribute\n        self._observable_sectors[self.name] = self\n\n    @classmethod\n    def load(cls, path: str) -&gt; None:\n        '''\n        Load an observable sector from a json file or several observable sectors from a directory containing json files.\n\n        Parameters\n        ----------\n        path : str\n            Path to a json file or a directory containing json files.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Load an observable sector from a json file:\n\n        &gt;&gt;&gt; ObservableSector.load('./observable_sector.json')\n\n        Load all observable sectors from a directory containing json files:\n\n        &gt;&gt;&gt; ObservableSector.load('./observable_sectors/')\n        '''\n        # load all json files in the directory\n        if os.path.isdir(path):\n            for file in os.listdir(path):\n                if file.endswith('.json'):\n                    cls._load_file(os.path.join(path, file))\n        # load single json file\n        else:\n            cls._load_file(path)\n\n    @classmethod\n    def _load_file(cls, path: str) -&gt; None:\n        '''\n        Load an observable sector from a json file.\n\n        Parameters\n        ----------\n        path : str\n            Path to a json file.\n\n        Returns\n        -------\n        None\n        '''\n        path = Path(path)\n        with path.open('r') as f:\n            json_data = json.load(f)\n        schema_name, schema_version = get_json_schema(json_data)\n        if schema_name == 'popxf' and schema_version in cls._popxf_versions:\n            cls(path.stem, json_data)\n\n\n    def get_prediction_data(self, eft: str, basis: str) -&gt; List[jnp.array]:\n        '''\n        Get the data needed to make a prediction for a given EFT and basis.\n\n        Parameters\n        ----------\n        eft : str\n            The EFT to make the prediction in.\n        basis : str\n            The basis to make the prediction in.\n\n        Returns\n        -------\n        list\n            A list containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients.\n\n        Examples\n        --------\n        Get the data needed to make a prediction in the Warsaw basis of the SMEFT:\n\n        &gt;&gt;&gt; observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n\n        Get the data needed to make a prediction in the flavio basis of the WET:\n\n        &gt;&gt;&gt; observable_sector.get_prediction_data('WET', 'flavio')\n        '''\n        return [\n            jnp.array(self.sector_indices[eft][basis]),\n            jnp.array(self.evolution_matrices[eft][basis], dtype=jnp.float32),\n            jnp.concatenate([\n                jnp.array(self.get_scales(eft, basis), dtype=jnp.float32),\n                jnp.array([jnp.nan], dtype=jnp.float32) # Add NaN to handle out-of-range cases\n            ]),\n            jnp.array(\n                self.observable_central if self.observable_expressions is None\n                else self.polynomial_central,\n                dtype=jnp.float64\n            ),\n        ]\n\n    def get_scales(self, eft: str, basis: str) -&gt; np.ndarray:\n        '''\n        Get the scales at which the Renormalization Group evolution matrices are defined for a given EFT and basis.\n\n        Parameters\n        ----------\n        eft : str\n            The EFT to get the Renormalization Group scales for.\n        basis : str\n            The basis to get the Renormalization Group scales for.\n\n        Returns\n        -------\n        np.ndarray\n            The scales at which the Renormalization Group evolution matrices are defined.\n        '''\n        if self.basis_mode == 'rgevolve':\n            return get_scales(eft, basis)\n        else:\n            return np.array([self.scale])\n\n    def _get_evolution_matrices(self, eft: str, basis: str) -&gt; np.ndarray:\n        '''\n        Get the Renormalization Group evolution matrices for a given EFT and basis.\n\n        Parameters\n        ----------\n        eft : str\n            The EFT to get the Renormalization Group evolution matrices for.\n        basis : str\n            The basis to get the Renormalization Group evolution matrices for.\n\n        Returns\n        -------\n        np.ndarray\n            The Renormalization Group evolution matrices.\n        '''\n        pars_out = dict(zip(self.sectors, self.keys_pars_by_sectors))\n        scales_in = get_scales(eft, basis)\n        if eft == 'SMEFT' and self.eft != 'SMEFT':\n            sectors_in = sorted({matching_sectors[sector] for sector in self.sectors})\n            shapes_in = [len(get_wc_basis(eft, basis, sector)) for sector in sectors_in]\n            shapes_out = [len(keys_pars) for keys_pars in self.keys_pars_by_sectors]\n        matrices_scales = []\n        for scale_in in scales_in:\n            matrices_sectors = []\n            for sector_out in self.sectors:\n                matrix_sector = run_and_match(\n                    eft_in=eft, eft_out=self.eft,\n                    basis_in=basis, basis_out=self.basis,\n                    scale_in=scale_in, scale_out=self.scale,\n                    sector_out=sector_out,\n                )\n                par_mask = get_wc_mask(self.eft, self.basis, sector_out, pars_out[sector_out])\n                matrices_sectors.append(matrix_sector[par_mask])\n            if eft == 'SMEFT' and self.eft != 'SMEFT':\n                matrix_scale = np.block([\n                    [\n                        matrices_sectors[i]\n                        if matching_sectors.get(self.sectors[i]) == sectors_in[j]\n                        else np.zeros((shapes_out[i], shapes_in[j]))\n                        for j in range(len(sectors_in))\n                    ]\n                    for i in range(len(self.sectors))\n                ])\n            else:\n                matrix_scale = scipy.linalg.block_diag(*matrices_sectors)\n            matrices_scales.append(matrix_scale)\n        return np.array(matrices_scales)\n\n    def _get_unit_evolution_matrices(self, shapes_in: List[int], shapes_out: List[int], n: int) -&gt; np.ndarray:\n        '''\n        Constructs a list of block-diagonal matrices composed of identity matrices.\n        Each identity matrix has shape (shapes_out[i], shapes_in[i]).\n\n        Parameters\n        ----------\n        shapes_in : list\n            List of input dimensions for each block.\n        shapes_out : list\n            List of output dimensions for each block.\n        n : int\n            Number of matrices to create.\n\n        Returns:\n        np.ndarray\n            An array of shape (n, ..., ...) containing block-diagonal matrices.\n\n        '''\n        matrices = []\n        for _ in range(n):\n            blocks = [np.eye(out_dim, in_dim) for in_dim, out_dim in zip(shapes_in, shapes_out)]\n            matrices.append(scipy.linalg.block_diag(*blocks))\n        return np.array(matrices)\n\n    def _get_prediction_function(self) -&gt; Callable[\n        [jnp.ndarray, Union[float, int, jnp.ndarray], List[jnp.ndarray]],\n        Tuple[jnp.ndarray, jnp.ndarray],\n    ]:\n        '''\n        Get the function that makes a prediction for the observable sector.\n\n        Returns\n        -------\n        function\n            The function that makes a prediction for the observable sector.\n\n            Parameters\n            ----------\n            par_array : jnp.ndarray\n                The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.\n\n            scale : float, int, or jnp.ndarray\n                The scale at which to make the prediction. If `par_array` has batch dimensions, `scale` can be a scalar or an array with the same shape as the batch dimensions of `par_array`. If `par_array` has no batch dimensions, `scale` can be a scalar or an array.\n\n            prediction_data : list\n                A list containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients.\n\n            Returns\n            -------\n            jnp.ndarray\n                The observable predictions.\n\n            jnp.ndarray\n                The parameter monomials corresponding to the `observable_central` polynomial coefficients. These are used to compute the parameter dependence of the theoretical uncertainties.\n\n        Examples\n        --------\n        Get the prediction function for the observable sector:\n\n        &gt;&gt;&gt; prediction = observable_sector.prediction\n\n        Make a prediction for the observable sector:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n        &gt;&gt;&gt; scale = 1000\n        &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n        Make a prediction for the observable sector with batch dimensions in `par_array` and a scalar `scale`:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n        &gt;&gt;&gt; scale = 1000\n        &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n        Make a prediction for the observable sector with no batch dimensions in `par_array` and an array `scale`:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n        &gt;&gt;&gt; scale = jnp.array([1000, 2000])\n        &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n        Make a prediction for the observable sector with both `par_array` and `scale` having batch dimensions:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n        &gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n        &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n        '''\n        if self.observable_expressions is None:\n\n            # Define the prediction function for the case where there are no observable expressions\n            # In this case, the prediction is just the polynomial prediction\n            def prediction(\n                par_array: jnp.array, scale: Union[float, int, jnp.array],\n                prediction_data: List[jnp.array]\n            ) -&gt; Tuple[jnp.array, jnp.array]:\n                sector_indices, evolution_matrices, evolution_scales, polynomial_coefficients = prediction_data\n                par_array_sector = jnp.take(par_array, sector_indices, axis=-1)\n                par_array_evolved = interpolate_rg_evolution(\n                    par_array_sector, scale, evolution_matrices, evolution_scales\n                )\n                par_monomials = self.construct_par_monomials_observable(par_array_evolved)\n                polynomial_predictions = jnp.dot(par_monomials, polynomial_coefficients)\n                return polynomial_predictions, par_monomials\n\n        else:\n\n            # Define the prediction function for the case where there are observable expressions\n            # In this case, the prediction is the observable prediction evaluated in terms of the polynomial prediction\n            def prediction(\n                par_array: jnp.array, scale: Union[float, int, jnp.array],\n                prediction_data: List[jnp.array]\n            ) -&gt; Tuple[jnp.array, jnp.array]:\n                sector_indices, evolution_matrices, evolution_scales, polynomial_coefficients = prediction_data\n                par_array_sector = jnp.take(par_array, sector_indices, axis=-1)\n                par_array_evolved = interpolate_rg_evolution(\n                    par_array_sector, scale, evolution_matrices, evolution_scales\n                )\n                par_monomials = self.construct_par_monomials_polynomial(par_array_evolved)\n                par_monomials_expansion = self.construct_par_monomials_observable(par_array_evolved)\n                polynomial_predictions = jnp.dot(par_monomials, polynomial_coefficients)\n                observable_predictions = jnp.asarray([\n                    observable_expression_function(polynomial_predictions)\n                    for observable_expression_function in self.observable_expression_functions\n                ])\n                return jnp.moveaxis(observable_predictions, 0, -1), par_monomials_expansion\n\n        return prediction\n\n    def _get_construct_par_monomials(self, keys_coeff: List[tuple]) -&gt; Callable[[jnp.ndarray], jnp.ndarray]:\n        '''\n        Get the function that constructs the parameter monomials from the parameter array.\n\n        Parameters\n        ----------\n        keys_coeff : list\n            The keys of the polynomial coefficients.\n\n        Returns\n        -------\n        function\n            The function that constructs the parameter monomials from the parameter array.\n\n            Parameters\n            ----------\n            par_array : jnp.ndarray\n                The parameter array.\n\n            Returns\n            -------\n            jnp.ndarray\n                The parameter monomials.\n        '''\n        if not keys_coeff:\n            return None\n        par_monomial_indices = get_par_monomial_indices(self.keys_pars, keys_coeff)\n\n        def construct_par_monomials(par_array: jnp.ndarray) -&gt; jnp.ndarray:\n\n            # insert 1 (in a batch-friendly way) to account for SM term\n            ones_column = jnp.ones((*par_array.shape[:-1], 1))\n            par_array = jnp.concatenate([ones_column, par_array], axis=-1)\n\n            par_monomial = batched_outer_ravel(par_array)\n            return jnp.take(par_monomial, par_monomial_indices, axis=-1)\n        return construct_par_monomials\n\n    def _get_observable_expression_function(self, i: int) -&gt; Callable[[jnp.ndarray], Union[float, jnp.ndarray]]:\n        '''\n        Get the function that evaluates a given observable expression in terms of the polynomial predictions.\n\n        Parameters\n        ----------\n        i : int\n            The index of the observable expression.\n\n        Returns\n        -------\n        function\n            The function that evaluates the observable expression in terms of the polynomial predictions.\n\n            Parameters\n            ----------\n            polynomial_predictions : jnp.ndarray\n                The polynomial predictions.\n\n            Returns\n            -------\n            float or jnp.ndarray\n                The value of the observable expression evaluated in terms of the polynomial predictions.\n        '''\n\n        # Create a function from the observable expression string\n        s = (\n            'from jax.numpy import sqrt\\n'\n            'def observable_expression(terms):\\n'\n            '    {}, = terms\\n'\n            '    return {}'\n        ).format(\n            ', '.join(self.observable_expressions[i]['terms'].keys()),\n            self.observable_expressions[i]['expression'],\n        )\n        namespace = OrderedDict()\n        exec(s, namespace)\n        observable_expression = namespace.popitem()[1]\n\n        # Create the observable expression function that takes the polynomial predictions as input\n        polynomial_indices = jnp.array([\n            self.polynomial_names.index(v)\n            for v in self.observable_expressions[i]['terms'].values()\n        ])\n        def observable_expression_function(polynomial_predictions: jnp.ndarray) -&gt; Union[float, jnp.ndarray]:\n            selected_polynomial_predictions = jnp.take(polynomial_predictions, polynomial_indices, axis=-1)\n\n            # Move the last dimension to the first dimension to allow for unpacking of the terms in batch mode\n            return observable_expression(jnp.moveaxis(selected_polynomial_predictions, -1, 0))\n\n        return observable_expression_function\n\n    @classmethod\n    def get_class_prediction_data(cls, eft: str, basis: str, observable_sector_names: List[str]) -&gt; List[jnp.array]:\n        '''\n        Get the data needed to make a prediction for a list of observable sectors.\n\n        Parameters\n        ----------\n        eft : str\n            The EFT to make the prediction in.\n        basis : str\n            The basis to make the prediction in.\n        observable_sector_names : list\n            The names of the observable sectors to make the prediction for.\n\n        Returns\n        -------\n        list\n            A list of lists containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients for each observable sector.\n\n        Examples\n        --------\n        Get the data needed to make a prediction in the Warsaw basis of the SMEFT for a list of observable sectors:\n\n        &gt;&gt;&gt; ObservableSector.get_all_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n\n        Get the data needed to make a prediction in the flavio basis of the WET for a list of observable sectors:\n\n        &gt;&gt;&gt; ObservableSector.get_all_prediction_data('WET', 'flavio', ['observable_sector_1', 'observable_sector_2'])\n        '''\n        return [\n            cls._observable_sectors[name].get_prediction_data(eft, basis)\n            for name in observable_sector_names\n        ]\n\n    @classmethod\n    def get_class_prediction_function(cls, observable_sector_names: List[str]) -&gt; Callable[\n        [jnp.ndarray, Union[float, int, jnp.ndarray], List[jnp.ndarray]],\n        jnp.ndarray\n    ]:\n        '''\n        Get the function that makes a prediction for a list of observable sectors.\n\n        Parameters\n        ----------\n        observable_sector_names : list\n            The names of the observable sectors to make the prediction for.\n\n        Returns\n        -------\n        function\n            The function that makes a prediction for a list of observable sectors.\n\n            Parameters\n            ----------\n              - `par_array : jnp.ndarray`\n                The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.\n\n              - `scale : float, int, or jnp.ndarray`\n                The scale at which to make the prediction. If `par_array` has batch dimensions, `scale` can be a scalar or an array with the same shape as the batch dimensions of `par_array`. If `par_array` has no batch dimensions, `scale` can be a scalar or an array.\n\n              - `prediction_data : list`\n                A list of lists containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients for each observable sector.\n\n            Returns\n            -------\n            `jnp.ndarray`\n                The observable predictions.\n\n        Examples\n        --------\n        Get the prediction function for a list of observable sectors:\n\n        &gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n\n        Make a prediction for a list of observable sectors:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n        &gt;&gt;&gt; scale = 1000\n        &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n        Make a prediction for a list of observable sectors with batch dimensions in `par_array` and a scalar `scale`:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n        &gt;&gt;&gt; scale = 1000\n        &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n        Make a prediction for a list of observable sectors with no batch dimensions in `par_array` and an array `scale`:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n        &gt;&gt;&gt; scale = jnp.array([1000, 2000])\n        &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n        Make a prediction for a list of observable sectors with both `par_array` and `scale` having batch dimensions:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n        &gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n        &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n        '''\n        prediction_functions = [\n            cls._observable_sectors[name].prediction\n            for name in observable_sector_names\n        ]\n\n        def prediction(\n            par_array: jnp.array, scale: Union[float, int, jnp.array],\n            prediction_data: List[List[jnp.array]]\n        ) -&gt; jnp.array:\n            return jnp.concatenate([\n                prediction_function(par_array, scale, data)[0]\n                for prediction_function, data in zip(prediction_functions, prediction_data)\n            ], axis=-1)\n\n        return prediction\n\n    @classmethod\n    def get_all_names(cls, eft: Optional[str] = None, basis: Optional[str] = None, custom_basis: Optional[str]=None) -&gt; List[str]:\n        '''\n        Get the names of all observable sectors.\n\n        Parameters\n        ----------\n        eft : str, optional\n            The EFT for which the observable sectors can provide predictions.\n        basis : str, optional\n            The basis for which the observable sectors can provide predictions\n        custom_basis : str, optional\n            The custom basis for which the observable sectors can provide predictions.\n\n        Notes\n        -----\n        If all parameters are None, all observable sectors are returned.\n\n        Returns\n        -------\n        list\n            The names of all observable sectors.\n\n        Examples\n        --------\n        Get the names of all observable sectors:\n        &gt;&gt;&gt; ObservableSector.get_all_names()\n\n        Get the names of all observable sectors that can provide predictions in the `SMEFT` basis `Warsaw`:\n        &gt;&gt;&gt; ObservableSector.get_all_names('SMEFT', 'Warsaw')\n\n        Get the names of all observable sectors that can provide predictions in the `WET` basis `flavio`:\n        &gt;&gt;&gt; ObservableSector.get_all_names('WET', 'flavio')\n\n        Get the names of all observable sectors that can provide predictions in the custom basis `custom_basis`:\n        &gt;&gt;&gt; ObservableSector.get_all_names(custom_basis='custom_basis')\n        '''\n\n        if custom_basis is not None:\n            if eft is not None or basis is not None:\n                raise ValueError(\n                    'The custom_basis parameter cannot be used together with the eft or basis parameters.'\n                )\n            return sorted(\n                name\n                for name, observable_sector in cls._observable_sectors.items()\n                if observable_sector.custom_basis == custom_basis\n            )\n        elif eft is not None:\n            if basis is not None:\n                observable_sectors_wcxf = sorted(name for name, observable_sector in cls._observable_sectors.items()\n                                                 if observable_sector.basis_mode == 'wcxf'\n                                                 and observable_sector.basis == basis\n                                                 and observable_sector.eft == eft)\n                if observable_sectors_wcxf:\n                    return observable_sectors_wcxf\n                else:\n                    if basis in bases_installed.get(eft, []):\n                        return sorted(\n                            name\n                            for name, observable_sector in cls._observable_sectors.items()\n                            if observable_sector.basis_mode == 'rgevolve'\n                            and eft in efts_available.get(observable_sector.eft, [])\n                            )\n                    else:\n                        return []\n            else:\n                raise ValueError(\n                    'The basis parameter is required when the eft parameter is provided.'\n                )\n        elif basis is not None:\n            raise ValueError(\n                'The basis parameter is only valid when the eft parameter is also provided.'\n            )\n        else:\n            return sorted(cls._observable_sectors.keys())\n\n    @classmethod\n    def get(cls, name: str) -&gt; 'ObservableSector':\n        '''\n        Get an observable sector by name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the observable sector.\n\n        Returns\n        -------\n        ObservableSector\n            The observable sector.\n\n        Examples\n        --------\n        Get an observable sector by name:\n\n        &gt;&gt;&gt; ObservableSector.get('observable_sector_1')\n        '''\n        return cls._observable_sectors[name]\n\n    @classmethod\n    def get_all(cls) -&gt; List['ObservableSector']:\n        '''\n        Get all observable sectors.\n\n        Returns\n        -------\n        list\n            A list of all observable sectors.\n\n        Examples\n        --------\n        Get all observable sectors:\n\n        &gt;&gt;&gt; ObservableSector.get_all()\n        '''\n        return [cls._observable_sectors[name] for name in cls.get_all_names()]\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.__init__","title":"<code>__init__(name, json_data)</code>","text":"<p>Initialize an observable sector.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the observable sector.</p> required <code>json_data</code> <code>dict</code> <p>The JSON data containing the metadata and data of the observable sector.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Initialize an observable sector:</p> <pre><code>&gt;&gt;&gt; ObservableSector(json_data)\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def __init__(self, name: str,  json_data: Dict[str, Any])-&gt; None:\n    '''\n    Initialize an observable sector.\n\n    Parameters\n    ----------\n    name : str\n        The name of the observable sector.\n    json_data : dict\n        The JSON data containing the metadata and data of the observable sector.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Initialize an observable sector:\n\n    &gt;&gt;&gt; ObservableSector(json_data)\n    '''\n    self.name = name\n    self.metadata = json_data['metadata']\n    self.data = {\n        k: {ast.literal_eval(kk): vv for kk, vv in v.items()}\n        for k, v in json_data['data'].items()\n    }\n\n    self.observable_names = self.metadata['observable_names']\n    self.polynomial_names = self.metadata.get('polynomial_names', None)\n    self.observable_expressions = self.metadata.get('observable_expressions', None)\n\n    observable_central = self.data.get('observable_central')\n    self.keys_coeff_observable = sorted(observable_central.keys())\n    self.observable_central = jnp.array(np.array([observable_central[k] for k in self.keys_coeff_observable]))\n\n    observable_uncertainties = self.data.get('observable_uncertainties', None)\n    self.observable_uncertainties = np.array([observable_uncertainties[k] for k in self.keys_coeff_observable]) if observable_uncertainties else None\n    self.observable_uncertainties_SM = self.observable_uncertainties[0].copy() if observable_uncertainties else None\n\n    polynomial_central = self.data.get('polynomial_central', None)\n    self.keys_coeff_polynomial = sorted(polynomial_central.keys()) if polynomial_central else None\n    self.polynomial_central = jnp.array(np.array([polynomial_central[k] for k in self.keys_coeff_polynomial])) if self.keys_coeff_polynomial else None\n\n    self.parameters = self.metadata['parameters']\n    self.scale = self.metadata['scale']\n    if isinstance(self.scale, list):\n        raise NotImplementedError(\n            f\"The current version of ObservableSector does not support lists of scales.\\n\"\n            f\"Please use a single scale for {self.name}.\\n\"\n        )\n    wcxf = self.metadata['basis'].get('wcxf')\n    if wcxf:\n        self.eft = wcxf['eft']\n        self.basis = wcxf['basis']\n        self.custom_basis = None\n        self.sectors = sorted(chain.from_iterable(\n            supersectors.get(sector, [sector])\n            for sector in wcxf['sectors']\n            ))\n        if self.basis in bases_installed.get(self.eft, []):\n            self.basis_mode = 'rgevolve'\n            _parameter_basis = {sector: get_wc_basis(eft=self.eft, basis=self.basis, sector=sector)\n                                for sector in self.sectors}\n        else:\n            self.basis_mode = 'wcxf'\n            _parameter_basis = {sector: get_wc_basis_from_wcxf(eft=self.eft, basis=self.basis, sector=sector)\n                                for sector in self.sectors}\n            if self.basis in bases_available.get(self.eft, []):\n                warnings.warn(\n                    f\"\\nRG evolution matrices for the {self.basis} basis in {self.eft} are not installed. \"\n                    f\"Falling back to fixed-scale mode: predictions will only be available at the fixed scale of {self.scale} GeV. \"\n                    f\"You can enable RG evolution by installing the corresponding module:\\n\"\n                    f\"    pip install rgevolve.{normalize(self.eft)}.{normalize(self.basis)}\",\n                    UserWarning,\n                    stacklevel=2\n                    )\n            else:\n                warnings.warn(\n                    f\"\\nRG evolution matrices for the {self.basis} basis in {self.eft} are not available. \"\n                    f\"Falling back to fixed-scale mode: predictions will only be available at the fixed scale of {self.scale} GeV.\",\n                    UserWarning,\n                    stacklevel=2\n                    )\n        self.keys_pars_by_sectors = [\n            tuple(\n                par_name\n                for par_name in _parameter_basis[sector]\n                if par_name[0] in self.parameters\n            ) for sector in self.sectors\n            ]\n\n        if self.basis_mode == 'rgevolve':\n            self.sector_indices = {\n                eft: {\n                    basis: get_sector_indices(\n                        eft, basis,\n                        sectors = (\n                            sorted({matching_sectors[sector] for sector in self.sectors})\n                            if eft == 'SMEFT' and self.eft != 'SMEFT' else self.sectors\n                        )\n                    )\n                    for basis in bases_installed.get(eft, [])\n                } for eft in efts_available.get(self.eft, [])\n            }\n            self.evolution_matrices = {\n                eft: {\n                    basis: self._get_evolution_matrices(eft, basis)\n                    for basis in bases_installed.get(eft, [])\n                } for eft in efts_available.get(self.eft, [])\n            }\n        else:\n            self.sector_indices = {\n                self.eft: {\n                    self.basis: get_sector_indices_from_wcxf(\n                        self.eft, self.basis, self.sectors\n                    )\n                }\n            }\n            shapes_in = [len(get_wc_basis_from_wcxf(self.eft, self.basis, sector)) for sector in self.sectors]\n            shapes_out = [len(keys_pars) for keys_pars in self.keys_pars_by_sectors]\n            self.evolution_matrices = {\n                self.eft: {\n                    self.basis: self._get_unit_evolution_matrices(\n                        shapes_in, shapes_out, 1\n                    )\n                }\n            }\n    else:\n        name = self.metadata['basis']['custom']['name']\n        custom_basis = CustomBasis.get(name)\n        if custom_basis:\n            self.eft = None\n            self.basis = None\n            self.custom_basis = name\n            self.sectors = [None]\n            _parameter_basis = custom_basis.get_parameter_basis()\n            self.basis_mode = 'custom'\n            self.keys_pars_by_sectors = [\n                tuple(\n                    parameter_name\n                    for parameter_name in _parameter_basis\n                    if parameter_name[0] in self.parameters\n                )\n                ]\n            self.sector_indices = {\n                None: {\n                    None: np.arange(len(_parameter_basis))\n                }\n            }\n            shapes_in = [len(_parameter_basis)]\n            shapes_out = [len(self.keys_pars_by_sectors[0])]\n            self.evolution_matrices = {\n                None: {\n                    None: self._get_unit_evolution_matrices(\n                        shapes_in, shapes_out, 1\n                    )\n                }\n            }\n            warnings.warn(\n                f\"\\nRG evolution matrices for the custom basis {self.custom_basis} are not available. \"\n                f\"Falling back to fixed-scale mode: predictions will only be available at the fixed scale of {self.scale} GeV.\",\n                UserWarning,\n                stacklevel=2\n            )\n        else:\n            raise ValueError(\n                f\"Basis {name} not found in CustomBasis. Please define it using `CustomBasis({name}, parameters)`.\"\n            )\n\n    self.keys_pars = [('', 'R')] + list(chain.from_iterable(self.keys_pars_by_sectors))\n\n    self.construct_par_monomials_observable = self._get_construct_par_monomials(self.keys_coeff_observable)\n    self.construct_par_monomials_polynomial = self._get_construct_par_monomials(self.keys_coeff_polynomial)\n\n    self.observable_expression_functions = [\n        self._get_observable_expression_function(i)\n        for i in range(len(self.observable_names))\n    ] if self.observable_expressions else None\n\n    self.prediction = self._get_prediction_function()\n\n    # Add observable sector to `_observable_sectors` class attribute\n    self._observable_sectors[self.name] = self\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get","title":"<code>get(name)</code>  <code>classmethod</code>","text":"<p>Get an observable sector by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the observable sector.</p> required <p>Returns:</p> Type Description <code>ObservableSector</code> <p>The observable sector.</p> <p>Examples:</p> <p>Get an observable sector by name:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get('observable_sector_1')\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef get(cls, name: str) -&gt; 'ObservableSector':\n    '''\n    Get an observable sector by name.\n\n    Parameters\n    ----------\n    name : str\n        The name of the observable sector.\n\n    Returns\n    -------\n    ObservableSector\n        The observable sector.\n\n    Examples\n    --------\n    Get an observable sector by name:\n\n    &gt;&gt;&gt; ObservableSector.get('observable_sector_1')\n    '''\n    return cls._observable_sectors[name]\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_all","title":"<code>get_all()</code>  <code>classmethod</code>","text":"<p>Get all observable sectors.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of all observable sectors.</p> <p>Examples:</p> <p>Get all observable sectors:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all()\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef get_all(cls) -&gt; List['ObservableSector']:\n    '''\n    Get all observable sectors.\n\n    Returns\n    -------\n    list\n        A list of all observable sectors.\n\n    Examples\n    --------\n    Get all observable sectors:\n\n    &gt;&gt;&gt; ObservableSector.get_all()\n    '''\n    return [cls._observable_sectors[name] for name in cls.get_all_names()]\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_all_names","title":"<code>get_all_names(eft=None, basis=None, custom_basis=None)</code>  <code>classmethod</code>","text":"<p>Get the names of all observable sectors.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The EFT for which the observable sectors can provide predictions.</p> <code>None</code> <code>basis</code> <code>str</code> <p>The basis for which the observable sectors can provide predictions</p> <code>None</code> <code>custom_basis</code> <code>str</code> <p>The custom basis for which the observable sectors can provide predictions.</p> <code>None</code> Notes <p>If all parameters are None, all observable sectors are returned.</p> <p>Returns:</p> Type Description <code>list</code> <p>The names of all observable sectors.</p> <p>Examples:</p> <p>Get the names of all observable sectors:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names()\n</code></pre> <p>Get the names of all observable sectors that can provide predictions in the <code>SMEFT</code> basis <code>Warsaw</code>:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names('SMEFT', 'Warsaw')\n</code></pre> <p>Get the names of all observable sectors that can provide predictions in the <code>WET</code> basis <code>flavio</code>:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names('WET', 'flavio')\n</code></pre> <p>Get the names of all observable sectors that can provide predictions in the custom basis <code>custom_basis</code>:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names(custom_basis='custom_basis')\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef get_all_names(cls, eft: Optional[str] = None, basis: Optional[str] = None, custom_basis: Optional[str]=None) -&gt; List[str]:\n    '''\n    Get the names of all observable sectors.\n\n    Parameters\n    ----------\n    eft : str, optional\n        The EFT for which the observable sectors can provide predictions.\n    basis : str, optional\n        The basis for which the observable sectors can provide predictions\n    custom_basis : str, optional\n        The custom basis for which the observable sectors can provide predictions.\n\n    Notes\n    -----\n    If all parameters are None, all observable sectors are returned.\n\n    Returns\n    -------\n    list\n        The names of all observable sectors.\n\n    Examples\n    --------\n    Get the names of all observable sectors:\n    &gt;&gt;&gt; ObservableSector.get_all_names()\n\n    Get the names of all observable sectors that can provide predictions in the `SMEFT` basis `Warsaw`:\n    &gt;&gt;&gt; ObservableSector.get_all_names('SMEFT', 'Warsaw')\n\n    Get the names of all observable sectors that can provide predictions in the `WET` basis `flavio`:\n    &gt;&gt;&gt; ObservableSector.get_all_names('WET', 'flavio')\n\n    Get the names of all observable sectors that can provide predictions in the custom basis `custom_basis`:\n    &gt;&gt;&gt; ObservableSector.get_all_names(custom_basis='custom_basis')\n    '''\n\n    if custom_basis is not None:\n        if eft is not None or basis is not None:\n            raise ValueError(\n                'The custom_basis parameter cannot be used together with the eft or basis parameters.'\n            )\n        return sorted(\n            name\n            for name, observable_sector in cls._observable_sectors.items()\n            if observable_sector.custom_basis == custom_basis\n        )\n    elif eft is not None:\n        if basis is not None:\n            observable_sectors_wcxf = sorted(name for name, observable_sector in cls._observable_sectors.items()\n                                             if observable_sector.basis_mode == 'wcxf'\n                                             and observable_sector.basis == basis\n                                             and observable_sector.eft == eft)\n            if observable_sectors_wcxf:\n                return observable_sectors_wcxf\n            else:\n                if basis in bases_installed.get(eft, []):\n                    return sorted(\n                        name\n                        for name, observable_sector in cls._observable_sectors.items()\n                        if observable_sector.basis_mode == 'rgevolve'\n                        and eft in efts_available.get(observable_sector.eft, [])\n                        )\n                else:\n                    return []\n        else:\n            raise ValueError(\n                'The basis parameter is required when the eft parameter is provided.'\n            )\n    elif basis is not None:\n        raise ValueError(\n            'The basis parameter is only valid when the eft parameter is also provided.'\n        )\n    else:\n        return sorted(cls._observable_sectors.keys())\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_class_prediction_data","title":"<code>get_class_prediction_data(eft, basis, observable_sector_names)</code>  <code>classmethod</code>","text":"<p>Get the data needed to make a prediction for a list of observable sectors.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The EFT to make the prediction in.</p> required <code>basis</code> <code>str</code> <p>The basis to make the prediction in.</p> required <code>observable_sector_names</code> <code>list</code> <p>The names of the observable sectors to make the prediction for.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of lists containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients for each observable sector.</p> <p>Examples:</p> <p>Get the data needed to make a prediction in the Warsaw basis of the SMEFT for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n</code></pre> <p>Get the data needed to make a prediction in the flavio basis of the WET for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_prediction_data('WET', 'flavio', ['observable_sector_1', 'observable_sector_2'])\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef get_class_prediction_data(cls, eft: str, basis: str, observable_sector_names: List[str]) -&gt; List[jnp.array]:\n    '''\n    Get the data needed to make a prediction for a list of observable sectors.\n\n    Parameters\n    ----------\n    eft : str\n        The EFT to make the prediction in.\n    basis : str\n        The basis to make the prediction in.\n    observable_sector_names : list\n        The names of the observable sectors to make the prediction for.\n\n    Returns\n    -------\n    list\n        A list of lists containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients for each observable sector.\n\n    Examples\n    --------\n    Get the data needed to make a prediction in the Warsaw basis of the SMEFT for a list of observable sectors:\n\n    &gt;&gt;&gt; ObservableSector.get_all_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n\n    Get the data needed to make a prediction in the flavio basis of the WET for a list of observable sectors:\n\n    &gt;&gt;&gt; ObservableSector.get_all_prediction_data('WET', 'flavio', ['observable_sector_1', 'observable_sector_2'])\n    '''\n    return [\n        cls._observable_sectors[name].get_prediction_data(eft, basis)\n        for name in observable_sector_names\n    ]\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_class_prediction_function","title":"<code>get_class_prediction_function(observable_sector_names)</code>  <code>classmethod</code>","text":"<p>Get the function that makes a prediction for a list of observable sectors.</p> <p>Parameters:</p> Name Type Description Default <code>observable_sector_names</code> <code>list</code> <p>The names of the observable sectors to make the prediction for.</p> required <p>Returns:</p> Type Description <code>function</code> <p>The function that makes a prediction for a list of observable sectors.</p> <p>Examples:</p> <p>Get the prediction function for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n</code></pre> <p>Make a prediction for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> <p>Make a prediction for a list of observable sectors with batch dimensions in <code>par_array</code> and a scalar <code>scale</code>:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> <p>Make a prediction for a list of observable sectors with no batch dimensions in <code>par_array</code> and an array <code>scale</code>:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = jnp.array([1000, 2000])\n&gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> <p>Make a prediction for a list of observable sectors with both <code>par_array</code> and <code>scale</code> having batch dimensions:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n&gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n&gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef get_class_prediction_function(cls, observable_sector_names: List[str]) -&gt; Callable[\n    [jnp.ndarray, Union[float, int, jnp.ndarray], List[jnp.ndarray]],\n    jnp.ndarray\n]:\n    '''\n    Get the function that makes a prediction for a list of observable sectors.\n\n    Parameters\n    ----------\n    observable_sector_names : list\n        The names of the observable sectors to make the prediction for.\n\n    Returns\n    -------\n    function\n        The function that makes a prediction for a list of observable sectors.\n\n        Parameters\n        ----------\n          - `par_array : jnp.ndarray`\n            The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.\n\n          - `scale : float, int, or jnp.ndarray`\n            The scale at which to make the prediction. If `par_array` has batch dimensions, `scale` can be a scalar or an array with the same shape as the batch dimensions of `par_array`. If `par_array` has no batch dimensions, `scale` can be a scalar or an array.\n\n          - `prediction_data : list`\n            A list of lists containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients for each observable sector.\n\n        Returns\n        -------\n        `jnp.ndarray`\n            The observable predictions.\n\n    Examples\n    --------\n    Get the prediction function for a list of observable sectors:\n\n    &gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n\n    Make a prediction for a list of observable sectors:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n    Make a prediction for a list of observable sectors with batch dimensions in `par_array` and a scalar `scale`:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n    Make a prediction for a list of observable sectors with no batch dimensions in `par_array` and an array `scale`:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = jnp.array([1000, 2000])\n    &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n    Make a prediction for a list of observable sectors with both `par_array` and `scale` having batch dimensions:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n    &gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n    &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n    '''\n    prediction_functions = [\n        cls._observable_sectors[name].prediction\n        for name in observable_sector_names\n    ]\n\n    def prediction(\n        par_array: jnp.array, scale: Union[float, int, jnp.array],\n        prediction_data: List[List[jnp.array]]\n    ) -&gt; jnp.array:\n        return jnp.concatenate([\n            prediction_function(par_array, scale, data)[0]\n            for prediction_function, data in zip(prediction_functions, prediction_data)\n        ], axis=-1)\n\n    return prediction\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_class_prediction_function--parameters","title":"Parameters","text":"<ul> <li> <p><code>par_array : jnp.ndarray</code>     The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.</p> </li> <li> <p><code>scale : float, int, or jnp.ndarray</code>     The scale at which to make the prediction. If <code>par_array</code> has batch dimensions, <code>scale</code> can be a scalar or an array with the same shape as the batch dimensions of <code>par_array</code>. If <code>par_array</code> has no batch dimensions, <code>scale</code> can be a scalar or an array.</p> </li> <li> <p><code>prediction_data : list</code>     A list of lists containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients for each observable sector.</p> </li> </ul>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_class_prediction_function--returns","title":"Returns","text":"<p><code>jnp.ndarray</code>     The observable predictions.</p>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_prediction_data","title":"<code>get_prediction_data(eft, basis)</code>","text":"<p>Get the data needed to make a prediction for a given EFT and basis.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The EFT to make the prediction in.</p> required <code>basis</code> <code>str</code> <p>The basis to make the prediction in.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients.</p> <p>Examples:</p> <p>Get the data needed to make a prediction in the Warsaw basis of the SMEFT:</p> <pre><code>&gt;&gt;&gt; observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n</code></pre> <p>Get the data needed to make a prediction in the flavio basis of the WET:</p> <pre><code>&gt;&gt;&gt; observable_sector.get_prediction_data('WET', 'flavio')\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def get_prediction_data(self, eft: str, basis: str) -&gt; List[jnp.array]:\n    '''\n    Get the data needed to make a prediction for a given EFT and basis.\n\n    Parameters\n    ----------\n    eft : str\n        The EFT to make the prediction in.\n    basis : str\n        The basis to make the prediction in.\n\n    Returns\n    -------\n    list\n        A list containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients.\n\n    Examples\n    --------\n    Get the data needed to make a prediction in the Warsaw basis of the SMEFT:\n\n    &gt;&gt;&gt; observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n\n    Get the data needed to make a prediction in the flavio basis of the WET:\n\n    &gt;&gt;&gt; observable_sector.get_prediction_data('WET', 'flavio')\n    '''\n    return [\n        jnp.array(self.sector_indices[eft][basis]),\n        jnp.array(self.evolution_matrices[eft][basis], dtype=jnp.float32),\n        jnp.concatenate([\n            jnp.array(self.get_scales(eft, basis), dtype=jnp.float32),\n            jnp.array([jnp.nan], dtype=jnp.float32) # Add NaN to handle out-of-range cases\n        ]),\n        jnp.array(\n            self.observable_central if self.observable_expressions is None\n            else self.polynomial_central,\n            dtype=jnp.float64\n        ),\n    ]\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_scales","title":"<code>get_scales(eft, basis)</code>","text":"<p>Get the scales at which the Renormalization Group evolution matrices are defined for a given EFT and basis.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The EFT to get the Renormalization Group scales for.</p> required <code>basis</code> <code>str</code> <p>The basis to get the Renormalization Group scales for.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The scales at which the Renormalization Group evolution matrices are defined.</p> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def get_scales(self, eft: str, basis: str) -&gt; np.ndarray:\n    '''\n    Get the scales at which the Renormalization Group evolution matrices are defined for a given EFT and basis.\n\n    Parameters\n    ----------\n    eft : str\n        The EFT to get the Renormalization Group scales for.\n    basis : str\n        The basis to get the Renormalization Group scales for.\n\n    Returns\n    -------\n    np.ndarray\n        The scales at which the Renormalization Group evolution matrices are defined.\n    '''\n    if self.basis_mode == 'rgevolve':\n        return get_scales(eft, basis)\n    else:\n        return np.array([self.scale])\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load an observable sector from a json file or several observable sectors from a directory containing json files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a json file or a directory containing json files.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Load an observable sector from a json file:</p> <pre><code>&gt;&gt;&gt; ObservableSector.load('./observable_sector.json')\n</code></pre> <p>Load all observable sectors from a directory containing json files:</p> <pre><code>&gt;&gt;&gt; ObservableSector.load('./observable_sectors/')\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; None:\n    '''\n    Load an observable sector from a json file or several observable sectors from a directory containing json files.\n\n    Parameters\n    ----------\n    path : str\n        Path to a json file or a directory containing json files.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Load an observable sector from a json file:\n\n    &gt;&gt;&gt; ObservableSector.load('./observable_sector.json')\n\n    Load all observable sectors from a directory containing json files:\n\n    &gt;&gt;&gt; ObservableSector.load('./observable_sectors/')\n    '''\n    # load all json files in the directory\n    if os.path.isdir(path):\n        for file in os.listdir(path):\n            if file.endswith('.json'):\n                cls._load_file(os.path.join(path, file))\n    # load single json file\n    else:\n        cls._load_file(path)\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.interpolate_rg_evolution","title":"<code>interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)</code>","text":"<p>Interpolate the Renormalization Group evolution of the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>par_array</code> <code>ndarray</code> <p>The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.</p> required <code>scale</code> <code>(float, int or ndarray)</code> <p>The scale at which to make the prediction. If <code>par_array</code> has batch dimensions, <code>scale</code> can be a scalar or an array with the same shape as the batch dimensions of <code>par_array</code>. If <code>par_array</code> has no batch dimensions, <code>scale</code> can be a scalar or an array.</p> required <code>evolution_matrices</code> <code>ndarray</code> <p>The Renormalization Group evolution matrices.</p> required <code>evolution_scales</code> <code>ndarray</code> <p>The scales at which the Renormalization Group evolution matrices are defined.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The evolved parameter array.</p> <p>Examples:</p> <p>Interpolate the Renormalization Group evolution of the parameters:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n</code></pre> <p>Interpolate the Renormalization Group evolution of the parameters with batch dimensions in <code>par_array</code> and a scalar <code>scale</code>:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n</code></pre> <p>Interpolate the Renormalization Group evolution of the parameters with no batch dimensions in <code>par_array</code> and an array <code>scale</code>:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = jnp.array([1000, 2000])\n&gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n</code></pre> <p>Interpolate the Renormalization Group evolution of the parameters with both <code>par_array</code> and <code>scale</code> having batch dimensions:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n&gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n&gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def interpolate_rg_evolution(\n    par_array: jnp.ndarray,\n    scale: Union[float, int, jnp.ndarray],\n    evolution_matrices: jnp.ndarray,\n    evolution_scales: jnp.ndarray\n) -&gt; jnp.ndarray:\n    '''\n    Interpolate the Renormalization Group evolution of the parameters.\n\n    Parameters\n    ----------\n    par_array : jnp.ndarray\n        The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.\n\n    scale : float, int or jnp.ndarray\n        The scale at which to make the prediction. If `par_array` has batch dimensions, `scale` can be a scalar or an array with the same shape as the batch dimensions of `par_array`. If `par_array` has no batch dimensions, `scale` can be a scalar or an array.\n\n    evolution_matrices : jnp.ndarray\n        The Renormalization Group evolution matrices.\n\n    evolution_scales : jnp.ndarray\n        The scales at which the Renormalization Group evolution matrices are defined.\n\n    Returns\n    -------\n    jnp.ndarray\n        The evolved parameter array.\n\n    Examples\n    --------\n    Interpolate the Renormalization Group evolution of the parameters:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n\n    Interpolate the Renormalization Group evolution of the parameters with batch dimensions in `par_array` and a scalar `scale`:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n\n    Interpolate the Renormalization Group evolution of the parameters with no batch dimensions in `par_array` and an array `scale`:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = jnp.array([1000, 2000])\n    &gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n\n    Interpolate the Renormalization Group evolution of the parameters with both `par_array` and `scale` having batch dimensions:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n    &gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n    &gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n    '''\n\n    # Searchsorted logic (supports batched scale)\n    index_low = jnp.searchsorted(evolution_scales, scale, side='right') - 1\n    index_high = index_low + 1\n\n    # Extract scales and matrices (supports batched indices)\n    scale_low = jnp.take(evolution_scales, index_low)\n    scale_high = jnp.take(evolution_scales, index_high)\n    matrix_low = jnp.take(evolution_matrices, index_low, axis=0)\n    matrix_high = jnp.take(evolution_matrices, index_high, axis=0)\n\n    # Expand scales for broadcasting with matrices\n    scale = jnp.expand_dims(scale, axis=(-2, -1))\n    scale_low = jnp.expand_dims(scale_low, axis=(-2, -1))\n    scale_high = jnp.expand_dims(scale_high, axis=(-2, -1))\n\n    # Logarithmic interpolation\n    matrix = jnp.where(\n        scale == scale_low,\n        matrix_low,\n        matrix_low + (matrix_high - matrix_low) * jnp.log(scale / scale_low) / jnp.log(scale_high / scale_low)\n    )\n\n    # Non-batched case (fast path)\n    if par_array.ndim == 1 and matrix.ndim == 2:\n        return jnp.dot(matrix, par_array)\n\n    # Ensure `par_array` behaves like a batch\n    if par_array.ndim == 1:\n        par_array = par_array[None, :]\n\n    # Add batch dimension to `matrix` if it\u2019s 2D (non-batched)\n    if matrix.ndim == 2:\n        matrix = matrix[None, :, :]\n\n    # Batched matrix-vector multiplication\n    return jnp.einsum('...ij,...j-&gt;...i', matrix, par_array)\n</code></pre>"},{"location":"jelli/core/theory_correlations/","title":"jelli.core.theory_correlations","text":""},{"location":"jelli/utils/data_io/","title":"jelli.utils.data_io","text":""},{"location":"jelli/utils/data_io/#jelli.utils.data_io.get_json_schema","title":"<code>get_json_schema(json_data)</code>","text":"<p>Extract the schema name and version from the JSON data.</p> <p>Parameters:</p> Name Type Description Default <code>json_data</code> <code>dict</code> <p>The JSON data containing the schema information.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the schema name and version. If not found, returns <code>(None, None)</code>.</p> Source code in <code>jelli/utils/data_io.py</code> <pre><code>def get_json_schema(json_data):\n    '''\n    Extract the schema name and version from the JSON data.\n\n    Parameters\n    ----------\n    json_data : dict\n        The JSON data containing the schema information.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the schema name and version. If not found, returns `(None, None)`.\n    '''\n    schema_name = None\n    schema_version = None\n    if '$schema' in json_data:\n        schema = json_data['$schema']\n        if isinstance(schema, (np.ndarray, list)):\n            schema = str(schema[0])\n        else:\n            schema = str(schema)\n        match = json_schema_name_pattern.search(schema)\n        if match:\n            schema_name = match.group(1)\n            schema_version = match.group(3)\n    return schema_name, schema_version\n</code></pre>"},{"location":"jelli/utils/distributions/","title":"jelli.utils.distributions","text":""},{"location":"jelli/utils/distributions/#jelli.utils.distributions.combine_distributions_numerically","title":"<code>combine_distributions_numerically(constraints, n_points=1000)</code>","text":"<p>Combine multiple distributions into a single numerical distribution by summing their logpdfs on a common support.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>Dict[str, Dict[str, ndarray]]</code> <p>A dictionary where keys are distribution types (e.g., <code>NumericalDistribution</code>, <code>NormalDistribution</code>, etc.) and values are dictionaries containing distribution information such as <code>central_value</code>, <code>standard_deviation</code>, etc.</p> required <code>n_points</code> <code>int</code> <p>Number of points in the common support for the output distribution. Default is <code>1000</code>.</p> <code>1000</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>A dictionary containing the combined numerical distribution information, including <code>measurement_name</code>, <code>observables</code>, <code>observable_indices</code>, <code>x</code>, <code>y</code>, and <code>log_y</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; constraints = {\n...     'NormalDistribution': {\n...         'measurement_name': np.array(['measurement1']),\n...         'observables': np.array(['observable1']),\n...         'observable_indices': np.array([0]),\n...         'central_value': np.array([1.0]),\n...         'standard_deviation': np.array([0.8])\n...     },\n...     'HalfNormalDistribution': {\n...         'measurement_name': np.array(['measurement2', 'measurement3']),\n...         'observables': np.array(['observable1', 'observable1']),\n...         'observable_indices': np.array([0, 0]),\n...         'standard_deviation': np.array([0.3, 0.4])\n...     }\n... }\n&gt;&gt;&gt; combine_distributions(constraints, n_points=1000)\n{\n    'measurement_name': np.array(['measurement1, measurement2, measurement3']),\n    'observables': np.array(['observable1']),\n    'observable_indices': np.array([0]),\n    'x': np.array([...]),  # combined support\n    'y': np.array([...]),  # combined pdf values\n    'log_y': np.array([...])  # combined log pdf values\n}\n</code></pre> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def combine_distributions_numerically(\n        constraints: Dict[str, Dict[str, np.ndarray]],\n        n_points: int = 1000,\n) -&gt; Dict[str, np.ndarray]:\n    '''\n    Combine multiple distributions into a single numerical distribution by summing their logpdfs on a common support.\n\n    Parameters\n    ----------\n    constraints : Dict[str, Dict[str, np.ndarray]]\n        A dictionary where keys are distribution types (e.g., `NumericalDistribution`, `NormalDistribution`, etc.)\n        and values are dictionaries containing distribution information such as `central_value`, `standard_deviation`, etc.\n    n_points : int, optional\n        Number of points in the common support for the output distribution. Default is `1000`.\n\n    Returns\n    -------\n    Dict[str, np.ndarray]\n        A dictionary containing the combined numerical distribution information, including `measurement_name`, `observables`,\n        `observable_indices`, `x`, `y`, and `log_y`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; constraints = {\n    ...     'NormalDistribution': {\n    ...         'measurement_name': np.array(['measurement1']),\n    ...         'observables': np.array(['observable1']),\n    ...         'observable_indices': np.array([0]),\n    ...         'central_value': np.array([1.0]),\n    ...         'standard_deviation': np.array([0.8])\n    ...     },\n    ...     'HalfNormalDistribution': {\n    ...         'measurement_name': np.array(['measurement2', 'measurement3']),\n    ...         'observables': np.array(['observable1', 'observable1']),\n    ...         'observable_indices': np.array([0, 0]),\n    ...         'standard_deviation': np.array([0.3, 0.4])\n    ...     }\n    ... }\n    &gt;&gt;&gt; combine_distributions(constraints, n_points=1000)\n    {\n        'measurement_name': np.array(['measurement1, measurement2, measurement3']),\n        'observables': np.array(['observable1']),\n        'observable_indices': np.array([0]),\n        'x': np.array([...]),  # combined support\n        'y': np.array([...]),  # combined pdf values\n        'log_y': np.array([...])  # combined log pdf values\n    }\n    '''\n\n    # get universal parameters for output\n    dist_info = next(iter(constraints.values()))\n    observables_out = dist_info['observables'][:1]\n    observable_indices_out = dist_info['observable_indices'][:1]\n\n    # get measurement names in each constraint and supports of distributions\n    measurement_names = []\n    supports = []\n    for dist_type, dist_info in constraints.items():\n        supports.append(\n            get_distribution_support(dist_type, dist_info)\n        )\n        measurement_names.append(dist_info['measurement_name'])\n\n    # combine measurement names for output\n    measurement_name_out = np.expand_dims(', '.join(np.unique(np.concatenate(measurement_names))), axis=0)\n\n    # common support for all distributions\n    support_min = np.min(np.concatenate([s[0] for s in supports]))\n    support_max = np.max(np.concatenate([s[1] for s in supports]))\n    xp_out = np.linspace(support_min, support_max, n_points)\n\n    # sum the logpdfs of all distributions on the common support\n    log_fp_out = np.zeros_like(xp_out)\n    for dist_type, dist_info in constraints.items():\n        unique_observables = np.unique(dist_info['observables'])\n        if len(unique_observables) &gt; 1 or unique_observables[0] != observables_out[0]:\n            raise ValueError(f\"Only distributions constraining the same observable can be combined.\")\n        n_constraints = len(dist_info['observables'])\n        x = np.broadcast_to(xp_out, (n_constraints, n_points)).reshape(-1)\n        observable_indices = np.arange(len(x))\n        selector_matrix = np.concatenate([np.eye(n_points)]*n_constraints, axis=1)\n        if dist_type == 'NumericalDistribution':\n            xp = dist_info['x']\n            log_fp = dist_info['log_y']\n            xp = np.broadcast_to(xp[:, None, :], (xp.shape[0], n_points, xp.shape[1]))\n            xp = xp.reshape(-1, xp.shape[2])\n            log_fp = np.broadcast_to(log_fp[:, None, :], (log_fp.shape[0], n_points, log_fp.shape[1]))\n            log_fp = log_fp.reshape(-1, log_fp.shape[2])\n            log_fp_out += logpdf_functions_summed[dist_type](\n                x,\n                selector_matrix,\n                observable_indices,\n                xp,\n                log_fp,\n            )\n        elif dist_type == 'NormalDistribution':\n            central_value = np.broadcast_to(dist_info['central_value'], (n_points, n_constraints)).T.reshape(-1)\n            standard_deviation = np.broadcast_to(dist_info['standard_deviation'], (n_points, n_constraints)).T.reshape(-1)\n            log_fp_out += logpdf_functions_summed[dist_type](\n                x,\n                selector_matrix,\n                observable_indices,\n                central_value,\n                standard_deviation,\n            )\n        elif dist_type == 'HalfNormalDistribution':\n            standard_deviation = np.broadcast_to(dist_info['standard_deviation'], (n_points, n_constraints)).T.reshape(-1)\n            log_fp_out += logpdf_functions_summed[dist_type](\n                x,\n                selector_matrix,\n                observable_indices,\n                standard_deviation,\n            )\n        elif dist_type == 'GammaDistributionPositive':\n            a = np.broadcast_to(dist_info['a'], (n_points, n_constraints)).T.reshape(-1)\n            loc = np.broadcast_to(dist_info['loc'], (n_points, n_constraints)).T.reshape(-1)\n            scale = np.broadcast_to(dist_info['scale'], (n_points, n_constraints)).T.reshape(-1)\n            log_fp_out += logpdf_functions_summed[dist_type](\n                x,\n                selector_matrix,\n                observable_indices,\n                a,\n                loc,\n                scale,\n            )\n        else:\n            raise NotImplementedError(f\"Combining distributions not implemented for {dist_type}.\")\n\n    # normalize the output distribution\n    log_fp_out -= log_trapz_exp(log_fp_out, xp_out)\n\n    return {\n        'measurement_name': measurement_name_out,\n        'observables': observables_out,\n        'observable_indices': observable_indices_out,\n        'x': xp_out,\n        'y': np.exp(log_fp_out),\n        'log_y': log_fp_out,\n    }\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.combine_normal_distributions","title":"<code>combine_normal_distributions(measurement_name, observables, observable_indices, central_value, standard_deviation)</code>","text":"<p>Combine multiple normal distributions into a single normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>measurement_name</code> <code>ndarray</code> <p>Names of the measurements.</p> required <code>observables</code> <code>ndarray</code> <p>Names of the observables.</p> required <code>observable_indices</code> <code>ndarray</code> <p>Indices of the observables.</p> required <code>central_value</code> <code>ndarray</code> <p>Central values of the normal distributions.</p> required <code>standard_deviation</code> <code>ndarray</code> <p>Standard deviations of the normal distributions.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>A dictionary containing the combined measurement name, observables, observable indices, central value, and standard deviation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; combine_normal_distributions(\n...     measurement_name=np.array(['measurement1', 'measurement2']),\n...     observables=np.array(['observable1', 'observable1']),\n...     observable_indices=np.array([3, 3]),\n...     central_value=np.array([1.0, 2.0]),\n...     standard_deviation=np.array([0.1, 0.2])\n... )\n{\n    'measurement_name': np.array(['measurement1, measurement2']),\n    'observables': np.array(['observable1']),\n    'observable_indices': np.array([3]),\n    'central_value': np.array([1.2]),\n    'standard_deviation': np.array([0.08944272])\n}\n</code></pre> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def combine_normal_distributions(\n        measurement_name: np.ndarray,\n        observables: np.ndarray,\n        observable_indices: np.ndarray,\n        central_value: np.ndarray,\n        standard_deviation: np.ndarray,\n    ) -&gt; Dict[str, np.ndarray]:\n    '''\n    Combine multiple normal distributions into a single normal distribution.\n\n    Parameters\n    ----------\n    measurement_name : np.ndarray\n        Names of the measurements.\n    observables : np.ndarray\n        Names of the observables.\n    observable_indices : np.ndarray\n        Indices of the observables.\n    central_value : np.ndarray\n        Central values of the normal distributions.\n    standard_deviation : np.ndarray\n        Standard deviations of the normal distributions.\n\n    Returns\n    -------\n    Dict[str, np.ndarray]\n        A dictionary containing the combined measurement name, observables, observable indices,\n        central value, and standard deviation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; combine_normal_distributions(\n    ...     measurement_name=np.array(['measurement1', 'measurement2']),\n    ...     observables=np.array(['observable1', 'observable1']),\n    ...     observable_indices=np.array([3, 3]),\n    ...     central_value=np.array([1.0, 2.0]),\n    ...     standard_deviation=np.array([0.1, 0.2])\n    ... )\n    {\n        'measurement_name': np.array(['measurement1, measurement2']),\n        'observables': np.array(['observable1']),\n        'observable_indices': np.array([3]),\n        'central_value': np.array([1.2]),\n        'standard_deviation': np.array([0.08944272])\n    }\n    '''\n\n    if len(measurement_name) &gt; 1:\n        if len(np.unique(observables)) &gt; 1:\n            raise ValueError(f\"Only distributions constraining the same observable can be combined.\")\n        measurement_name = np.expand_dims(', '.join(np.unique(measurement_name)), axis=0)\n        observables = observables[:1]\n        observable_indices = observable_indices[:1]\n        weights = 1 / standard_deviation**2\n        central_value = np.average(central_value, weights=weights, keepdims=True)\n        standard_deviation = np.sqrt(1 / np.sum(weights, keepdims=True))\n    return {\n        'measurement_name': measurement_name,\n        'observables': observables,\n        'observable_indices': observable_indices,\n        'central_value': central_value,\n        'standard_deviation': standard_deviation,\n    }\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.get_distribution_samples","title":"<code>get_distribution_samples(dist_type, dist_info, n_samples, seed=None)</code>","text":"<p>Generate samples from a specified distribution type using the provided distribution information.</p> <p>Parameters:</p> Name Type Description Default <code>dist_type</code> <code>str</code> <p>Type of the distribution (e.g., <code>NumericalDistribution</code>, <code>NormalDistribution</code>, etc.).</p> required <code>dist_info</code> <code>Dict[str, ndarray]</code> <p>Information about the distribution, such as <code>central_value</code>, <code>standard_deviation</code>, etc.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples to generate.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ndarray] or ndarray</code> <p>A list of arrays in case of <code>MultivariateNormalDistribution</code>, where the length of the list is the number of constraints, and each array is of shape <code>(n_observables, n_samples)</code>. For other distributions, returns a single array of shape <code>(n_constraints, n_samples)</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dist_info = {\n...     'central_value': np.array([0.0, 1.0]),\n...     'standard_deviation': np.array([1.0, 2.0])\n... }\n&gt;&gt;&gt; get_distribution_samples('NormalDistribution', dist_info, n_samples=1000)\narray([[ 0.12345678,  1.23456789, ...\n       [-0.98765432,  2.34567890, ...]])\n</code></pre> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def get_distribution_samples(\n        dist_type: str,\n        dist_info: Dict[str, np.ndarray],\n        n_samples: int,\n        seed: Optional[int] = None,\n) -&gt; Union[List[np.ndarray], np.ndarray]:\n    \"\"\"\n    Generate samples from a specified distribution type using the provided distribution information.\n\n    Parameters\n    ----------\n    dist_type : str\n        Type of the distribution (e.g., `NumericalDistribution`, `NormalDistribution`, etc.).\n    dist_info : Dict[str, np.ndarray]\n        Information about the distribution, such as `central_value`, `standard_deviation`, etc.\n    n_samples : int\n        Number of samples to generate.\n    seed : int, optional\n        Random seed for reproducibility. Default is `None`.\n\n    Returns\n    -------\n    List[np.ndarray] or np.ndarray\n        A list of arrays in case of `MultivariateNormalDistribution`, where the length of the list is the number of constraints,\n        and each array is of shape `(n_observables, n_samples)`.\n        For other distributions, returns a single array of shape `(n_constraints, n_samples)`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; dist_info = {\n    ...     'central_value': np.array([0.0, 1.0]),\n    ...     'standard_deviation': np.array([1.0, 2.0])\n    ... }\n    &gt;&gt;&gt; get_distribution_samples('NormalDistribution', dist_info, n_samples=1000)\n    array([[ 0.12345678,  1.23456789, ...\n           [-0.98765432,  2.34567890, ...]])\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    if dist_type == 'GammaDistributionPositive':\n        a = dist_info['a']\n        loc = dist_info['loc']\n        scale = dist_info['scale']\n        n_constraints = len(a)\n        ppf = get_ppf_gamma_distribution_positive(a, loc, scale)\n        return get_inverse_transform_samples(ppf, n_samples, n_constraints)\n    elif dist_type == 'NumericalDistribution':\n        xp = dist_info['x']\n        fp = dist_info['y']\n        ppf = get_ppf_numerical_distribution(xp, fp)\n        n_constraints = len(xp)\n        return get_inverse_transform_samples(ppf, n_samples, n_constraints)\n    elif dist_type == 'NormalDistribution':\n        central_value = dist_info['central_value']\n        standard_deviation = dist_info['standard_deviation']\n        n_constraints = len(central_value)\n        return np.random.normal(central_value, standard_deviation, size=(n_samples, n_constraints)).T\n    elif dist_type == 'MultivariateNormalDistribution':\n        central_value = dist_info['central_value']\n        standard_deviation = dist_info['standard_deviation']\n        inverse_correlation = dist_info['inverse_correlation']\n        samples = []\n        n_constraints = len(central_value)\n        for i in range(n_constraints):\n            correlation = np.linalg.inv(inverse_correlation[i])  # TODO: think about saving the correlation matrix also in the constraint dict\n            samples.append(\n                np.random.multivariate_normal(\n                    central_value[i],\n                    correlation * np.outer(standard_deviation[i], standard_deviation[i]), n_samples).T\n            )\n        return samples\n    elif dist_type == 'HalfNormalDistribution':\n        standard_deviation = dist_info['standard_deviation']\n        n_constraints = len(standard_deviation)\n        return np.abs(np.random.normal(0, standard_deviation, size=(n_samples, n_constraints)).T)\n    else:\n        raise ValueError(f\"Sampling not implemented for distribution type: {dist_type}\")\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.get_distribution_support","title":"<code>get_distribution_support(dist_type, dist_info)</code>","text":"<p>Get the support of one or more distributions based on the distribution parameters.</p> <p>Parameters:</p> Name Type Description Default <code>dist_type</code> <code>str</code> <p>Type of the distribution (e.g., <code>NumericalDistribution</code>, <code>NormalDistribution</code>, etc.).</p> required <code>dist_info</code> <code>Dict[str, ndarray]</code> <p>Information about the distribution, such as <code>central_value</code>, <code>standard_deviation</code>, etc.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple containing the minimum and maximum values of the support of the distributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_distribution_support('NormalDistribution', {'central_value': np.array([0.0, 1.0]), 'standard_deviation': np.array([1.0, 2.0])})\n(array([-6., -11.]), array([6., 13.]))\n</code></pre> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def get_distribution_support(\n        dist_type: str,\n        dist_info: Dict[str, np.ndarray]\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n    '''\n    Get the support of one or more distributions based on the distribution parameters.\n\n    Parameters\n    ----------\n    dist_type : str\n        Type of the distribution (e.g., `NumericalDistribution`, `NormalDistribution`, etc.).\n    dist_info : Dict[str, np.ndarray]\n        Information about the distribution, such as `central_value`, `standard_deviation`, etc.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        A tuple containing the minimum and maximum values of the support of the distributions.\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_distribution_support('NormalDistribution', {'central_value': np.array([0.0, 1.0]), 'standard_deviation': np.array([1.0, 2.0])})\n    (array([-6., -11.]), array([6., 13.]))\n\n    '''\n\n    if dist_type == 'NumericalDistribution':\n        xp = dist_info['x']\n        return np.min(xp, axis=1), np.max(xp, axis=1)\n    elif dist_type == 'NormalDistribution':\n        central_value = dist_info['central_value']\n        standard_deviation = dist_info['standard_deviation']\n        return central_value - 6*standard_deviation, central_value + 6*standard_deviation\n    elif dist_type == 'HalfNormalDistribution':\n        standard_deviation = dist_info['standard_deviation']\n        return np.zeros_like(standard_deviation), 6*standard_deviation\n    elif dist_type == 'GammaDistributionPositive':\n        a = dist_info['a']\n        loc = dist_info['loc']\n        scale = dist_info['scale']\n        mode = np.maximum(loc + (a-1)*scale, 0)\n        gamma = sp.stats.gamma(a, loc, scale)\n        support_min = np.maximum(np.minimum(gamma.ppf(1e-9), mode), 0)\n        support_max = gamma.ppf(1-1e-9*(1-gamma.cdf(0)))\n        return support_min, support_max\n    else:\n        raise NotImplementedError(f\"Computing the support not implemented for {dist_type}.\")\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.get_inverse_transform_samples","title":"<code>get_inverse_transform_samples(ppf, n_samples, n_constraints)</code>","text":"<p>Generate samples from a distribution using inverse transform sampling.</p> <p>Parameters:</p> Name Type Description Default <code>ppf</code> <code>Callable</code> <p>The percent-point function (PPF) of the distribution.</p> required <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <code>n_constraints</code> <code>int</code> <p>The number of constraints.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of samples drawn from the distribution defined by the PPF.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def get_inverse_transform_samples(\n        ppf: Callable,\n        n_samples: int,\n        n_constraints: int\n    ) -&gt; np.ndarray:\n    \"\"\"\n    Generate samples from a distribution using inverse transform sampling.\n\n    Parameters\n    ----------\n    ppf : Callable\n        The percent-point function (PPF) of the distribution.\n    n_samples : int\n        The number of samples to generate.\n    n_constraints : int\n        The number of constraints.\n\n    Returns\n    -------\n    np.ndarray\n        An array of samples drawn from the distribution defined by the PPF.\n    \"\"\"\n    return ppf(np.random.uniform(0, 1, (n_samples, n_constraints))).T\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.get_mode_and_uncertainty","title":"<code>get_mode_and_uncertainty(dist_type, dist_info)</code>","text":"<p>Get the mode and uncertainty of one or more distributions based on the distribution parameters.</p> <p>A Gaussian approximation or an upper limit based on the 95% confidence level is used, depending on the distribution type and parameters.</p> <p>In case of the upper limit, the mode is set to <code>nan</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dist_type</code> <code>str</code> <p>Type of the distribution (e.g., <code>NumericalDistribution</code>, <code>NormalDistribution</code>, etc.).</p> required <code>dist_info</code> <code>Dict[str, ndarray]</code> <p>Information about the distribution, such as <code>central_value</code>, <code>standard_deviation</code>, etc.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple containing the mode and uncertainty of the distributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_mode_and_uncertainty('NormalDistribution', {'central_value': np.array([0.0, 1.0]), 'standard_deviation': np.array([1.0, 2.0])})\n(array([0., 1.]), array([1., 2.]))\n&gt;&gt;&gt; get_mode_and_uncertainty('HalfNormalDistribution', {'standard_deviation': np.array([0.3, 0.4])})\n(array([nan, nan]), array([0.588, 0.784]))\n&gt;&gt;&gt; get_mode_and_uncertainty('GammaDistributionPositive', {'a': np.array([2.0, 4.0]), 'loc': np.array([-1.0, 0.0]), 'scale': np.array([1.0, 2.0])})\n(array([nan,  6.]), array([4.11300328, 3.46410162]))\n&gt;&gt;&gt; central_value = np.array([[0.0], [6.4]])\n&gt;&gt;&gt; standard_deviation = np.array([[1.0], [1.2]])\n&gt;&gt;&gt; xp = np.broadcast_to(np.linspace(0, 10, 10000), (2, 10000))\n&gt;&gt;&gt; fp = sp.stats.norm.pdf(xp, loc=central_value, scale=standard_deviation)\n&gt;&gt;&gt; get_mode_and_uncertainty('NumericalDistribution', {'x': xp, 'y': fp, 'log_y': np.log(fp)})\n(array([nan, 6.4]), array([1.96, 1.2]))\n</code></pre> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def get_mode_and_uncertainty(\n        dist_type: str,\n        dist_info: Dict[str, np.ndarray],\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    '''\n    Get the mode and uncertainty of one or more distributions based on the distribution parameters.\n\n    A Gaussian approximation or an upper limit based on the 95% confidence level is used, depending on the distribution type and parameters.\n\n    In case of the upper limit, the mode is set to `nan`.\n\n    Parameters\n    ----------\n    dist_type : str\n        Type of the distribution (e.g., `NumericalDistribution`, `NormalDistribution`, etc.).\n    dist_info : Dict[str, np.ndarray]\n        Information about the distribution, such as `central_value`, `standard_deviation`, etc.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        A tuple containing the mode and uncertainty of the distributions.\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_mode_and_uncertainty('NormalDistribution', {'central_value': np.array([0.0, 1.0]), 'standard_deviation': np.array([1.0, 2.0])})\n    (array([0., 1.]), array([1., 2.]))\n    &gt;&gt;&gt; get_mode_and_uncertainty('HalfNormalDistribution', {'standard_deviation': np.array([0.3, 0.4])})\n    (array([nan, nan]), array([0.588, 0.784]))\n    &gt;&gt;&gt; get_mode_and_uncertainty('GammaDistributionPositive', {'a': np.array([2.0, 4.0]), 'loc': np.array([-1.0, 0.0]), 'scale': np.array([1.0, 2.0])})\n    (array([nan,  6.]), array([4.11300328, 3.46410162]))\n    &gt;&gt;&gt; central_value = np.array([[0.0], [6.4]])\n    &gt;&gt;&gt; standard_deviation = np.array([[1.0], [1.2]])\n    &gt;&gt;&gt; xp = np.broadcast_to(np.linspace(0, 10, 10000), (2, 10000))\n    &gt;&gt;&gt; fp = sp.stats.norm.pdf(xp, loc=central_value, scale=standard_deviation)\n    &gt;&gt;&gt; get_mode_and_uncertainty('NumericalDistribution', {'x': xp, 'y': fp, 'log_y': np.log(fp)})\n    (array([nan, 6.4]), array([1.96, 1.2]))\n    '''\n    if dist_type == 'NormalDistribution':\n        mode = dist_info['central_value']\n        uncertainty = dist_info['standard_deviation']\n        return mode, uncertainty\n    elif dist_type == 'HalfNormalDistribution':\n        uncertainty = dist_info['standard_deviation']*1.96  # 95% CL\n        return np.full_like(uncertainty, np.nan), uncertainty\n    elif dist_type == 'GammaDistributionPositive':\n        a = dist_info['a']\n        loc = dist_info['loc']\n        scale = dist_info['scale']\n        mode = np.maximum(loc + (a-1)*scale, 0)\n\n        # if mode is negative, use the 95% CL upper limit, otherwise use the standard deviation at the mode\n        upper_limit = mode &lt;= 0\n        gaussian = ~upper_limit\n        uncertainty = np.empty_like(mode, dtype=float)\n        uncertainty[gaussian] = np.sqrt((loc[gaussian]-mode[gaussian])**2 / (a[gaussian]-1))  # standard deviation at the mode, defined as sqrt(-1/(d^2/dx^2 log(gamma(x, a, loc, scale))))\n        ppf = get_ppf_gamma_distribution_positive(a[upper_limit], loc[upper_limit], scale[upper_limit])\n        uncertainty[upper_limit] = ppf(0.95)  # 95% CL upper limit using the ppf of the gamma distribution restricted to positive values\n        mode[upper_limit] = np.nan  # set the modes to nan where they are not defined\n\n        # check if mode/uncertainty is smaller than 1.7 and mode &gt; 0, in this case compute 95% CL upper limit\n        # 1.7 is selected as threshold where the gaussian and halfnormal approximation are approximately equally good based on the KL divergence\n        upper_limit = (mode/uncertainty &lt; 1.7) &amp; (mode &gt; 0)\n        ppf = get_ppf_gamma_distribution_positive(a[upper_limit], loc[upper_limit], scale[upper_limit])\n        uncertainty[upper_limit] = ppf(0.95)  # 95% CL upper limit using the ppf of the gamma distribution restricted to positive values\n        mode[upper_limit] = np.nan\n        return mode, uncertainty\n    elif dist_type == 'NumericalDistribution':\n        xp = dist_info['x']\n        log_fp = dist_info['log_y']\n        fp = dist_info['y']\n        n_constraints = len(log_fp)\n        mode = np.empty(n_constraints, dtype=float)\n        uncertainty = np.empty(n_constraints, dtype=float)\n        for i in range(n_constraints):\n            log_fp_i = log_fp[i]\n            fp_i = fp[i]\n            xp_i = xp[i]\n            fit_points = log_fp_i &gt; np.max(log_fp_i) - 0.5  # points of logpdf within 0.5 of the maximum\n            a, b, _ = np.polyfit(xp_i[fit_points], log_fp_i[fit_points], 2)  # fit a quadratic polynomial to the logpdf\n            mode_i = -b / (2 * a)\n            uncertainty_i = np.sqrt(-1 / (2 * a))\n            if np.abs(mode_i/uncertainty_i) &gt; 1.7:  # if mode/uncertainty is larger than 1.7, use gaussian approximation\n                mode[i] = mode_i\n                uncertainty[i] = uncertainty_i\n            else:  # compute 95% CL upper limit using ppf of the numerical distribution\n                ppf = get_ppf_numerical_distribution(xp_i, fp_i)\n                mode[i] = np.nan\n                uncertainty[i] = ppf(0.95)\n        return mode, uncertainty\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.get_ppf_gamma_distribution_positive","title":"<code>get_ppf_gamma_distribution_positive(a, loc, scale)</code>","text":"<p>Get the percent-point function (PPF) for a gamma distribution restricted to positive values.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>ndarray</code> <p>Shape parameter of the gamma distribution.</p> required <code>loc</code> <code>ndarray</code> <p>Location parameter of the gamma distribution.</p> required <code>scale</code> <code>ndarray</code> <p>Scale parameter of the gamma distribution.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The PPF that can be used to compute the quantiles for given probabilities.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def get_ppf_gamma_distribution_positive(\n        a: np.ndarray,\n        loc: np.ndarray,\n        scale: np.ndarray,\n) -&gt; Callable:\n    \"\"\"\n    Get the percent-point function (PPF) for a gamma distribution restricted to positive values.\n\n    Parameters\n    ----------\n    a : np.ndarray\n        Shape parameter of the gamma distribution.\n    loc : np.ndarray\n        Location parameter of the gamma distribution.\n    scale : np.ndarray\n        Scale parameter of the gamma distribution.\n\n    Returns\n    -------\n    Callable\n        The PPF that can be used to compute the quantiles for given probabilities.\n    \"\"\"\n    gamma = sp.stats.gamma(a, loc, scale)\n    def ppf(q):\n        return gamma.ppf(q + (1-q)*gamma.cdf(0))\n    return ppf\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.get_ppf_numerical_distribution","title":"<code>get_ppf_numerical_distribution(xp, fp)</code>","text":"<p>Get the percent-point function (PPF) for one or more numerical distributions.</p> <p>Parameters:</p> Name Type Description Default <code>xp</code> <code>ndarray</code> <p>Points at which the PDF is defined.</p> required <code>fp</code> <code>ndarray</code> <p>PDF values at the points <code>xp</code>.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The PPF that can be used to compute the quantiles for given probabilities.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def get_ppf_numerical_distribution(\n        xp: np.ndarray,\n        fp: np.ndarray,\n) -&gt; Callable:\n    '''\n    Get the percent-point function (PPF) for one or more numerical distributions.\n\n    Parameters\n    ----------\n    xp : np.ndarray\n        Points at which the PDF is defined.\n    fp : np.ndarray\n        PDF values at the points `xp`.\n\n    Returns\n    -------\n    Callable\n        The PPF that can be used to compute the quantiles for given probabilities.\n    '''\n    if xp.ndim == 1: # single distribution\n        cdf = np.concatenate([[0], np.cumsum((fp[1:] + fp[:-1]) * 0.5 * np.diff(xp))])\n        cdf /= cdf[-1]\n        return partial(np.interp, xp=cdf, fp=xp)\n    elif xp.ndim == 2: # multiple distributions\n        dx = np.diff(xp, axis=1)\n        avg_fp = 0.5 * (fp[:, 1:] + fp[:, :-1])\n        cdf = np.cumsum(avg_fp * dx, axis=1)\n        cdf = np.concatenate([np.zeros((cdf.shape[0], 1)), cdf], axis=1)\n        cdf /= cdf[:, [-1]]\n\n        def batched_ppf(q: Union[float, np.ndarray]) -&gt; np.ndarray:\n            \"\"\"\n            Batched PPF for multiple distributions.\n\n            Parameters\n            ----------\n            q : float or np.ndarray\n                Lower-tail probabilities at which to compute the PPF.\n\n                  - If scalar, computes PPF for that probability across all distributions.\n\n                  - If 1D array of shape (k,), computes PPF at k probabilities for all distributions.\n\n                  - If 2D array of shape (k, m), computes PPF at k probabilities for each of the m distributions.\n\n            Returns\n            -------\n            np.ndarray\n                The quantiles corresponding to the input probabilities.\n\n                  - If input is scalar, returns 1D array of shape (m,)\n\n                  - Otherwise returns array of shape (k, m)\n            \"\"\"\n\n            q = np.asarray(q)\n            scalar_input = False\n            if q.ndim == 0:  # single probability for all distributions\n                q = np.full((1, cdf.shape[0]), q)\n                scalar_input = True\n            elif q.ndim == 1:  # a vector of probabilities for all distributions\n                q = np.tile(q[None, :], (1, cdf.shape[0]))\n            result = np.empty_like(q)\n            for i in range(q.shape[1]):  # iterate over distributions\n                result[:, i] = np.interp(q[:, i], cdf[i], xp[i])\n            return result[0] if scalar_input else result\n        return batched_ppf\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.log_trapz_exp","title":"<code>log_trapz_exp(log_y, x)</code>","text":"<p>Compute the log of the trapezoidal integral of the exponential of <code>log_y</code> over <code>x</code>.</p> <p>Parameters:</p> Name Type Description Default <code>log_y</code> <code>ndarray</code> <p>Logarithm of the values to be integrated.</p> required <code>x</code> <code>ndarray</code> <p>Points at which <code>log_y</code> is defined. It is assumed that <code>x</code> is uniformly spaced.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The logarithm of the trapezoidal integral of <code>exp(log_y)</code> over <code>x</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; log_y = np.array([0.1, 0.2, 0.3])\n&gt;&gt;&gt; x = np.array([1.0, 2.0, 3.0])\n&gt;&gt;&gt; log_trapz_exp(log_y, x)\n0.8956461395871966\n</code></pre> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def log_trapz_exp(\n        log_y: np.ndarray,\n        x: np.ndarray,\n    ) -&gt; np.float64:\n    '''\n    Compute the log of the trapezoidal integral of the exponential of `log_y` over `x`.\n\n    Parameters\n    ----------\n    log_y : np.ndarray\n        Logarithm of the values to be integrated.\n    x : np.ndarray\n        Points at which `log_y` is defined. It is assumed that `x` is uniformly spaced.\n\n    Returns\n    -------\n    float\n        The logarithm of the trapezoidal integral of `exp(log_y)` over `x`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; log_y = np.array([0.1, 0.2, 0.3])\n    &gt;&gt;&gt; x = np.array([1.0, 2.0, 3.0])\n    &gt;&gt;&gt; log_trapz_exp(log_y, x)\n    0.8956461395871966\n    '''\n    log_dx = np.log(x[1] - x[0])  # assume uniform spacing\n    log_weights = np.zeros(len(x))\n    log_weights[[0,-1]] = np.log(0.5)\n    return log_dx + sp.special.logsumexp(log_y + log_weights)\n</code></pre>"},{"location":"jelli/utils/jax_helpers/","title":"jelli.utils.jax_helpers","text":""},{"location":"jelli/utils/par_helpers/","title":"jelli.utils.par_helpers","text":""},{"location":"jelli/utils/par_helpers/#jelli.utils.par_helpers.get_par_monomial_indices","title":"<code>get_par_monomial_indices(keys_pars, keys_coeff)</code>","text":"<p>Computes sorted indices mapping linear parameters to bilinear ones that exist in the provided coefficient list.</p> <p>Parameters:</p> Name Type Description Default <code>keys_pars</code> <code>list</code> <p>List of linear parameter keys, each element is a tuple <code>(w, c)</code>, where <code>w</code> is the parameter name and <code>c</code> is <code>R</code> for real or <code>I</code> for imaginary.</p> required <code>keys_coeff</code> <code>list</code> <p>List of bilinear coefficient keys. Each element is a tuple <code>(w1, w2, c)</code>, where <code>w1</code> and <code>w2</code> are the parameter names and <code>c</code> is <code>RR</code>, <code>RI</code>, <code>IR</code>, or <code>II</code>, denoting all possible interferences.</p> required <p>Returns:</p> Type Description <code>`np.ndarray`</code> <p>Sorted indices of bilinear coefficients that match <code>keys_coeff</code>.</p> Source code in <code>jelli/utils/par_helpers.py</code> <pre><code>def get_par_monomial_indices(keys_pars, keys_coeff):\n    \"\"\"\n    Computes sorted indices mapping linear parameters\n    to bilinear ones that exist in the provided coefficient list.\n\n    Parameters\n    ----------\n    keys_pars : list\n        List of linear parameter keys, each element is a tuple `(w, c)`,\n        where `w` is the parameter name and `c` is `R` for real or `I` for imaginary.\n    keys_coeff : list\n        List of bilinear coefficient keys. Each element is a tuple `(w1, w2, c)`,\n        where `w1` and `w2` are the parameter names and `c` is `RR`, `RI`, `IR`, or `II`,\n        denoting all possible interferences.\n\n    Returns\n    -------\n    `np.ndarray`\n        Sorted indices of bilinear coefficients that match `keys_coeff`.\n    \"\"\"\n    # Generate all possible bilinear combinations of keys_pars\n    keys_pars_bilinears = keys_array(keys_product(\n        keys_pars, keys_pars\n    ))\n    bilin_bools = keys_isin(keys_pars_bilinears, keys_coeff)\n    # Take elements of keys_pars_bilinears that exist in keys_coeff and obtain indices that sort them\n    sort_indices = np.argsort(keys_pars_bilinears[bilin_bools])\n    bilin_indices = np.where(bilin_bools)[0]\n    bilin_sort_indices = bilin_indices[sort_indices]\n    return bilin_sort_indices\n</code></pre>"},{"location":"jelli/utils/par_helpers/#jelli.utils.par_helpers.keys_array","title":"<code>keys_array(keys)</code>","text":"<p>Converts a list of tuples into a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list</code> <p>A list containing tuples.</p> required <p>Returns:</p> Type Description <code>`np.ndarray`</code> <p>A numpy array with dtype=tuple containing the provided keys.</p> Source code in <code>jelli/utils/par_helpers.py</code> <pre><code>def keys_array(keys):\n    \"\"\"\n    Converts a list of tuples into a numpy array.\n\n    Parameters\n    ----------\n    keys : list\n        A list containing tuples.\n\n    Returns\n    -------\n    `np.ndarray`\n        A numpy array with dtype=tuple containing the provided keys.\n    \"\"\"\n    array = np.empty(len(keys), dtype=tuple)\n    array[:] = keys\n    return array\n</code></pre>"},{"location":"jelli/utils/par_helpers/#jelli.utils.par_helpers.keys_isin","title":"<code>keys_isin(keys_a, keys_b)</code>","text":"<p>Checks if elements in <code>keys_a</code> exist in <code>keys_b</code>.</p> <p>Parameters:</p> Name Type Description Default <code>keys_a</code> <code>list</code> <p>List of keys to check.</p> required <code>keys_b</code> <code>list</code> <p>List of reference keys.</p> required <p>Returns:</p> Type Description <code>`np.ndarray`</code> <p>Boolean numpy array indicating presence of each key in <code>keys_a</code> within <code>keys_b</code>.</p> Source code in <code>jelli/utils/par_helpers.py</code> <pre><code>def keys_isin(keys_a, keys_b):\n    \"\"\"\n    Checks if elements in `keys_a` exist in `keys_b`.\n\n    Parameters\n    ----------\n    keys_a : list\n        List of keys to check.\n    keys_b : list\n        List of reference keys.\n\n    Returns\n    -------\n    `np.ndarray`\n        Boolean numpy array indicating presence of each key in `keys_a` within `keys_b`.\n    \"\"\"\n    set_b = set(keys_b)\n    res = np.array([item in set_b for item in keys_a])\n    return res if res.size &gt; 0 else np.array([], dtype=bool)\n</code></pre>"},{"location":"jelli/utils/par_helpers/#jelli.utils.par_helpers.keys_product","title":"<code>keys_product(keys_a, keys_b)</code>","text":"<p>Computes the Cartesian product of two sets of keys, producing bilinear combinations.</p> <p>Parameters:</p> Name Type Description Default <code>keys_a</code> <p>A list where each element is a tuple of the form (w, c).</p> required <code>keys_b</code> <p>Another list with elements of the form (w, c).</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of bilinear combinations in the form (w_a, w_b, c_a + c_b).</p> Source code in <code>jelli/utils/par_helpers.py</code> <pre><code>def keys_product(keys_a, keys_b):\n    \"\"\"\n    Computes the Cartesian product of two sets of keys, producing bilinear combinations.\n\n    Parameters\n    ----------\n    keys_a: list\n        A list where each element is a tuple of the form (w, c).\n    keys_b: list\n        Another list with elements of the form (w, c).\n\n    Returns\n    -------\n    list\n        A list of bilinear combinations in the form (w_a, w_b, c_a + c_b).\n    \"\"\"\n    if len(keys_a[0]) == 2:\n        return [\n            (w_a, w_b, c_a+c_b)\n            for ((w_a, c_a), (w_b, c_b)) in product(keys_a, keys_b)\n        ]\n    else:\n        raise ValueError(\"keys must be of the form (w,c)\")\n</code></pre>"},{"location":"jelli/utils/probability/","title":"jelli.utils.probability","text":""},{"location":"jelli/utils/probability/#jelli.utils.probability.GammaDistribution","title":"<code>GammaDistribution</code>","text":"<p>               Bases: <code>ProbabilityDistribution</code></p> <p>A Gamma distribution defined like the <code>gamma</code> distribution in <code>scipy.stats</code> (with parameters <code>a</code>, <code>loc</code>, <code>scale</code>).</p> <p>The <code>central_value</code> attribute returns the location of the mode.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>class GammaDistribution(ProbabilityDistribution):\n    r\"\"\"A Gamma distribution defined like the `gamma` distribution in\n    `scipy.stats` (with parameters `a`, `loc`, `scale`).\n\n    The `central_value` attribute returns the location of the mode.\n    \"\"\"\n\n    def __init__(self, a, loc, scale):\n        if loc &gt; 0:\n            raise ValueError(\"loc must be negative or zero\")\n        # \"frozen\" scipy distribution object\n        self.scipy_dist = scipy.stats.gamma(a=a, loc=loc, scale=scale)\n        mode = loc + (a-1)*scale\n        # support extends until the CDF is roughly \"6 sigma\"\n        support_min = min(self.scipy_dist.ppf(1e-9), mode)\n        support_max = self.scipy_dist.ppf(1-1e-9)\n        super().__init__(central_value=mode, # the mode\n                         support=(support_min, support_max))\n        self.a = a\n        self.loc = loc\n        self.scale = scale\n\n    def __repr__(self):\n        return 'flavio.statistics.probability.GammaDistribution' + \\\n               '({}, {}, {})'.format(self.a, self.loc, self.scale)\n\n    def get_random(self, size):\n        return self.scipy_dist.rvs(size=size)\n\n    def cdf(self, x):\n        return self.scipy_dist.cdf(x)\n\n    def ppf(self, x):\n        return self.scipy_dist.ppf(x)\n\n    def logpdf(self, x):\n        return self.scipy_dist.logpdf(x)\n\n    def _find_error_cdf(self, confidence_level):\n        # find the value of the CDF at the position of the left boundary\n        # of the `confidence_level`% CL range by demanding that the value\n        # of the PDF is the same at the two boundaries\n        def x_left(a):\n            return self.ppf(a)\n        def x_right(a):\n            return self.ppf(a + confidence_level)\n        def diff_logpdf(a):\n            logpdf_x_left = self.logpdf(x_left(a))\n            logpdf_x_right = self.logpdf(x_right(a))\n            return logpdf_x_left - logpdf_x_right\n        return scipy.optimize.brentq(diff_logpdf, 0,  1 - confidence_level-1e-6)\n\n    def get_error_left(self, nsigma=1, **kwargs):\n        \"\"\"Return the lower error\"\"\"\n        a = self._find_error_cdf(confidence_level(nsigma))\n        return self.central_value - self.ppf(a)\n\n    def get_error_right(self, nsigma=1, **kwargs):\n        \"\"\"Return the upper error\"\"\"\n        a = self._find_error_cdf(confidence_level(nsigma))\n        return self.ppf(a + confidence_level(nsigma)) - self.central_value\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.GammaDistribution.get_error_left","title":"<code>get_error_left(nsigma=1, **kwargs)</code>","text":"<p>Return the lower error</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_error_left(self, nsigma=1, **kwargs):\n    \"\"\"Return the lower error\"\"\"\n    a = self._find_error_cdf(confidence_level(nsigma))\n    return self.central_value - self.ppf(a)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.GammaDistribution.get_error_right","title":"<code>get_error_right(nsigma=1, **kwargs)</code>","text":"<p>Return the upper error</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_error_right(self, nsigma=1, **kwargs):\n    \"\"\"Return the upper error\"\"\"\n    a = self._find_error_cdf(confidence_level(nsigma))\n    return self.ppf(a + confidence_level(nsigma)) - self.central_value\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NormalDistribution","title":"<code>NormalDistribution</code>","text":"<p>               Bases: <code>ProbabilityDistribution</code></p> <p>Univariate normal or Gaussian distribution.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>class NormalDistribution(ProbabilityDistribution):\n    \"\"\"Univariate normal or Gaussian distribution.\"\"\"\n\n    def __init__(self, central_value, standard_deviation):\n        \"\"\"Initialize the distribution.\n\n        Parameters:\n\n        - central_value: location (mode and mean)\n        - standard_deviation: standard deviation\n        \"\"\"\n        super().__init__(central_value,\n                         support=(central_value - 6 * standard_deviation,\n                                  central_value + 6 * standard_deviation))\n        if standard_deviation &lt;= 0:\n            raise ValueError(\"Standard deviation must be positive number\")\n        self.standard_deviation = standard_deviation\n\n    def __repr__(self):\n        return 'flavio.statistics.probability.NormalDistribution' + \\\n               '({}, {})'.format(self.central_value, self.standard_deviation)\n\n    def get_random(self, size=None):\n        return np.random.normal(self.central_value, self.standard_deviation, size)\n\n    def logpdf(self, x):\n        return normal_logpdf(x, self.central_value, self.standard_deviation)\n\n    def pdf(self, x):\n        return normal_pdf(x, self.central_value, self.standard_deviation)\n\n    def cdf(self, x):\n        return scipy.stats.norm.cdf(x, self.central_value, self.standard_deviation)\n\n    def ppf(self, x):\n        return scipy.stats.norm.ppf(x, self.central_value, self.standard_deviation)\n\n    def get_error_left(self, nsigma=1, **kwargs):\n        \"\"\"Return the lower error\"\"\"\n        return nsigma * self.standard_deviation\n\n    def get_error_right(self, nsigma=1, **kwargs):\n        \"\"\"Return the upper error\"\"\"\n        return nsigma * self.standard_deviation\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NormalDistribution.__init__","title":"<code>__init__(central_value, standard_deviation)</code>","text":"<p>Initialize the distribution.</p> <p>Parameters:</p> <ul> <li>central_value: location (mode and mean)</li> <li>standard_deviation: standard deviation</li> </ul> Source code in <code>jelli/utils/probability.py</code> <pre><code>def __init__(self, central_value, standard_deviation):\n    \"\"\"Initialize the distribution.\n\n    Parameters:\n\n    - central_value: location (mode and mean)\n    - standard_deviation: standard deviation\n    \"\"\"\n    super().__init__(central_value,\n                     support=(central_value - 6 * standard_deviation,\n                              central_value + 6 * standard_deviation))\n    if standard_deviation &lt;= 0:\n        raise ValueError(\"Standard deviation must be positive number\")\n    self.standard_deviation = standard_deviation\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NormalDistribution.get_error_left","title":"<code>get_error_left(nsigma=1, **kwargs)</code>","text":"<p>Return the lower error</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_error_left(self, nsigma=1, **kwargs):\n    \"\"\"Return the lower error\"\"\"\n    return nsigma * self.standard_deviation\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NormalDistribution.get_error_right","title":"<code>get_error_right(nsigma=1, **kwargs)</code>","text":"<p>Return the upper error</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_error_right(self, nsigma=1, **kwargs):\n    \"\"\"Return the upper error\"\"\"\n    return nsigma * self.standard_deviation\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NumericalDistribution","title":"<code>NumericalDistribution</code>","text":"<p>               Bases: <code>ProbabilityDistribution</code></p> <p>Univariate distribution defined in terms of numerical values for the PDF.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>class NumericalDistribution(ProbabilityDistribution):\n    \"\"\"Univariate distribution defined in terms of numerical values for the\n    PDF.\"\"\"\n\n    def __init__(self, x, y, central_value=None):\n        \"\"\"Initialize a 1D numerical distribution.\n\n        Parameters:\n\n        - `x`: x-axis values. Must be a 1D array of real values in strictly\n          ascending order (but not necessarily evenly spaced)\n        - `y`: PDF values. Must be a 1D array of real positive values with the\n          same length as `x`\n        - central_value: if None (default), will be set to the mode of the\n          distribution, i.e. the x-value where y is largest (by looking up\n          the input arrays, i.e. without interpolation!)\n        \"\"\"\n        self.x = x\n        self.y = y\n        if central_value is not None:\n            if x[0] &lt;= central_value &lt;= x[-1]:\n                super().__init__(central_value=central_value,\n                                 support=(x[0], x[-1]))\n            else:\n                raise ValueError(\"Central value must be within range provided\")\n        else:\n            mode = x[np.argmax(y)]\n            super().__init__(central_value=mode, support=(x[0], x[-1]))\n        self.y_norm = y /  np.trapz(y, x=x)  # normalize PDF to 1\n        self.y_norm[self.y_norm &lt; 0] = 0\n        self.pdf_interp = interp1d(x, self.y_norm,\n                                        fill_value=0, bounds_error=False)\n        _cdf = np.zeros(len(x))\n        _cdf[1:] = np.cumsum(self.y_norm[:-1] * np.diff(x))\n        _cdf = _cdf/_cdf[-1] # normalize CDF to 1\n        self.ppf_interp = interp1d(_cdf, x)\n        self.cdf_interp = interp1d(x, _cdf)\n\n    def __repr__(self):\n        return 'flavio.statistics.probability.NumericalDistribution' + \\\n               '({}, {})'.format(self.x, self.y)\n\n    def get_random(self, size=None):\n        \"\"\"Draw a random number from the distribution.\n\n        If size is not None but an integer N, return an array of N numbers.\"\"\"\n        r = np.random.uniform(size=size)\n        return self.ppf_interp(r)\n\n    def ppf(self, x):\n        return self.ppf_interp(x)\n\n    def cdf(self, x):\n        return self.cdf_interp(x)\n\n    def pdf(self, x):\n        return self.pdf_interp(x)\n\n    def logpdf(self, x):\n        # ignore warning from log(0)=-np.inf\n        with np.errstate(divide='ignore', invalid='ignore'):\n            return np.log(self.pdf_interp(x))\n\n    def _find_error_cdf(self, confidence_level):\n        # find the value of the CDF at the position of the left boundary\n        # of the `confidence_level`% CL range by demanding that the value\n        # of the PDF is the same at the two boundaries\n        def x_left(a):\n            return self.ppf(a)\n        def x_right(a):\n            return self.ppf(a + confidence_level)\n        def diff_logpdf(a):\n            logpdf_x_left = self.logpdf(x_left(a))\n            logpdf_x_right = self.logpdf(x_right(a))\n            return logpdf_x_left - logpdf_x_right\n        return scipy.optimize.brentq(diff_logpdf, 0,  1 - confidence_level-1e-6)\n\n    def get_error_left(self, nsigma=1, method='central'):\n        \"\"\"Return the lower error.\n\n        'method' should be one of:\n\n        - 'central' for a central interval (same probability on both sides of\n          the central value)\n        - 'hpd' for highest posterior density, i.e. probability is larger inside\n          the interval than outside\n        - 'limit' for a one-sided error, i.e. a lower limit\"\"\"\n        if method == 'limit':\n            return self.central_value - self.ppf(1 - confidence_level(nsigma))\n        cdf_central = self.cdf(self.central_value)\n        err_left = self.central_value - self.ppf(cdf_central * (1 - confidence_level(nsigma)))\n        if method == 'central':\n            return err_left\n        elif method == 'hpd':\n            if self.pdf(self.central_value + self.get_error_right(method='central')) == self.pdf(self.central_value - err_left):\n                return err_left\n            try:\n                a = self._find_error_cdf(confidence_level(nsigma))\n            except ValueError:\n                return np.nan\n            return self.central_value - self.ppf(a)\n        else:\n            raise ValueError(\"Method \" + str(method) + \" unknown\")\n\n    def get_error_right(self, nsigma=1, method='central'):\n        \"\"\"Return the upper error\n\n        'method' should be one of:\n\n        - 'central' for a central interval (same probability on both sides of\n          the central value)\n        - 'hpd' for highest posterior density, i.e. probability is larger inside\n          the interval than outside\n        - 'limit' for a one-sided error, i.e. an upper limit\"\"\"\n        if method == 'limit':\n            return self.ppf(confidence_level(nsigma)) - self.central_value\n        cdf_central = self.cdf(self.central_value)\n        err_right = self.ppf(cdf_central + (1 - cdf_central) * confidence_level(nsigma)) - self.central_value\n        if method == 'central':\n            return err_right\n        elif method == 'hpd':\n            if self.pdf(self.central_value - self.get_error_left(method='central')) == self.pdf(self.central_value + err_right):\n                return err_right\n            try:\n                a = self._find_error_cdf(confidence_level(nsigma))\n            except ValueError:\n                return np.nan\n            return self.ppf(a + confidence_level(nsigma)) - self.central_value\n        else:\n            raise ValueError(\"Method \" + str(method) + \" unknown\")\n\n    @classmethod\n    def from_pd(cls, pd, nsteps=1000):\n        if isinstance(pd, NumericalDistribution):\n            return pd\n        _x = np.linspace(pd.support[0], pd.support[-1], nsteps)\n        _y = np.exp(pd.logpdf(_x))\n        return cls(central_value=pd.central_value, x=_x, y=_y)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NumericalDistribution.__init__","title":"<code>__init__(x, y, central_value=None)</code>","text":"<p>Initialize a 1D numerical distribution.</p> <p>Parameters:</p> <ul> <li><code>x</code>: x-axis values. Must be a 1D array of real values in strictly   ascending order (but not necessarily evenly spaced)</li> <li><code>y</code>: PDF values. Must be a 1D array of real positive values with the   same length as <code>x</code></li> <li>central_value: if None (default), will be set to the mode of the   distribution, i.e. the x-value where y is largest (by looking up   the input arrays, i.e. without interpolation!)</li> </ul> Source code in <code>jelli/utils/probability.py</code> <pre><code>def __init__(self, x, y, central_value=None):\n    \"\"\"Initialize a 1D numerical distribution.\n\n    Parameters:\n\n    - `x`: x-axis values. Must be a 1D array of real values in strictly\n      ascending order (but not necessarily evenly spaced)\n    - `y`: PDF values. Must be a 1D array of real positive values with the\n      same length as `x`\n    - central_value: if None (default), will be set to the mode of the\n      distribution, i.e. the x-value where y is largest (by looking up\n      the input arrays, i.e. without interpolation!)\n    \"\"\"\n    self.x = x\n    self.y = y\n    if central_value is not None:\n        if x[0] &lt;= central_value &lt;= x[-1]:\n            super().__init__(central_value=central_value,\n                             support=(x[0], x[-1]))\n        else:\n            raise ValueError(\"Central value must be within range provided\")\n    else:\n        mode = x[np.argmax(y)]\n        super().__init__(central_value=mode, support=(x[0], x[-1]))\n    self.y_norm = y /  np.trapz(y, x=x)  # normalize PDF to 1\n    self.y_norm[self.y_norm &lt; 0] = 0\n    self.pdf_interp = interp1d(x, self.y_norm,\n                                    fill_value=0, bounds_error=False)\n    _cdf = np.zeros(len(x))\n    _cdf[1:] = np.cumsum(self.y_norm[:-1] * np.diff(x))\n    _cdf = _cdf/_cdf[-1] # normalize CDF to 1\n    self.ppf_interp = interp1d(_cdf, x)\n    self.cdf_interp = interp1d(x, _cdf)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NumericalDistribution.get_error_left","title":"<code>get_error_left(nsigma=1, method='central')</code>","text":"<p>Return the lower error.</p> <p>'method' should be one of:</p> <ul> <li>'central' for a central interval (same probability on both sides of   the central value)</li> <li>'hpd' for highest posterior density, i.e. probability is larger inside   the interval than outside</li> <li>'limit' for a one-sided error, i.e. a lower limit</li> </ul> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_error_left(self, nsigma=1, method='central'):\n    \"\"\"Return the lower error.\n\n    'method' should be one of:\n\n    - 'central' for a central interval (same probability on both sides of\n      the central value)\n    - 'hpd' for highest posterior density, i.e. probability is larger inside\n      the interval than outside\n    - 'limit' for a one-sided error, i.e. a lower limit\"\"\"\n    if method == 'limit':\n        return self.central_value - self.ppf(1 - confidence_level(nsigma))\n    cdf_central = self.cdf(self.central_value)\n    err_left = self.central_value - self.ppf(cdf_central * (1 - confidence_level(nsigma)))\n    if method == 'central':\n        return err_left\n    elif method == 'hpd':\n        if self.pdf(self.central_value + self.get_error_right(method='central')) == self.pdf(self.central_value - err_left):\n            return err_left\n        try:\n            a = self._find_error_cdf(confidence_level(nsigma))\n        except ValueError:\n            return np.nan\n        return self.central_value - self.ppf(a)\n    else:\n        raise ValueError(\"Method \" + str(method) + \" unknown\")\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NumericalDistribution.get_error_right","title":"<code>get_error_right(nsigma=1, method='central')</code>","text":"<p>Return the upper error</p> <p>'method' should be one of:</p> <ul> <li>'central' for a central interval (same probability on both sides of   the central value)</li> <li>'hpd' for highest posterior density, i.e. probability is larger inside   the interval than outside</li> <li>'limit' for a one-sided error, i.e. an upper limit</li> </ul> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_error_right(self, nsigma=1, method='central'):\n    \"\"\"Return the upper error\n\n    'method' should be one of:\n\n    - 'central' for a central interval (same probability on both sides of\n      the central value)\n    - 'hpd' for highest posterior density, i.e. probability is larger inside\n      the interval than outside\n    - 'limit' for a one-sided error, i.e. an upper limit\"\"\"\n    if method == 'limit':\n        return self.ppf(confidence_level(nsigma)) - self.central_value\n    cdf_central = self.cdf(self.central_value)\n    err_right = self.ppf(cdf_central + (1 - cdf_central) * confidence_level(nsigma)) - self.central_value\n    if method == 'central':\n        return err_right\n    elif method == 'hpd':\n        if self.pdf(self.central_value - self.get_error_left(method='central')) == self.pdf(self.central_value + err_right):\n            return err_right\n        try:\n            a = self._find_error_cdf(confidence_level(nsigma))\n        except ValueError:\n            return np.nan\n        return self.ppf(a + confidence_level(nsigma)) - self.central_value\n    else:\n        raise ValueError(\"Method \" + str(method) + \" unknown\")\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NumericalDistribution.get_random","title":"<code>get_random(size=None)</code>","text":"<p>Draw a random number from the distribution.</p> <p>If size is not None but an integer N, return an array of N numbers.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_random(self, size=None):\n    \"\"\"Draw a random number from the distribution.\n\n    If size is not None but an integer N, return an array of N numbers.\"\"\"\n    r = np.random.uniform(size=size)\n    return self.ppf_interp(r)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution","title":"<code>ProbabilityDistribution</code>","text":"<p>               Bases: <code>object</code></p> <p>Common base class for all probability distributions</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>class ProbabilityDistribution(object):\n    \"\"\"Common base class for all probability distributions\"\"\"\n\n    def __init__(self, central_value, support):\n        self.central_value = central_value\n        self.support = support\n\n    @classmethod\n    def get_subclasses(cls):\n        \"\"\"Return all subclasses (including subclasses of subclasses).\"\"\"\n        for subclass in cls.__subclasses__():\n            yield from subclass.get_subclasses()\n            yield subclass\n\n    def get_central(self):\n        return self.central_value\n\n    @property\n    def error_left(self):\n        \"\"\"Return the lower error\"\"\"\n        return self.get_error_left()\n\n    @property\n    def error_right(self):\n        \"\"\"Return the upper error\"\"\"\n        return self.get_error_right()\n\n    @classmethod\n    def class_to_string(cls):\n        \"\"\"Get a string name for a given ProbabilityDistribution subclass.\n\n        This converts camel case to underscore and removes the word\n        'distribution'.\n\n        Example: class_to_string(AsymmetricNormalDistribution) returns\n        'asymmetric_normal'.\n        \"\"\"\n        name = _camel_to_underscore(cls.__name__)\n        return name.replace('_distribution', '')\n\n    def get_dict(self, distribution=False, iterate=False, arraytolist=False):\n        \"\"\"Get an ordered dictionary with arguments and values needed to\n        the instantiate the distribution.\n\n        Optional arguments (default to False):\n\n        - `distribution`: add a 'distribution' key to the dictionary with the\n        value being the string representation of the distribution's name\n        (e.g. 'asymmetric_normal').\n        - `iterate`: If ProbabilityDistribution instances are among the\n        arguments (e.g. for KernelDensityEstimate), return the instance's\n        get_dict instead of the instance as value.\n        - `arraytolist`: convert numpy arrays to lists\n        \"\"\"\n        args = inspect.signature(self.__class__).parameters.keys()\n        d = self.__dict__\n        od = OrderedDict()\n        if distribution:\n            od['distribution'] = self.class_to_string()\n        od.update(OrderedDict((a, d[a]) for a in args))\n        if iterate:\n            for k in od:\n                if isinstance(od[k], ProbabilityDistribution):\n                    od[k] = od[k].get_dict(distribution=True)\n        if arraytolist:\n            for k in od:\n                if isinstance(od[k], np.ndarray):\n                    od[k] = od[k].tolist()\n                if isinstance(od[k], list):\n                    for i, x in enumerate(od[k]):\n                        if isinstance(x, np.ndarray):\n                            od[k][i] = od[k][i].tolist()\n        for k in od:\n            if isinstance(od[k], int):\n                od[k] = int(od[k])\n            elif isinstance(od[k], float):\n                od[k] = float(od[k])\n            if isinstance(od[k], list):\n                for i, x in enumerate(od[k]):\n                    if isinstance(x, float):\n                        od[k][i] = float(od[k][i])\n                    elif isinstance(x, int):\n                        od[k][i] = int(od[k][i])\n        return od\n\n    def get_yaml(self, *args, **kwargs):\n        \"\"\"Get a YAML string representing the dictionary returned by the\n        get_dict method.\n\n        Arguments will be passed to `yaml.dump`.\"\"\"\n        od = self.get_dict(distribution=True, iterate=True, arraytolist=True)\n        return yaml.dump(od, *args, **kwargs)\n\n    def delta_logpdf(self, x, **kwargs):\n        exclude = kwargs.get('exclude', None)\n        if exclude is not None:\n            d = len(self.central_value)\n            cv = [self.central_value[i] for i in range(d) if i not in exclude]\n        else:\n            cv = self.central_value\n        return self.logpdf(x, **kwargs) - self.logpdf(cv, **kwargs)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution.error_left","title":"<code>error_left</code>  <code>property</code>","text":"<p>Return the lower error</p>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution.error_right","title":"<code>error_right</code>  <code>property</code>","text":"<p>Return the upper error</p>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution.class_to_string","title":"<code>class_to_string()</code>  <code>classmethod</code>","text":"<p>Get a string name for a given ProbabilityDistribution subclass.</p> <p>This converts camel case to underscore and removes the word 'distribution'.</p> <p>Example: class_to_string(AsymmetricNormalDistribution) returns 'asymmetric_normal'.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>@classmethod\ndef class_to_string(cls):\n    \"\"\"Get a string name for a given ProbabilityDistribution subclass.\n\n    This converts camel case to underscore and removes the word\n    'distribution'.\n\n    Example: class_to_string(AsymmetricNormalDistribution) returns\n    'asymmetric_normal'.\n    \"\"\"\n    name = _camel_to_underscore(cls.__name__)\n    return name.replace('_distribution', '')\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution.get_dict","title":"<code>get_dict(distribution=False, iterate=False, arraytolist=False)</code>","text":"<p>Get an ordered dictionary with arguments and values needed to the instantiate the distribution.</p> <p>Optional arguments (default to False):</p> <ul> <li><code>distribution</code>: add a 'distribution' key to the dictionary with the value being the string representation of the distribution's name (e.g. 'asymmetric_normal').</li> <li><code>iterate</code>: If ProbabilityDistribution instances are among the arguments (e.g. for KernelDensityEstimate), return the instance's get_dict instead of the instance as value.</li> <li><code>arraytolist</code>: convert numpy arrays to lists</li> </ul> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_dict(self, distribution=False, iterate=False, arraytolist=False):\n    \"\"\"Get an ordered dictionary with arguments and values needed to\n    the instantiate the distribution.\n\n    Optional arguments (default to False):\n\n    - `distribution`: add a 'distribution' key to the dictionary with the\n    value being the string representation of the distribution's name\n    (e.g. 'asymmetric_normal').\n    - `iterate`: If ProbabilityDistribution instances are among the\n    arguments (e.g. for KernelDensityEstimate), return the instance's\n    get_dict instead of the instance as value.\n    - `arraytolist`: convert numpy arrays to lists\n    \"\"\"\n    args = inspect.signature(self.__class__).parameters.keys()\n    d = self.__dict__\n    od = OrderedDict()\n    if distribution:\n        od['distribution'] = self.class_to_string()\n    od.update(OrderedDict((a, d[a]) for a in args))\n    if iterate:\n        for k in od:\n            if isinstance(od[k], ProbabilityDistribution):\n                od[k] = od[k].get_dict(distribution=True)\n    if arraytolist:\n        for k in od:\n            if isinstance(od[k], np.ndarray):\n                od[k] = od[k].tolist()\n            if isinstance(od[k], list):\n                for i, x in enumerate(od[k]):\n                    if isinstance(x, np.ndarray):\n                        od[k][i] = od[k][i].tolist()\n    for k in od:\n        if isinstance(od[k], int):\n            od[k] = int(od[k])\n        elif isinstance(od[k], float):\n            od[k] = float(od[k])\n        if isinstance(od[k], list):\n            for i, x in enumerate(od[k]):\n                if isinstance(x, float):\n                    od[k][i] = float(od[k][i])\n                elif isinstance(x, int):\n                    od[k][i] = int(od[k][i])\n    return od\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution.get_subclasses","title":"<code>get_subclasses()</code>  <code>classmethod</code>","text":"<p>Return all subclasses (including subclasses of subclasses).</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>@classmethod\ndef get_subclasses(cls):\n    \"\"\"Return all subclasses (including subclasses of subclasses).\"\"\"\n    for subclass in cls.__subclasses__():\n        yield from subclass.get_subclasses()\n        yield subclass\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution.get_yaml","title":"<code>get_yaml(*args, **kwargs)</code>","text":"<p>Get a YAML string representing the dictionary returned by the get_dict method.</p> <p>Arguments will be passed to <code>yaml.dump</code>.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_yaml(self, *args, **kwargs):\n    \"\"\"Get a YAML string representing the dictionary returned by the\n    get_dict method.\n\n    Arguments will be passed to `yaml.dump`.\"\"\"\n    od = self.get_dict(distribution=True, iterate=True, arraytolist=True)\n    return yaml.dump(od, *args, **kwargs)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.confidence_level","title":"<code>confidence_level(nsigma)</code>  <code>cached</code>","text":"<p>Return the confidence level corresponding to a number of sigmas, i.e. the probability contained in the normal distribution between \\(-n\\sigma\\) and \\(+n\\sigma\\).</p> <p>Example: <code>confidence_level(1)</code> returns approximately 0.68.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>@lru_cache(maxsize=10)\ndef confidence_level(nsigma):\n    r\"\"\"Return the confidence level corresponding to a number of sigmas,\n    i.e. the probability contained in the normal distribution between $-n\\sigma$\n    and $+n\\sigma$.\n\n    Example: `confidence_level(1)` returns approximately 0.68.\"\"\"\n    return (scipy.stats.norm.cdf(nsigma)-0.5)*2\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.normal_logpdf","title":"<code>normal_logpdf(x, mu, sigma)</code>","text":"<p>Logarithm of the PDF of the normal distribution</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def normal_logpdf(x, mu, sigma):\n    \"\"\"Logarithm of the PDF of the normal distribution\"\"\"\n    # this turns out to be 2 orders of magnitude faster than scipy.stats.norm.logpdf\n    if isinstance(x, float):\n        _x = x\n    else:\n        _x = np.asarray(x)\n    return -(_x-mu)**2/sigma**2/2 - math.log(math.sqrt(2*math.pi)*sigma)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.normal_pdf","title":"<code>normal_pdf(x, mu, sigma)</code>","text":"<p>PDF of the normal distribution</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def normal_pdf(x, mu, sigma):\n    \"\"\"PDF of the normal distribution\"\"\"\n    # this turns out to be 2 orders of magnitude faster than scipy.stats.norm.logpdf\n    if isinstance(x, float):\n        _x = x\n    else:\n        _x = np.asarray(x)\n    return np.exp(-(_x-mu)**2/sigma**2/2)/(np.sqrt(2*math.pi)*sigma)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.string_to_class","title":"<code>string_to_class(string)</code>","text":"<p>Get a ProbabilityDistribution subclass from a string. This can either be the class name itself or a string in underscore format as returned from <code>class_to_string</code>.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def string_to_class(string):\n    \"\"\"Get a ProbabilityDistribution subclass from a string. This can\n    either be the class name itself or a string in underscore format\n    as returned from `class_to_string`.\"\"\"\n    try:\n        return eval(string)\n    except NameError:\n        pass\n    for c in ProbabilityDistribution.get_subclasses():\n        if c.class_to_string() == string:\n            return c\n    raise NameError(\"Distribution \" + string + \" not found.\")\n</code></pre>"}]}