{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"jelli - JAX-based EFT Likelihoods","text":"<p><code>jelli</code> is a Python package for building and evaluating likelihood functions in the Effective Field Theory (EFT) framework.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>EFT Framework: Construction of likelihoods in EFTs, such as the Standard Model Effective Field Theory (SMEFT) and Weak Effective Theory (WET).</li> <li>Flexibility: Supports arbitrary observable predictions provided in the POPxf data format, and a multitude of experimental likelihood assumptions.</li> <li>JAX Integration: Built on JAX for high-performance numerical computing.</li> <li>Differentiable: Fully differentiable likelihood functions due to JAX's autodiff, enabling efficient gradient and Hessian computations, gradient-based optimization and sampling, and more.</li> <li>Fast: Utilizes JAX's Just-In-Time (JIT) compilation for optimized performance.</li> <li>Multi-scale: Interfaced with rgevolve for fast renormalization group evolution using the evolution matrix formalism.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>The package can be installed via pip:</p> <pre><code>pip install jelli\n</code></pre>"},{"location":"#repository","title":"Repository","text":"<p>The source code is available on GitHub.</p>"},{"location":"#citation","title":"Citation","text":"<p>A paper describing <code>jelli</code> is in preparation.</p>"},{"location":"#bugs-and-feature-requests","title":"Bugs and feature requests","text":"<p>Please report bugs and request features via the GitHub issues page.</p>"},{"location":"#contributors","title":"Contributors","text":"<p>Authors:</p> <ul> <li>Aleks Smolkovic (@alekssmolkovic)</li> <li>Peter Stangl (@peterstangl)</li> </ul>"},{"location":"#license","title":"License","text":"<p><code>jelli</code> is licensed under the MIT License.</p>"},{"location":"jelli/core/compiled_likelihood/","title":"jelli.core.compiled_likelihood","text":""},{"location":"jelli/core/compiled_likelihood/#jelli.core.compiled_likelihood.CompiledLikelihood","title":"<code>CompiledLikelihood</code>","text":"<p>A class to retrieve JIT compiled likelihood functions for the global likelihood instance.</p> <p>Parameters:</p> Name Type Description Default <code>global_likelihood_instance</code> <code>`GlobalLikelihood`</code> <p>An instance of the <code>GlobalLikelihood</code> class.</p> required <code>par_list</code> <code>List[Tuple[str, str]]</code> <p>A list of tuples specifying the parameters to be considered. Each tuple contains the parameter name and its type (e.g., <code>('param1', 'R')</code> for a real parameter, <code>('param2', 'I')</code> for an imaginary parameter).</p> required <code>likelihood</code> <code>str or Tuple[str, ...]</code> <p>The likelihood to be used. Default is 'global'.</p> <code>'global'</code> <code>par_dep_cov</code> <code>bool</code> <p>If <code>True</code>, the covariance matrix depends on the parameters. Default is <code>False</code>.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>global_likelihood_instance</code> <code>`GlobalLikelihood`</code> <p>The instance of the <code>GlobalLikelihood</code> class.</p> <code>par_list</code> <code>List[Tuple[str, str]]</code> <p>The list of parameters considered.</p> <code>likelihood</code> <code>str or Tuple[str, ...]</code> <p>The likelihood used.</p> <code>par_dep_cov</code> <code>bool</code> <p>Indicates if the covariance matrix depends on the parameters.</p> <code>_negative_log_likelihood_function</code> <code>Callable</code> <p>The function to compute the negative log-likelihood.</p> <code>_log_likelihood_data</code> <code>dict</code> <p>The data required for the log-likelihood computation.</p> <code>_functions</code> <code>dict</code> <p>A cache for the compiled functions.</p> <p>Methods:</p> Name Description <code>negative_log_likelihood_value</code> <p>Get the jitted function for the negative log-likelihood value.</p> <code>negative_log_likelihood_grad</code> <p>Get the jitted function for the gradient of the negative log-likelihood.</p> <code>negative_log_likelihood_value_and_grad</code> <p>Get the jitted function for both the value and gradient of the negative log-likelihood.</p> <code>negative_log_likelihood_hessian</code> <p>Get the jitted function for the Hessian of the negative log-likelihood.</p> <code>observed_fisher_information</code> <p>Get the jitted function for the observed Fisher information (same as Hessian).</p> <code>negative_log_likelihood_inverse_hessian</code> <p>Get the jitted function for the inverse of the Hessian of the negative log-likelihood.</p> <code>asymptotic_covariance</code> <p>Get the jitted function for the asymptotic covariance (same as inverse Hessian).</p> <p>Examples:</p> <p>Initialize the <code>CompiledLikelihood</code> class with a <code>GlobalLikelihood</code> instance and a parameter list:</p> <pre><code>&gt;&gt;&gt; gl = GlobalLikelihood(...)\n&gt;&gt;&gt; par_list = [('param1', 'R'), ('param2', 'I')]\n&gt;&gt;&gt; likelihood = 'global'\n&gt;&gt;&gt; par_dep_cov = False\n&gt;&gt;&gt; compiled_likelihood = CompiledLikelihood(gl, par_list, likelihood, par_dep_cov)\n</code></pre> <p>Get the jitted function for the negative log-likelihood value:</p> <pre><code>&gt;&gt;&gt; nll_value_func = compiled_likelihood.negative_log_likelihood_value()\n&gt;&gt;&gt; nll_value = nll_value_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> <p>Get the jitted function for the gradient of the negative log-likelihood with respect to the parameters:</p> <pre><code>&gt;&gt;&gt; nll_grad_func = compiled_likelihood.negative_log_likelihood_grad(argnums=0)\n&gt;&gt;&gt; nll_grad = nll_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> <p>Get the jitted function for the gradient of the negative log-likelihood with respect to both the parameters and the scale:</p> <pre><code>&gt;&gt;&gt; nll_grad_func = compiled_likelihood.negative_log_likelihood_grad(argnums=(0, 1))\n&gt;&gt;&gt; nll_grad = nll_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> <p>Get the jitted function for both the value and gradient of the negative log-likelihood:</p> <pre><code>&gt;&gt;&gt; nll_value_and_grad_func = compiled_likelihood.negative_log_likelihood_value_and_grad(argnums=0)\n&gt;&gt;&gt; nll_value, nll_grad = nll_value_and_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> <p>Get the jitted function for the Hessian of the negative log-likelihood:</p> <pre><code>&gt;&gt;&gt; nll_hess_func = compiled_likelihood.negative_log_likelihood_hessian(argnums=0)\n&gt;&gt;&gt; nll_hess = nll_hess_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> <p>Get the jitted function for the observed Fisher information (same as Hessian):</p> <pre><code>&gt;&gt;&gt; fisher_info_func = compiled_likelihood.observed_fisher_information(argnums=0)\n&gt;&gt;&gt; fisher_info = fisher_info_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> <p>Get the jitted function for the inverse of the Hessian of the negative log-likelihood:</p> <pre><code>&gt;&gt;&gt; nll_inv_hess_func = compiled_likelihood.negative_log_likelihood_inverse_hessian(argnums=0)\n&gt;&gt;&gt; nll_inv_hess = nll_inv_hess_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> <p>Get the jitted function for the asymptotic covariance (same as inverse Hessian):</p> <pre><code>&gt;&gt;&gt; asymp_cov_func = compiled_likelihood.asymptotic_covariance(argnums=0)\n&gt;&gt;&gt; asymp_cov = asymp_cov_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> Source code in <code>jelli/core/compiled_likelihood.py</code> <pre><code>class CompiledLikelihood:\n    '''\n    A class to retrieve JIT compiled likelihood functions for the global likelihood instance.\n\n    Parameters\n    ----------\n    global_likelihood_instance : `GlobalLikelihood`\n        An instance of the `GlobalLikelihood` class.\n    par_list : List[Tuple[str, str]]\n        A list of tuples specifying the parameters to be considered. Each tuple contains the parameter name and its type (e.g., `('param1', 'R')` for a real parameter, `('param2', 'I')` for an imaginary parameter).\n    likelihood : str or Tuple[str, ...], optional\n        The likelihood to be used. Default is 'global'.\n    par_dep_cov : bool, optional\n        If `True`, the covariance matrix depends on the parameters. Default is `False`.\n\n    Attributes\n    ----------\n    global_likelihood_instance : `GlobalLikelihood`\n        The instance of the `GlobalLikelihood` class.\n    par_list : List[Tuple[str, str]]\n        The list of parameters considered.\n    likelihood : str or Tuple[str, ...]\n        The likelihood used.\n    par_dep_cov : bool\n        Indicates if the covariance matrix depends on the parameters.\n    _negative_log_likelihood_function : Callable\n        The function to compute the negative log-likelihood.\n    _log_likelihood_data : dict\n        The data required for the log-likelihood computation.\n    _functions : dict\n        A cache for the compiled functions.\n\n    Methods\n    -------\n    negative_log_likelihood_value(precompiled=True) -&gt; Callable\n        Get the jitted function for the negative log-likelihood value.\n    negative_log_likelihood_grad(argnums=0, precompiled=True) -&gt; Callable\n        Get the jitted function for the gradient of the negative log-likelihood.\n    negative_log_likelihood_value_and_grad(argnums=0, precompiled=True) -&gt; Callable\n        Get the jitted function for both the value and gradient of the negative log-likelihood.\n    negative_log_likelihood_hessian(argnums=0, precompiled=True) -&gt; Callable\n        Get the jitted function for the Hessian of the negative log-likelihood.\n    observed_fisher_information(argnums=0, precompiled=True) -&gt; Callable\n        Get the jitted function for the observed Fisher information (same as Hessian).\n    negative_log_likelihood_inverse_hessian(argnums=0, precompiled=True) -&gt; Callable\n        Get the jitted function for the inverse of the Hessian of the negative log-likelihood.\n    asymptotic_covariance(argnums=0, precompiled=True) -&gt; Callable\n        Get the jitted function for the asymptotic covariance (same as inverse Hessian).\n\n    Examples\n    --------\n    Initialize the `CompiledLikelihood` class with a `GlobalLikelihood` instance and a parameter list:\n\n    &gt;&gt;&gt; gl = GlobalLikelihood(...)\n    &gt;&gt;&gt; par_list = [('param1', 'R'), ('param2', 'I')]\n    &gt;&gt;&gt; likelihood = 'global'\n    &gt;&gt;&gt; par_dep_cov = False\n    &gt;&gt;&gt; compiled_likelihood = CompiledLikelihood(gl, par_list, likelihood, par_dep_cov)\n\n    Get the jitted function for the negative log-likelihood value:\n\n    &gt;&gt;&gt; nll_value_func = compiled_likelihood.negative_log_likelihood_value()\n    &gt;&gt;&gt; nll_value = nll_value_func(jnp.array([0.1, 0.2]), 1000.0)\n\n    Get the jitted function for the gradient of the negative log-likelihood with respect to the parameters:\n\n    &gt;&gt;&gt; nll_grad_func = compiled_likelihood.negative_log_likelihood_grad(argnums=0)\n    &gt;&gt;&gt; nll_grad = nll_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n\n    Get the jitted function for the gradient of the negative log-likelihood with respect to both the parameters and the scale:\n\n    &gt;&gt;&gt; nll_grad_func = compiled_likelihood.negative_log_likelihood_grad(argnums=(0, 1))\n    &gt;&gt;&gt; nll_grad = nll_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n\n    Get the jitted function for both the value and gradient of the negative log-likelihood:\n\n    &gt;&gt;&gt; nll_value_and_grad_func = compiled_likelihood.negative_log_likelihood_value_and_grad(argnums=0)\n    &gt;&gt;&gt; nll_value, nll_grad = nll_value_and_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n\n    Get the jitted function for the Hessian of the negative log-likelihood:\n\n    &gt;&gt;&gt; nll_hess_func = compiled_likelihood.negative_log_likelihood_hessian(argnums=0)\n    &gt;&gt;&gt; nll_hess = nll_hess_func(jnp.array([0.1, 0.2]), 1000.0)\n\n    Get the jitted function for the observed Fisher information (same as Hessian):\n\n    &gt;&gt;&gt; fisher_info_func = compiled_likelihood.observed_fisher_information(argnums=0)\n    &gt;&gt;&gt; fisher_info = fisher_info_func(jnp.array([0.1, 0.2]), 1000.0)\n\n    Get the jitted function for the inverse of the Hessian of the negative log-likelihood:\n\n    &gt;&gt;&gt; nll_inv_hess_func = compiled_likelihood.negative_log_likelihood_inverse_hessian(argnums=0)\n    &gt;&gt;&gt; nll_inv_hess = nll_inv_hess_func(jnp.array([0.1, 0.2]), 1000.0)\n\n    Get the jitted function for the asymptotic covariance (same as inverse Hessian):\n\n    &gt;&gt;&gt; asymp_cov_func = compiled_likelihood.asymptotic_covariance(argnums=0)\n    &gt;&gt;&gt; asymp_cov = asymp_cov_func(jnp.array([0.1, 0.2]), 1000.0)\n    '''\n\n    def __init__(\n        self,\n        global_likelihood_instance,\n        par_list: List[Tuple[str, str]],\n        likelihood: Union[str, Tuple[str, ...]] = 'global',\n        par_dep_cov: bool = False,\n    ):\n        '''\n        Initialize the `CompiledLikelihood` class.\n\n        Parameters\n        ----------\n        global_likelihood_instance : `jelli.core.global_likelihood.GlobalLikelihood`\n            An instance of the `GlobalLikelihood` class.\n        par_list : List[Tuple[str, str]]\n            A list of tuples specifying the parameters to be considered. Each tuple contains the parameter name and its type (e.g., `('param1', 'R')` for a real parameter, `('param2', 'I')` for an imaginary parameter).\n        likelihood : str or Tuple[str, ...], optional\n            The likelihood to be used. Default is 'global'.\n        par_dep_cov : bool, optional\n            If `True`, the covariance matrix depends on the parameters. Default is `False`.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n\n        Initialize the `CompiledLikelihood` class with a `GlobalLikelihood` instance and a parameter list:\n\n        &gt;&gt;&gt; gl = GlobalLikelihood(...)\n        &gt;&gt;&gt; par_list = [('param1', 'R'), ('param2', 'I')]\n        &gt;&gt;&gt; likelihood = 'global'\n        &gt;&gt;&gt; par_dep_cov = False\n        &gt;&gt;&gt; compiled_likelihood = CompiledLikelihood(gl, par_list, likelihood, par_dep_cov)\n        '''\n        self.global_likelihood_instance = global_likelihood_instance\n        self.par_list = par_list\n        self.likelihood = likelihood\n        self.par_dep_cov = par_dep_cov\n        self._negative_log_likelihood_function, self._log_likelihood_data = self.global_likelihood_instance.get_negative_log_likelihood(par_list, likelihood, par_dep_cov)\n        self._functions = {}\n\n    def negative_log_likelihood_value(\n        self,\n        precompiled: bool = True,\n    ) -&gt; Callable:\n        '''\n        Get the jitted function for the negative log-likelihood value.\n\n        Parameters\n        ----------\n        precompiled : bool, optional\n            If `True`, precompile the function. Default is `True`.\n\n        Returns\n        -------\n        Callable\n            The jitted function for the negative log-likelihood value.\n\n        Examples\n        --------\n        Get the jitted function for the negative log-likelihood value:\n\n        &gt;&gt;&gt; nll_value_func = compiled_likelihood.negative_log_likelihood_value()\n        &gt;&gt;&gt; nll_value = nll_value_func(jnp.array([0.1, 0.2]), 1000.0)\n        '''\n        if \"negative_log_likelihood_value\" not in self._functions:\n            f = partial(\n                jit(self._negative_log_likelihood_function),\n                log_likelihood_data=self._log_likelihood_data,\n            )\n            if precompiled:\n                f(jnp.zeros(len(self.par_list)), self.global_likelihood_instance._reference_scale)\n            self._functions[\"negative_log_likelihood_value\"] = f\n        return self._functions[\"negative_log_likelihood_value\"]\n\n    def negative_log_likelihood_grad(\n        self,\n        argnums: Union[int, Tuple[int, ...]] = 0,\n        precompiled: bool = True,\n    ) -&gt; Callable:\n        '''\n        Get the jitted function for the gradient of the negative log-likelihood.\n\n        Parameters\n        ----------\n        argnums : int or Tuple[int, ...], optional\n            The argument numbers with respect to which the gradient is computed. Default is `0` (the parameters). Use `(0, 1)` to include the scale as well.\n        precompiled : bool, optional\n            If `True`, precompile the function. Default is `True`.\n\n        Returns\n        -------\n        Callable\n            The jitted function for the gradient of the negative log-likelihood.\n\n        Examples\n        --------\n        Get the jitted function for the gradient of the negative log-likelihood with respect to the parameters:\n\n        &gt;&gt;&gt; nll_grad_func = compiled_likelihood.negative_log_likelihood_grad(argnums=0)\n        &gt;&gt;&gt; nll_grad = nll_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n\n        Get the jitted function for the gradient of the negative log-likelihood with respect to both the parameters and the scale:\n\n        &gt;&gt;&gt; nll_grad_func = compiled_likelihood.negative_log_likelihood_grad(argnums=(0, 1))\n        &gt;&gt;&gt; nll_grad = nll_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n        '''\n        if (\"negative_log_likelihood_grad\", argnums) not in self._functions:\n            f = partial(\n                jit(grad(self._negative_log_likelihood_function, argnums=argnums)),\n                log_likelihood_data=self._log_likelihood_data,\n            )\n            if precompiled:\n                f(jnp.zeros(len(self.par_list)), self.global_likelihood_instance._reference_scale)\n            self._functions[(\"negative_log_likelihood_grad\", argnums)] = f\n        return self._functions[(\"negative_log_likelihood_grad\", argnums)]\n\n    def negative_log_likelihood_value_and_grad(\n        self,\n        argnums: Union[int, Tuple[int, ...]] = 0,\n        precompiled: bool = True,\n    ) -&gt; Callable:\n        '''\n        Get the jitted function for both the value and gradient of the negative log-likelihood.\n\n        Parameters\n        ----------\n        argnums : int or Tuple[int, ...], optional\n            The argument numbers with respect to which the gradient is computed. Default is `0` (the parameters). Use `(0, 1)` to include the scale as well.\n        precompiled : bool, optional\n            If `True`, precompile the function. Default is `True`.\n\n        Returns\n        -------\n        Callable\n            The jitted function for both the value and gradient of the negative log-likelihood.\n\n        Examples\n        --------\n        Get the jitted function for both the value and gradient of the negative log-likelihood:\n\n        &gt;&gt;&gt; nll_value_and_grad_func = compiled_likelihood.negative_log_likelihood_value_and_grad(argnums=0)\n        &gt;&gt;&gt; nll_value, nll_grad = nll_value_and_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n        '''\n        if (\"negative_log_likelihood_value_and_grad\", argnums) not in self._functions:\n            f = partial(\n                jit(value_and_grad(self._negative_log_likelihood_function, argnums=argnums)),\n                log_likelihood_data=self._log_likelihood_data,\n            )\n            if precompiled:\n                f(jnp.zeros(len(self.par_list)), self.global_likelihood_instance._reference_scale)\n            self._functions[(\"negative_log_likelihood_value_and_grad\", argnums)] = f\n        return self._functions[(\"negative_log_likelihood_value_and_grad\", argnums)]\n\n    def negative_log_likelihood_hessian(\n        self,\n        argnums: Union[int, Tuple[int, ...]] = 0,\n        precompiled: bool = True,\n    ) -&gt; Callable:\n        '''\n        Get the jitted function for the Hessian of the negative log-likelihood.\n\n        Parameters\n        ----------\n        argnums : int or Tuple[int, ...], optional\n            The argument numbers with respect to which the Hessian is computed. Default is `0` (the parameters). Use `(0, 1)` to include the scale as well.\n        precompiled : bool, optional\n            If `True`, precompile the function. Default is `True`.\n\n        Returns\n        -------\n        Callable\n            The jitted function for the Hessian of the negative log-likelihood.\n\n        Examples\n        --------\n        Get the jitted function for the Hessian of the negative log-likelihood:\n\n        &gt;&gt;&gt; nll_hess_func = compiled_likelihood.negative_log_likelihood_hessian(argnums=0)\n        &gt;&gt;&gt; nll_hess = nll_hess_func(jnp.array([0.1, 0.2]), 1000.0)\n        '''\n        if (\"negative_log_likelihood_hessian\", argnums) not in self._functions:\n            f = partial(\n                jit(hessian(self._negative_log_likelihood_function, argnums=argnums)),\n                log_likelihood_data=self._log_likelihood_data,\n            )\n            if precompiled:\n                f(jnp.zeros(len(self.par_list)), self.global_likelihood_instance._reference_scale)\n            self._functions[(\"negative_log_likelihood_hessian\", argnums)] = f\n        return self._functions[(\"negative_log_likelihood_hessian\", argnums)]\n\n    def observed_fisher_information(\n        self,\n        argnums: Union[int, Tuple[int, ...]] = 0,\n        precompiled: bool = True,\n    ) -&gt; Callable:\n        '''\n        Get the jitted function for the observed Fisher information (same as Hessian).\n\n        Parameters\n        ----------\n        argnums : int or Tuple[int, ...], optional\n            The argument numbers with respect to which the Fisher information is computed. Default is `0` (the parameters). Use `(0, 1)` to include the scale as well.\n        precompiled : bool, optional\n            If `True`, precompile the function. Default is `True`.\n\n        Returns\n        -------\n        Callable\n            The jitted function for the observed Fisher information.\n\n        Examples\n        --------\n        Get the jitted function for the observed Fisher information (same as Hessian):\n\n        &gt;&gt;&gt; fisher_info_func = compiled_likelihood.observed_fisher_information(argnums=0)\n        &gt;&gt;&gt; fisher_info = fisher_info_func(jnp.array([0.1, 0.2]), 1000.0)\n        '''\n        return self.negative_log_likelihood_hessian(argnums=argnums, precompiled=precompiled)\n\n    def negative_log_likelihood_inverse_hessian(\n        self,\n        argnums: Union[int, Tuple[int, ...]] = 0,\n        precompiled: bool = True,\n    ) -&gt; Callable:\n        '''\n        Get the jitted function for the inverse of the Hessian of the negative log-likelihood.\n\n        Parameters\n        ----------\n        argnums : int or Tuple[int, ...], optional\n            The argument numbers with respect to which the inverse Hessian is computed. Default is `0` (the parameters). Use `(0, 1)` to include the scale as well.\n        precompiled : bool, optional\n            If `True`, precompile the function. Default is `True`.\n\n        Returns\n        -------\n        Callable\n            The jitted function for the inverse of the Hessian of the negative log-likelihood.\n\n        Examples\n        --------\n        Get the jitted function for the inverse of the Hessian of the negative log-likelihood:\n\n        &gt;&gt;&gt; nll_inv_hess_func = compiled_likelihood.negative_log_likelihood_inverse_hessian(argnums=0)\n        &gt;&gt;&gt; nll_inv_hess = nll_inv_hess_func(jnp.array([0.1, 0.2]), 1000.0)\n        '''\n        if (\"negative_log_likelihood_inverse_hessian\", argnums) not in self._functions:\n            def f(par_array, scale, log_likelihood_data):\n                hess = hessian(self._negative_log_likelihood_function, argnums=argnums)(par_array, scale, log_likelihood_data)\n                # regularize the inverse\n                d = jnp.sqrt(jnp.diag(hess))\n                d2 = jnp.outer(d, d)\n                R = hess / d2\n                inv_R = jnp.linalg.inv(R)\n                inv_hess = inv_R / d2\n                return inv_hess\n            f = partial(\n                jit(f),\n                log_likelihood_data=self._log_likelihood_data,\n            )\n            if precompiled:\n                f(jnp.zeros(len(self.par_list)), self.global_likelihood_instance._reference_scale)\n            self._functions[(\"negative_log_likelihood_inverse_hessian\", argnums)] = f\n        return self._functions[(\"negative_log_likelihood_inverse_hessian\", argnums)]\n\n    def asymptotic_covariance(\n        self,\n        argnums: Union[int, Tuple[int, ...]] = 0,\n        precompiled: bool = True,\n    ) -&gt; Callable:\n        '''\n        Get the jitted function for the asymptotic covariance (same as inverse Hessian).\n\n        Parameters\n        ----------\n        argnums : int or Tuple[int, ...], optional\n            The argument numbers with respect to which the asymptotic covariance is computed. Default is `0` (the parameters). Use `(0, 1)` to include the scale as well.\n        precompiled : bool, optional\n            If `True`, precompile the function. Default is `True`.\n\n        Returns\n        -------\n        Callable\n            The jitted function for the asymptotic covariance.\n\n        Examples\n        --------\n        Get the jitted function for the asymptotic covariance (same as inverse Hessian):\n\n        &gt;&gt;&gt; asymp_cov_func = compiled_likelihood.asymptotic_covariance(argnums=0)\n        &gt;&gt;&gt; asymp_cov = asymp_cov_func(jnp.array([0.1, 0.2]), 1000.0)\n        '''\n        return self.negative_log_likelihood_inverse_hessian(argnums=argnums, precompiled=precompiled)\n</code></pre>"},{"location":"jelli/core/compiled_likelihood/#jelli.core.compiled_likelihood.CompiledLikelihood.__init__","title":"<code>__init__(global_likelihood_instance, par_list, likelihood='global', par_dep_cov=False)</code>","text":"<p>Initialize the <code>CompiledLikelihood</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>global_likelihood_instance</code> <code>`jelli.core.global_likelihood.GlobalLikelihood`</code> <p>An instance of the <code>GlobalLikelihood</code> class.</p> required <code>par_list</code> <code>List[Tuple[str, str]]</code> <p>A list of tuples specifying the parameters to be considered. Each tuple contains the parameter name and its type (e.g., <code>('param1', 'R')</code> for a real parameter, <code>('param2', 'I')</code> for an imaginary parameter).</p> required <code>likelihood</code> <code>str or Tuple[str, ...]</code> <p>The likelihood to be used. Default is 'global'.</p> <code>'global'</code> <code>par_dep_cov</code> <code>bool</code> <p>If <code>True</code>, the covariance matrix depends on the parameters. Default is <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Initialize the <code>CompiledLikelihood</code> class with a <code>GlobalLikelihood</code> instance and a parameter list:</p> <pre><code>&gt;&gt;&gt; gl = GlobalLikelihood(...)\n&gt;&gt;&gt; par_list = [('param1', 'R'), ('param2', 'I')]\n&gt;&gt;&gt; likelihood = 'global'\n&gt;&gt;&gt; par_dep_cov = False\n&gt;&gt;&gt; compiled_likelihood = CompiledLikelihood(gl, par_list, likelihood, par_dep_cov)\n</code></pre> Source code in <code>jelli/core/compiled_likelihood.py</code> <pre><code>def __init__(\n    self,\n    global_likelihood_instance,\n    par_list: List[Tuple[str, str]],\n    likelihood: Union[str, Tuple[str, ...]] = 'global',\n    par_dep_cov: bool = False,\n):\n    '''\n    Initialize the `CompiledLikelihood` class.\n\n    Parameters\n    ----------\n    global_likelihood_instance : `jelli.core.global_likelihood.GlobalLikelihood`\n        An instance of the `GlobalLikelihood` class.\n    par_list : List[Tuple[str, str]]\n        A list of tuples specifying the parameters to be considered. Each tuple contains the parameter name and its type (e.g., `('param1', 'R')` for a real parameter, `('param2', 'I')` for an imaginary parameter).\n    likelihood : str or Tuple[str, ...], optional\n        The likelihood to be used. Default is 'global'.\n    par_dep_cov : bool, optional\n        If `True`, the covariance matrix depends on the parameters. Default is `False`.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n\n    Initialize the `CompiledLikelihood` class with a `GlobalLikelihood` instance and a parameter list:\n\n    &gt;&gt;&gt; gl = GlobalLikelihood(...)\n    &gt;&gt;&gt; par_list = [('param1', 'R'), ('param2', 'I')]\n    &gt;&gt;&gt; likelihood = 'global'\n    &gt;&gt;&gt; par_dep_cov = False\n    &gt;&gt;&gt; compiled_likelihood = CompiledLikelihood(gl, par_list, likelihood, par_dep_cov)\n    '''\n    self.global_likelihood_instance = global_likelihood_instance\n    self.par_list = par_list\n    self.likelihood = likelihood\n    self.par_dep_cov = par_dep_cov\n    self._negative_log_likelihood_function, self._log_likelihood_data = self.global_likelihood_instance.get_negative_log_likelihood(par_list, likelihood, par_dep_cov)\n    self._functions = {}\n</code></pre>"},{"location":"jelli/core/compiled_likelihood/#jelli.core.compiled_likelihood.CompiledLikelihood.asymptotic_covariance","title":"<code>asymptotic_covariance(argnums=0, precompiled=True)</code>","text":"<p>Get the jitted function for the asymptotic covariance (same as inverse Hessian).</p> <p>Parameters:</p> Name Type Description Default <code>argnums</code> <code>int or Tuple[int, ...]</code> <p>The argument numbers with respect to which the asymptotic covariance is computed. Default is <code>0</code> (the parameters). Use <code>(0, 1)</code> to include the scale as well.</p> <code>0</code> <code>precompiled</code> <code>bool</code> <p>If <code>True</code>, precompile the function. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The jitted function for the asymptotic covariance.</p> <p>Examples:</p> <p>Get the jitted function for the asymptotic covariance (same as inverse Hessian):</p> <pre><code>&gt;&gt;&gt; asymp_cov_func = compiled_likelihood.asymptotic_covariance(argnums=0)\n&gt;&gt;&gt; asymp_cov = asymp_cov_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> Source code in <code>jelli/core/compiled_likelihood.py</code> <pre><code>def asymptotic_covariance(\n    self,\n    argnums: Union[int, Tuple[int, ...]] = 0,\n    precompiled: bool = True,\n) -&gt; Callable:\n    '''\n    Get the jitted function for the asymptotic covariance (same as inverse Hessian).\n\n    Parameters\n    ----------\n    argnums : int or Tuple[int, ...], optional\n        The argument numbers with respect to which the asymptotic covariance is computed. Default is `0` (the parameters). Use `(0, 1)` to include the scale as well.\n    precompiled : bool, optional\n        If `True`, precompile the function. Default is `True`.\n\n    Returns\n    -------\n    Callable\n        The jitted function for the asymptotic covariance.\n\n    Examples\n    --------\n    Get the jitted function for the asymptotic covariance (same as inverse Hessian):\n\n    &gt;&gt;&gt; asymp_cov_func = compiled_likelihood.asymptotic_covariance(argnums=0)\n    &gt;&gt;&gt; asymp_cov = asymp_cov_func(jnp.array([0.1, 0.2]), 1000.0)\n    '''\n    return self.negative_log_likelihood_inverse_hessian(argnums=argnums, precompiled=precompiled)\n</code></pre>"},{"location":"jelli/core/compiled_likelihood/#jelli.core.compiled_likelihood.CompiledLikelihood.negative_log_likelihood_grad","title":"<code>negative_log_likelihood_grad(argnums=0, precompiled=True)</code>","text":"<p>Get the jitted function for the gradient of the negative log-likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>argnums</code> <code>int or Tuple[int, ...]</code> <p>The argument numbers with respect to which the gradient is computed. Default is <code>0</code> (the parameters). Use <code>(0, 1)</code> to include the scale as well.</p> <code>0</code> <code>precompiled</code> <code>bool</code> <p>If <code>True</code>, precompile the function. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The jitted function for the gradient of the negative log-likelihood.</p> <p>Examples:</p> <p>Get the jitted function for the gradient of the negative log-likelihood with respect to the parameters:</p> <pre><code>&gt;&gt;&gt; nll_grad_func = compiled_likelihood.negative_log_likelihood_grad(argnums=0)\n&gt;&gt;&gt; nll_grad = nll_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> <p>Get the jitted function for the gradient of the negative log-likelihood with respect to both the parameters and the scale:</p> <pre><code>&gt;&gt;&gt; nll_grad_func = compiled_likelihood.negative_log_likelihood_grad(argnums=(0, 1))\n&gt;&gt;&gt; nll_grad = nll_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> Source code in <code>jelli/core/compiled_likelihood.py</code> <pre><code>def negative_log_likelihood_grad(\n    self,\n    argnums: Union[int, Tuple[int, ...]] = 0,\n    precompiled: bool = True,\n) -&gt; Callable:\n    '''\n    Get the jitted function for the gradient of the negative log-likelihood.\n\n    Parameters\n    ----------\n    argnums : int or Tuple[int, ...], optional\n        The argument numbers with respect to which the gradient is computed. Default is `0` (the parameters). Use `(0, 1)` to include the scale as well.\n    precompiled : bool, optional\n        If `True`, precompile the function. Default is `True`.\n\n    Returns\n    -------\n    Callable\n        The jitted function for the gradient of the negative log-likelihood.\n\n    Examples\n    --------\n    Get the jitted function for the gradient of the negative log-likelihood with respect to the parameters:\n\n    &gt;&gt;&gt; nll_grad_func = compiled_likelihood.negative_log_likelihood_grad(argnums=0)\n    &gt;&gt;&gt; nll_grad = nll_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n\n    Get the jitted function for the gradient of the negative log-likelihood with respect to both the parameters and the scale:\n\n    &gt;&gt;&gt; nll_grad_func = compiled_likelihood.negative_log_likelihood_grad(argnums=(0, 1))\n    &gt;&gt;&gt; nll_grad = nll_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n    '''\n    if (\"negative_log_likelihood_grad\", argnums) not in self._functions:\n        f = partial(\n            jit(grad(self._negative_log_likelihood_function, argnums=argnums)),\n            log_likelihood_data=self._log_likelihood_data,\n        )\n        if precompiled:\n            f(jnp.zeros(len(self.par_list)), self.global_likelihood_instance._reference_scale)\n        self._functions[(\"negative_log_likelihood_grad\", argnums)] = f\n    return self._functions[(\"negative_log_likelihood_grad\", argnums)]\n</code></pre>"},{"location":"jelli/core/compiled_likelihood/#jelli.core.compiled_likelihood.CompiledLikelihood.negative_log_likelihood_hessian","title":"<code>negative_log_likelihood_hessian(argnums=0, precompiled=True)</code>","text":"<p>Get the jitted function for the Hessian of the negative log-likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>argnums</code> <code>int or Tuple[int, ...]</code> <p>The argument numbers with respect to which the Hessian is computed. Default is <code>0</code> (the parameters). Use <code>(0, 1)</code> to include the scale as well.</p> <code>0</code> <code>precompiled</code> <code>bool</code> <p>If <code>True</code>, precompile the function. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The jitted function for the Hessian of the negative log-likelihood.</p> <p>Examples:</p> <p>Get the jitted function for the Hessian of the negative log-likelihood:</p> <pre><code>&gt;&gt;&gt; nll_hess_func = compiled_likelihood.negative_log_likelihood_hessian(argnums=0)\n&gt;&gt;&gt; nll_hess = nll_hess_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> Source code in <code>jelli/core/compiled_likelihood.py</code> <pre><code>def negative_log_likelihood_hessian(\n    self,\n    argnums: Union[int, Tuple[int, ...]] = 0,\n    precompiled: bool = True,\n) -&gt; Callable:\n    '''\n    Get the jitted function for the Hessian of the negative log-likelihood.\n\n    Parameters\n    ----------\n    argnums : int or Tuple[int, ...], optional\n        The argument numbers with respect to which the Hessian is computed. Default is `0` (the parameters). Use `(0, 1)` to include the scale as well.\n    precompiled : bool, optional\n        If `True`, precompile the function. Default is `True`.\n\n    Returns\n    -------\n    Callable\n        The jitted function for the Hessian of the negative log-likelihood.\n\n    Examples\n    --------\n    Get the jitted function for the Hessian of the negative log-likelihood:\n\n    &gt;&gt;&gt; nll_hess_func = compiled_likelihood.negative_log_likelihood_hessian(argnums=0)\n    &gt;&gt;&gt; nll_hess = nll_hess_func(jnp.array([0.1, 0.2]), 1000.0)\n    '''\n    if (\"negative_log_likelihood_hessian\", argnums) not in self._functions:\n        f = partial(\n            jit(hessian(self._negative_log_likelihood_function, argnums=argnums)),\n            log_likelihood_data=self._log_likelihood_data,\n        )\n        if precompiled:\n            f(jnp.zeros(len(self.par_list)), self.global_likelihood_instance._reference_scale)\n        self._functions[(\"negative_log_likelihood_hessian\", argnums)] = f\n    return self._functions[(\"negative_log_likelihood_hessian\", argnums)]\n</code></pre>"},{"location":"jelli/core/compiled_likelihood/#jelli.core.compiled_likelihood.CompiledLikelihood.negative_log_likelihood_inverse_hessian","title":"<code>negative_log_likelihood_inverse_hessian(argnums=0, precompiled=True)</code>","text":"<p>Get the jitted function for the inverse of the Hessian of the negative log-likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>argnums</code> <code>int or Tuple[int, ...]</code> <p>The argument numbers with respect to which the inverse Hessian is computed. Default is <code>0</code> (the parameters). Use <code>(0, 1)</code> to include the scale as well.</p> <code>0</code> <code>precompiled</code> <code>bool</code> <p>If <code>True</code>, precompile the function. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The jitted function for the inverse of the Hessian of the negative log-likelihood.</p> <p>Examples:</p> <p>Get the jitted function for the inverse of the Hessian of the negative log-likelihood:</p> <pre><code>&gt;&gt;&gt; nll_inv_hess_func = compiled_likelihood.negative_log_likelihood_inverse_hessian(argnums=0)\n&gt;&gt;&gt; nll_inv_hess = nll_inv_hess_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> Source code in <code>jelli/core/compiled_likelihood.py</code> <pre><code>def negative_log_likelihood_inverse_hessian(\n    self,\n    argnums: Union[int, Tuple[int, ...]] = 0,\n    precompiled: bool = True,\n) -&gt; Callable:\n    '''\n    Get the jitted function for the inverse of the Hessian of the negative log-likelihood.\n\n    Parameters\n    ----------\n    argnums : int or Tuple[int, ...], optional\n        The argument numbers with respect to which the inverse Hessian is computed. Default is `0` (the parameters). Use `(0, 1)` to include the scale as well.\n    precompiled : bool, optional\n        If `True`, precompile the function. Default is `True`.\n\n    Returns\n    -------\n    Callable\n        The jitted function for the inverse of the Hessian of the negative log-likelihood.\n\n    Examples\n    --------\n    Get the jitted function for the inverse of the Hessian of the negative log-likelihood:\n\n    &gt;&gt;&gt; nll_inv_hess_func = compiled_likelihood.negative_log_likelihood_inverse_hessian(argnums=0)\n    &gt;&gt;&gt; nll_inv_hess = nll_inv_hess_func(jnp.array([0.1, 0.2]), 1000.0)\n    '''\n    if (\"negative_log_likelihood_inverse_hessian\", argnums) not in self._functions:\n        def f(par_array, scale, log_likelihood_data):\n            hess = hessian(self._negative_log_likelihood_function, argnums=argnums)(par_array, scale, log_likelihood_data)\n            # regularize the inverse\n            d = jnp.sqrt(jnp.diag(hess))\n            d2 = jnp.outer(d, d)\n            R = hess / d2\n            inv_R = jnp.linalg.inv(R)\n            inv_hess = inv_R / d2\n            return inv_hess\n        f = partial(\n            jit(f),\n            log_likelihood_data=self._log_likelihood_data,\n        )\n        if precompiled:\n            f(jnp.zeros(len(self.par_list)), self.global_likelihood_instance._reference_scale)\n        self._functions[(\"negative_log_likelihood_inverse_hessian\", argnums)] = f\n    return self._functions[(\"negative_log_likelihood_inverse_hessian\", argnums)]\n</code></pre>"},{"location":"jelli/core/compiled_likelihood/#jelli.core.compiled_likelihood.CompiledLikelihood.negative_log_likelihood_value","title":"<code>negative_log_likelihood_value(precompiled=True)</code>","text":"<p>Get the jitted function for the negative log-likelihood value.</p> <p>Parameters:</p> Name Type Description Default <code>precompiled</code> <code>bool</code> <p>If <code>True</code>, precompile the function. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The jitted function for the negative log-likelihood value.</p> <p>Examples:</p> <p>Get the jitted function for the negative log-likelihood value:</p> <pre><code>&gt;&gt;&gt; nll_value_func = compiled_likelihood.negative_log_likelihood_value()\n&gt;&gt;&gt; nll_value = nll_value_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> Source code in <code>jelli/core/compiled_likelihood.py</code> <pre><code>def negative_log_likelihood_value(\n    self,\n    precompiled: bool = True,\n) -&gt; Callable:\n    '''\n    Get the jitted function for the negative log-likelihood value.\n\n    Parameters\n    ----------\n    precompiled : bool, optional\n        If `True`, precompile the function. Default is `True`.\n\n    Returns\n    -------\n    Callable\n        The jitted function for the negative log-likelihood value.\n\n    Examples\n    --------\n    Get the jitted function for the negative log-likelihood value:\n\n    &gt;&gt;&gt; nll_value_func = compiled_likelihood.negative_log_likelihood_value()\n    &gt;&gt;&gt; nll_value = nll_value_func(jnp.array([0.1, 0.2]), 1000.0)\n    '''\n    if \"negative_log_likelihood_value\" not in self._functions:\n        f = partial(\n            jit(self._negative_log_likelihood_function),\n            log_likelihood_data=self._log_likelihood_data,\n        )\n        if precompiled:\n            f(jnp.zeros(len(self.par_list)), self.global_likelihood_instance._reference_scale)\n        self._functions[\"negative_log_likelihood_value\"] = f\n    return self._functions[\"negative_log_likelihood_value\"]\n</code></pre>"},{"location":"jelli/core/compiled_likelihood/#jelli.core.compiled_likelihood.CompiledLikelihood.negative_log_likelihood_value_and_grad","title":"<code>negative_log_likelihood_value_and_grad(argnums=0, precompiled=True)</code>","text":"<p>Get the jitted function for both the value and gradient of the negative log-likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>argnums</code> <code>int or Tuple[int, ...]</code> <p>The argument numbers with respect to which the gradient is computed. Default is <code>0</code> (the parameters). Use <code>(0, 1)</code> to include the scale as well.</p> <code>0</code> <code>precompiled</code> <code>bool</code> <p>If <code>True</code>, precompile the function. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The jitted function for both the value and gradient of the negative log-likelihood.</p> <p>Examples:</p> <p>Get the jitted function for both the value and gradient of the negative log-likelihood:</p> <pre><code>&gt;&gt;&gt; nll_value_and_grad_func = compiled_likelihood.negative_log_likelihood_value_and_grad(argnums=0)\n&gt;&gt;&gt; nll_value, nll_grad = nll_value_and_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> Source code in <code>jelli/core/compiled_likelihood.py</code> <pre><code>def negative_log_likelihood_value_and_grad(\n    self,\n    argnums: Union[int, Tuple[int, ...]] = 0,\n    precompiled: bool = True,\n) -&gt; Callable:\n    '''\n    Get the jitted function for both the value and gradient of the negative log-likelihood.\n\n    Parameters\n    ----------\n    argnums : int or Tuple[int, ...], optional\n        The argument numbers with respect to which the gradient is computed. Default is `0` (the parameters). Use `(0, 1)` to include the scale as well.\n    precompiled : bool, optional\n        If `True`, precompile the function. Default is `True`.\n\n    Returns\n    -------\n    Callable\n        The jitted function for both the value and gradient of the negative log-likelihood.\n\n    Examples\n    --------\n    Get the jitted function for both the value and gradient of the negative log-likelihood:\n\n    &gt;&gt;&gt; nll_value_and_grad_func = compiled_likelihood.negative_log_likelihood_value_and_grad(argnums=0)\n    &gt;&gt;&gt; nll_value, nll_grad = nll_value_and_grad_func(jnp.array([0.1, 0.2]), 1000.0)\n    '''\n    if (\"negative_log_likelihood_value_and_grad\", argnums) not in self._functions:\n        f = partial(\n            jit(value_and_grad(self._negative_log_likelihood_function, argnums=argnums)),\n            log_likelihood_data=self._log_likelihood_data,\n        )\n        if precompiled:\n            f(jnp.zeros(len(self.par_list)), self.global_likelihood_instance._reference_scale)\n        self._functions[(\"negative_log_likelihood_value_and_grad\", argnums)] = f\n    return self._functions[(\"negative_log_likelihood_value_and_grad\", argnums)]\n</code></pre>"},{"location":"jelli/core/compiled_likelihood/#jelli.core.compiled_likelihood.CompiledLikelihood.observed_fisher_information","title":"<code>observed_fisher_information(argnums=0, precompiled=True)</code>","text":"<p>Get the jitted function for the observed Fisher information (same as Hessian).</p> <p>Parameters:</p> Name Type Description Default <code>argnums</code> <code>int or Tuple[int, ...]</code> <p>The argument numbers with respect to which the Fisher information is computed. Default is <code>0</code> (the parameters). Use <code>(0, 1)</code> to include the scale as well.</p> <code>0</code> <code>precompiled</code> <code>bool</code> <p>If <code>True</code>, precompile the function. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>Callable</code> <p>The jitted function for the observed Fisher information.</p> <p>Examples:</p> <p>Get the jitted function for the observed Fisher information (same as Hessian):</p> <pre><code>&gt;&gt;&gt; fisher_info_func = compiled_likelihood.observed_fisher_information(argnums=0)\n&gt;&gt;&gt; fisher_info = fisher_info_func(jnp.array([0.1, 0.2]), 1000.0)\n</code></pre> Source code in <code>jelli/core/compiled_likelihood.py</code> <pre><code>def observed_fisher_information(\n    self,\n    argnums: Union[int, Tuple[int, ...]] = 0,\n    precompiled: bool = True,\n) -&gt; Callable:\n    '''\n    Get the jitted function for the observed Fisher information (same as Hessian).\n\n    Parameters\n    ----------\n    argnums : int or Tuple[int, ...], optional\n        The argument numbers with respect to which the Fisher information is computed. Default is `0` (the parameters). Use `(0, 1)` to include the scale as well.\n    precompiled : bool, optional\n        If `True`, precompile the function. Default is `True`.\n\n    Returns\n    -------\n    Callable\n        The jitted function for the observed Fisher information.\n\n    Examples\n    --------\n    Get the jitted function for the observed Fisher information (same as Hessian):\n\n    &gt;&gt;&gt; fisher_info_func = compiled_likelihood.observed_fisher_information(argnums=0)\n    &gt;&gt;&gt; fisher_info = fisher_info_func(jnp.array([0.1, 0.2]), 1000.0)\n    '''\n    return self.negative_log_likelihood_hessian(argnums=argnums, precompiled=precompiled)\n</code></pre>"},{"location":"jelli/core/custom_basis/","title":"jelli.core.custom_basis","text":""},{"location":"jelli/core/custom_basis/#jelli.core.custom_basis.CustomBasis","title":"<code>CustomBasis</code>","text":"<p>A class to represent a custom parameter basis.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the custom basis.</p> required <code>parameters</code> <code>list or dict</code> <p>The parameters in the custom basis. If a list is provided, all parameters are assumed to be real. If a dict is provided, the keys are the parameter names and the values are the types (e.g., <code>R</code> for real, <code>C</code> for complex).</p> required <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the custom basis.</p> <code>parameters</code> <code>dict</code> <p>The parameters in the custom basis. The keys are the parameter names and the values are the types (e.g., <code>R</code> for real, <code>C</code> for complex).</p> <p>Methods:</p> Name Description <code>get_all_names</code> <p>Get all custom basis names.</p> <code>get</code> <p>Get a custom basis by name.</p> <code>get_all</code> <p>Get all custom basis objects.</p> <code>get_parameter_basis</code> <p>Get the parameter basis.</p> <p>Examples:</p> <p>Initialize a custom basis with real parameters:</p> <pre><code>&gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n</code></pre> <p>Initialize a custom basis with mixed parameters:</p> <pre><code>&gt;&gt;&gt; custom_basis = CustomBasis('example_basis', {'param1': 'R', 'param2': 'C'})\n</code></pre> <p>Get all custom basis names:</p> <pre><code>&gt;&gt;&gt; CustomBasis.get_all_names()\n</code></pre> <p>Get a custom basis by name:</p> <pre><code>&gt;&gt;&gt; custom_basis = CustomBasis.get('example_basis')\n</code></pre> <p>Get all custom basis objects:</p> <pre><code>&gt;&gt;&gt; custom_bases = CustomBasis.get_all()\n</code></pre> <p>Get the parameter basis:</p> <pre><code>&gt;&gt;&gt; parameter_basis = custom_basis.get_parameter_basis()\n</code></pre> Source code in <code>jelli/core/custom_basis.py</code> <pre><code>class CustomBasis:\n    '''\n    A class to represent a custom parameter basis.\n\n    Parameters\n    ----------\n    name : str\n        The name of the custom basis.\n    parameters : list or dict\n        The parameters in the custom basis. If a list is provided, all parameters are assumed to be real. If a dict is provided, the keys are the parameter names and the values are the types (e.g., `R` for real, `C` for complex).\n\n    Attributes\n    ----------\n    name : str\n        The name of the custom basis.\n    parameters : dict\n        The parameters in the custom basis. The keys are the parameter names and the values are the types (e.g., `R` for real, `C` for complex).\n\n    Methods\n    -------\n    get_all_names() -&gt; List[str]\n        Get all custom basis names.\n    get(name: str) -&gt; 'CustomBasis'\n        Get a custom basis by name.\n    get_all() -&gt; List['CustomBasis']\n        Get all custom basis objects.\n    get_parameter_basis() -&gt; List\n        Get the parameter basis.\n\n    Examples\n    --------\n    Initialize a custom basis with real parameters:\n\n    &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n\n    Initialize a custom basis with mixed parameters:\n\n    &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', {'param1': 'R', 'param2': 'C'})\n\n    Get all custom basis names:\n\n    &gt;&gt;&gt; CustomBasis.get_all_names()\n\n    Get a custom basis by name:\n\n    &gt;&gt;&gt; custom_basis = CustomBasis.get('example_basis')\n\n    Get all custom basis objects:\n\n    &gt;&gt;&gt; custom_bases = CustomBasis.get_all()\n\n    Get the parameter basis:\n\n    &gt;&gt;&gt; parameter_basis = custom_basis.get_parameter_basis()\n\n    '''\n\n    _custom_bases: Dict[str, 'CustomBasis'] = {}  # Class attribute to store all custom bases\n\n    def __init__(self, name: str, parameters: Union[List[str], Dict[str, str]]):\n        \"\"\"\n        Initialize the CustomBasis class.\n\n        Parameters\n        ----------\n        name : str\n            The name of the custom basis.\n        parameters : list or dict\n            The parameters in the custom basis. If a list is provided, all parameters are assumed to be real. If a dict is provided, the keys are the parameter names and the values are the types (e.g., `R` for real, `C` for complex).\n\n        Returns\n        --------\n        None\n\n        Examples\n        --------\n        Initialize a custom basis with real parameters:\n\n        &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n\n        Initialize a custom basis with mixed parameters:\n\n        &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', {'param1': 'R', 'param2': 'C'})\n        \"\"\"\n        self.name = name\n        if isinstance(parameters, list):\n            self.parameters = {param: 'R' for param in parameters}\n        elif isinstance(parameters, dict):\n            if not all(value in ['R', 'C'] for value in parameters.values()):\n                raise ValueError(\"All parameter types must be either 'R' or 'C'.\")\n            self.parameters = parameters\n        else:\n            raise ValueError(\"Parameters must be a list or a dictionary.\")\n        self._custom_bases[self.name] = self\n\n    @classmethod\n    def get_all_names(cls) -&gt; List[str]:\n        \"\"\"\n        Get all custom basis names.\n\n        Returns\n        --------\n        list\n            A list of all custom basis names.\n\n        Examples\n        --------\n            &gt;&gt;&gt; CustomBasis.get_all_names()\n        \"\"\"\n        return sorted(cls._custom_bases.keys())\n\n    @classmethod\n    def get(cls, name: str) -&gt; 'CustomBasis':\n        \"\"\"\n        Get a custom basis by name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the custom basis.\n        Returns\n        --------\n        CustomBasis\n            The custom basis object.\n\n        Examples\n        --------\n        &gt;&gt;&gt; CustomBasis.get('example_basis')\n        \"\"\"\n        return cls._custom_bases.get(name)\n\n    @classmethod\n    def get_all(cls) -&gt; List['CustomBasis']:\n        \"\"\"\n        Get all custom basis objects.\n\n        Returns\n        --------\n        list\n            A list of all custom basis objects.\n\n        Examples\n        --------\n            &gt;&gt;&gt; CustomBasis.get_all()\n        \"\"\"\n        return list(cls._custom_bases.values())\n\n    def get_parameter_basis(self, split_re_im=True) -&gt; List:\n        \"\"\"\n        Get the parameter basis.\n\n        Parameters\n        ----------\n        split_re_im : bool, optional\n            If `True`, split parameters into real and imaginary parts, otherwise return the parameters directly. Default is `True`.\n\n        Returns\n        --------\n        list\n            A list containing the parameter basis.\n\n        Examples\n        --------\n            &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n            &gt;&gt;&gt; custom_basis.get_parameter_basis('example_basis')\n        \"\"\"\n        if split_re_im:\n            parameter_basis = []\n            for parameter, parameter_type in self.parameters.items():\n                parameter_basis.append((parameter, 'R'))\n                if parameter_type == 'C':\n                    parameter_basis.append((parameter, 'I'))\n        else:\n            parameter_basis = self.parameters.keys()\n        return sorted(parameter_basis)\n</code></pre>"},{"location":"jelli/core/custom_basis/#jelli.core.custom_basis.CustomBasis.__init__","title":"<code>__init__(name, parameters)</code>","text":"<p>Initialize the CustomBasis class.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the custom basis.</p> required <code>parameters</code> <code>list or dict</code> <p>The parameters in the custom basis. If a list is provided, all parameters are assumed to be real. If a dict is provided, the keys are the parameter names and the values are the types (e.g., <code>R</code> for real, <code>C</code> for complex).</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Initialize a custom basis with real parameters:</p> <pre><code>&gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n</code></pre> <p>Initialize a custom basis with mixed parameters:</p> <pre><code>&gt;&gt;&gt; custom_basis = CustomBasis('example_basis', {'param1': 'R', 'param2': 'C'})\n</code></pre> Source code in <code>jelli/core/custom_basis.py</code> <pre><code>def __init__(self, name: str, parameters: Union[List[str], Dict[str, str]]):\n    \"\"\"\n    Initialize the CustomBasis class.\n\n    Parameters\n    ----------\n    name : str\n        The name of the custom basis.\n    parameters : list or dict\n        The parameters in the custom basis. If a list is provided, all parameters are assumed to be real. If a dict is provided, the keys are the parameter names and the values are the types (e.g., `R` for real, `C` for complex).\n\n    Returns\n    --------\n    None\n\n    Examples\n    --------\n    Initialize a custom basis with real parameters:\n\n    &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n\n    Initialize a custom basis with mixed parameters:\n\n    &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', {'param1': 'R', 'param2': 'C'})\n    \"\"\"\n    self.name = name\n    if isinstance(parameters, list):\n        self.parameters = {param: 'R' for param in parameters}\n    elif isinstance(parameters, dict):\n        if not all(value in ['R', 'C'] for value in parameters.values()):\n            raise ValueError(\"All parameter types must be either 'R' or 'C'.\")\n        self.parameters = parameters\n    else:\n        raise ValueError(\"Parameters must be a list or a dictionary.\")\n    self._custom_bases[self.name] = self\n</code></pre>"},{"location":"jelli/core/custom_basis/#jelli.core.custom_basis.CustomBasis.get","title":"<code>get(name)</code>  <code>classmethod</code>","text":"<p>Get a custom basis by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the custom basis.</p> required <p>Returns:</p> Type Description <code>CustomBasis</code> <p>The custom basis object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; CustomBasis.get('example_basis')\n</code></pre> Source code in <code>jelli/core/custom_basis.py</code> <pre><code>@classmethod\ndef get(cls, name: str) -&gt; 'CustomBasis':\n    \"\"\"\n    Get a custom basis by name.\n\n    Parameters\n    ----------\n    name : str\n        The name of the custom basis.\n    Returns\n    --------\n    CustomBasis\n        The custom basis object.\n\n    Examples\n    --------\n    &gt;&gt;&gt; CustomBasis.get('example_basis')\n    \"\"\"\n    return cls._custom_bases.get(name)\n</code></pre>"},{"location":"jelli/core/custom_basis/#jelli.core.custom_basis.CustomBasis.get_all","title":"<code>get_all()</code>  <code>classmethod</code>","text":"<p>Get all custom basis objects.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of all custom basis objects.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; CustomBasis.get_all()\n</code></pre> Source code in <code>jelli/core/custom_basis.py</code> <pre><code>@classmethod\ndef get_all(cls) -&gt; List['CustomBasis']:\n    \"\"\"\n    Get all custom basis objects.\n\n    Returns\n    --------\n    list\n        A list of all custom basis objects.\n\n    Examples\n    --------\n        &gt;&gt;&gt; CustomBasis.get_all()\n    \"\"\"\n    return list(cls._custom_bases.values())\n</code></pre>"},{"location":"jelli/core/custom_basis/#jelli.core.custom_basis.CustomBasis.get_all_names","title":"<code>get_all_names()</code>  <code>classmethod</code>","text":"<p>Get all custom basis names.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of all custom basis names.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; CustomBasis.get_all_names()\n</code></pre> Source code in <code>jelli/core/custom_basis.py</code> <pre><code>@classmethod\ndef get_all_names(cls) -&gt; List[str]:\n    \"\"\"\n    Get all custom basis names.\n\n    Returns\n    --------\n    list\n        A list of all custom basis names.\n\n    Examples\n    --------\n        &gt;&gt;&gt; CustomBasis.get_all_names()\n    \"\"\"\n    return sorted(cls._custom_bases.keys())\n</code></pre>"},{"location":"jelli/core/custom_basis/#jelli.core.custom_basis.CustomBasis.get_parameter_basis","title":"<code>get_parameter_basis(split_re_im=True)</code>","text":"<p>Get the parameter basis.</p> <p>Parameters:</p> Name Type Description Default <code>split_re_im</code> <code>bool</code> <p>If <code>True</code>, split parameters into real and imaginary parts, otherwise return the parameters directly. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>list</code> <p>A list containing the parameter basis.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n&gt;&gt;&gt; custom_basis.get_parameter_basis('example_basis')\n</code></pre> Source code in <code>jelli/core/custom_basis.py</code> <pre><code>def get_parameter_basis(self, split_re_im=True) -&gt; List:\n    \"\"\"\n    Get the parameter basis.\n\n    Parameters\n    ----------\n    split_re_im : bool, optional\n        If `True`, split parameters into real and imaginary parts, otherwise return the parameters directly. Default is `True`.\n\n    Returns\n    --------\n    list\n        A list containing the parameter basis.\n\n    Examples\n    --------\n        &gt;&gt;&gt; custom_basis = CustomBasis('example_basis', ['param1', 'param2'])\n        &gt;&gt;&gt; custom_basis.get_parameter_basis('example_basis')\n    \"\"\"\n    if split_re_im:\n        parameter_basis = []\n        for parameter, parameter_type in self.parameters.items():\n            parameter_basis.append((parameter, 'R'))\n            if parameter_type == 'C':\n                parameter_basis.append((parameter, 'I'))\n    else:\n        parameter_basis = self.parameters.keys()\n    return sorted(parameter_basis)\n</code></pre>"},{"location":"jelli/core/experimental_correlations/","title":"jelli.core.experimental_correlations","text":""},{"location":"jelli/core/experimental_correlations/#jelli.core.experimental_correlations.ExperimentalCorrelations","title":"<code>ExperimentalCorrelations</code>","text":"<p>A class to represent experimental correlations.</p> <p>Parameters:</p> Name Type Description Default <code>hash_val</code> <code>str</code> <p>A unique hash value representing the combination of measurements and observables.</p> required <code>data_type</code> <code>str</code> <p>The type of data stored in the instance. It can be <code>correlations</code>, <code>central</code>, or <code>uncertainties</code>.</p> required <code>data</code> <code>ndarray</code> <p>The data array containing the correlation matrix, central values, or uncertainties.</p> required <code>row_names</code> <code>Iterable[str]</code> <p>The names of the observables corresponding to the rows of the data array.</p> required <code>col_names</code> <code>Iterable[str]</code> <p>The names of the observables corresponding to the columns of the data array.</p> required <p>Attributes:</p> Name Type Description <code>hash_val</code> <code>str</code> <p>A unique hash value representing the combination of measurements and observables.</p> <code>data_type</code> <code>str</code> <p>The type of data stored in the instance. It can be <code>correlations</code>, <code>central</code>, or <code>uncertainties</code>.</p> <code>data</code> <code>ndarray</code> <p>The data array containing the correlation matrix, central values, or uncertainties.</p> <code>row_names</code> <code>Iterable[str]</code> <p>The names of the observables corresponding to the rows of the data array.</p> <code>col_names</code> <code>Iterable[str]</code> <p>The names of the observables corresponding to the columns of the data array.</p> <code>_instances</code> <code>Dict[str, Dict[str, ExperimentalCorrelations]]</code> <p>A class-level dictionary to hold instances of <code>ExperimentalCorrelations</code> for each data type.</p> <code>_covariance_scaled</code> <code>Dict[str, ndarray]</code> <p>A class-level dictionary to hold scaled covariance matrices for each unique combination of measurements and observables.</p> <code>_observable_names</code> <code>Iterable[Iterable[str]]</code> <p>A class-level iterable to hold lists of observable names for each observable sector.</p> <p>Methods:</p> Name Description <code>load</code> <p>Load observable names from all correlated observable sectors and initialize class-level attributes.</p> <code>compute</code> <p>Compute the correlation matrices, central values, and uncertainties from the specified measurements.</p> <code>get_data</code> <p>Retrieve the data array for the specified data type and combination of observables and measurements.</p> <code>get_cov_scaled</code> <p>Retrieve the scaled covariance matrix for the specified combination of observables and measurements.</p> <p>Examples:</p> <p>Load observable names from all correlated observable sectors:</p> <pre><code>&gt;&gt;&gt; ExperimentalCorrelations.load()\n</code></pre> <p>Compute the correlation matrices, central values, and uncertainties from the specified measurements:</p> <pre><code>&gt;&gt;&gt; ExperimentalCorrelations.compute(include_measurements=['measurement1', 'measurement2'])\n</code></pre> <p>Retrieve the data array for a specific data type and combination of observables and measurements:</p> <pre><code>&gt;&gt;&gt; data = ExperimentalCorrelations.get_data(data_type='correlations', include_measurements=['measurement1'], row_names=['observable1'], col_names=['observable2'])\n</code></pre> <p>Retrieve the scaled covariance matrix for a specific combination of observables and measurements:</p> <pre><code>&gt;&gt;&gt; cov_scaled = ExperimentalCorrelations.get_cov_scaled(include_measurements=['measurement1'], row_names=['observable1'], col_names=['observable2'], std_exp_scaled_row=np.array([0.5]), std_exp_scaled_col=np.array([1.2]))\n</code></pre> Source code in <code>jelli/core/experimental_correlations.py</code> <pre><code>class ExperimentalCorrelations:\n    '''\n    A class to represent experimental correlations.\n\n    Parameters\n    ----------\n    hash_val : str\n        A unique hash value representing the combination of measurements and observables.\n    data_type : str\n        The type of data stored in the instance. It can be `correlations`, `central`, or `uncertainties`.\n    data : np.ndarray\n        The data array containing the correlation matrix, central values, or uncertainties.\n    row_names : Iterable[str]\n        The names of the observables corresponding to the rows of the data array.\n    col_names : Iterable[str]\n        The names of the observables corresponding to the columns of the data array.\n\n    Attributes\n    ----------\n    hash_val : str\n        A unique hash value representing the combination of measurements and observables.\n    data_type : str\n        The type of data stored in the instance. It can be `correlations`, `central`, or `uncertainties`.\n    data : np.ndarray\n        The data array containing the correlation matrix, central values, or uncertainties.\n    row_names : Iterable[str]\n        The names of the observables corresponding to the rows of the data array.\n    col_names : Iterable[str]\n        The names of the observables corresponding to the columns of the data array.\n    _instances : Dict[str, Dict[str, 'ExperimentalCorrelations']]\n        A class-level dictionary to hold instances of `ExperimentalCorrelations` for each data type.\n    _covariance_scaled : Dict[str, jnp.ndarray]\n        A class-level dictionary to hold scaled covariance matrices for each unique combination of measurements and observables.\n    _observable_names : Iterable[Iterable[str]]\n        A class-level iterable to hold lists of observable names for each observable sector.\n\n    Methods\n    -------\n    load() -&gt; None\n        Load observable names from all correlated observable sectors and initialize class-level attributes.\n    compute(include_measurements: Iterable[str], n_samples: int = int(1e6), seed: int = None) -&gt; None\n        Compute the correlation matrices, central values, and uncertainties from the specified measurements.\n    get_data(data_type: str, include_measurements: Iterable[str], row_names: Iterable[str], col_names: Optional[Iterable[str]] = []) -&gt; Optional[jnp.ndarray]\n        Retrieve the data array for the specified data type and combination of observables and measurements.\n    get_cov_scaled(include_measurements: Iterable[str], row_names: Iterable[str], col_names: Iterable[str], std_exp_scaled_row: np.ndarray, std_exp_scaled_col: np.ndarray) -&gt; jnp.ndarray\n        Retrieve the scaled covariance matrix for the specified combination of observables and measurements.\n\n    Examples\n    --------\n    Load observable names from all correlated observable sectors:\n\n    &gt;&gt;&gt; ExperimentalCorrelations.load()\n\n    Compute the correlation matrices, central values, and uncertainties from the specified measurements:\n\n    &gt;&gt;&gt; ExperimentalCorrelations.compute(include_measurements=['measurement1', 'measurement2'])\n\n    Retrieve the data array for a specific data type and combination of observables and measurements:\n\n    &gt;&gt;&gt; data = ExperimentalCorrelations.get_data(data_type='correlations', include_measurements=['measurement1'], row_names=['observable1'], col_names=['observable2'])\n\n    Retrieve the scaled covariance matrix for a specific combination of observables and measurements:\n\n    &gt;&gt;&gt; cov_scaled = ExperimentalCorrelations.get_cov_scaled(include_measurements=['measurement1'], row_names=['observable1'], col_names=['observable2'], std_exp_scaled_row=np.array([0.5]), std_exp_scaled_col=np.array([1.2]))\n    '''\n\n    _instances: Dict[str, Dict[str, 'ExperimentalCorrelations']] = {\n        'correlations': {},\n        'central': {},\n        'uncertainties': {}\n    }  # Dictionary to hold instances of ExperimentalCorrelations\n    _covariance_scaled: Dict[str, jnp.ndarray] = {}\n    _observable_names: Iterable[Iterable[str]] = []\n\n    def __init__(\n        self,\n        hash_val: str,\n        data_type: str,\n        data: np.ndarray,\n        row_names: Iterable[str],\n        col_names: Iterable[str],\n    ) -&gt; None:\n        '''\n        Initialize an instance of the ExperimentalCorrelations class.\n\n        Parameters\n        ----------\n        hash_val : str\n            A unique hash value representing the combination of measurements and observables.\n        data_type : str\n            The type of data stored in the instance. It can be `correlations`, `central`, or `uncertainties`.\n        data : np.ndarray\n            The data array containing the correlation matrix, central values, or uncertainties.\n        row_names : Iterable[str]\n            The names of the observables corresponding to the rows of the data array.\n        col_names : Iterable[str]\n            The names of the observables corresponding to the columns of the data array.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Initialize an instance of the ExperimentalCorrelations class:\n\n        &gt;&gt;&gt; exp_corr = ExperimentalCorrelations(\n        ...     hash_val='unique_hash_value',\n        ...     data_type='correlations',\n        ...     data=np.array([[1.0, 0.5], [0.5, 1.0]]),\n        ...     row_names=['observable1', 'observable2'],\n        ...     col_names=['observable1', 'observable2']\n        ... )\n        '''\n        self.hash_val = hash_val\n        self.data_type = data_type\n        self.data = data\n        self.row_names = row_names\n        self.col_names = col_names\n        self._instances[data_type][hash_val] = self\n\n    @classmethod\n    def load(cls) -&gt; None:\n        '''\n        Load observable names from all correlated observable sectors and initialize class-level attributes.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Load observable names from all correlated observable sectors:\n\n        &gt;&gt;&gt; ExperimentalCorrelations.load()\n        '''\n        observable_names = []\n        for observable_sector in ObservableSector.get_all():\n            if observable_sector.observable_uncertainties is not None:\n                observable_names.append(observable_sector.observable_names)\n        cls._observable_names = observable_names\n        cls._instances = {\n            'correlations': {},\n            'central': {},\n            'uncertainties': {}\n        }\n        cls._covariance_scaled = {}\n\n    @classmethod\n    def compute(\n        cls,\n        include_measurements: Iterable[str],\n        n_samples: int = int(1e6),\n        seed: int = None\n    ) -&gt; None:\n        '''\n        Compute the correlation matrices, central values, and uncertainties from the specified measurements.\n\n        Parameters\n        ----------\n        include_measurements : Iterable[str]\n            A list of measurement names to include in the computation.\n        n_samples : int, optional\n            The number of samples to draw for numerical approximations of non-Gaussian distributions. Default is 1e6.\n        seed : int, optional\n            A random seed for reproducibility of the samples. Default is None.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Compute the correlation matrices, central values, and uncertainties from the specified measurements:\n\n        &gt;&gt;&gt; ExperimentalCorrelations.compute(include_measurements=['measurement1', 'measurement2'])\n        '''\n\n        observables = list(chain.from_iterable(cls._observable_names))\n\n        # get univariate constraints and combine them for each observable\n        constraints_univariate = Measurement.get_constraints(\n            observables,\n            distribution_types=[k for k in logpdf_functions.keys() if k not in ['MultivariateNormalDistribution']],\n            include_measurements=include_measurements,\n            )\n        constraints_list = []\n        for observable in observables:\n            constraints_observable = {}\n            for dist_type, dist_info in constraints_univariate.items():\n                mask = dist_info['observables'] == observable\n                if np.any(mask):\n                    constraints_observable[dist_type] = {\n                        k: v[mask] for k, v in dist_info.items()\n                    }\n            if constraints_observable:\n                constraints_list.append(constraints_observable)\n        constraints_univariate = Measurement.combine_constraints(constraints_list)\n\n        # construct covariance matrix and mean vector from univariate constraints\n        cov = np.diag([np.inf] * len(observables))\n        mean = np.zeros(len(observables))\n        for dist_type, dist_info in constraints_univariate.items():\n            if dist_type == 'NormalDistribution' or dist_type == 'HalfNormalDistribution':  # replace HalfNormalDistribution with zero-mean NormalDistribution\n                central_value = dist_info['central_value']\n                standard_deviation = dist_info['standard_deviation']\n                observable_indices = dist_info['observable_indices']\n            else:  # numerically obtain the Gaussian approximation\n                samples = get_distribution_samples(dist_type, dist_info, n_samples, seed)\n                central_value = np.mean(samples, axis=1)\n                standard_deviation = np.std(samples, axis=1)\n                observable_indices = dist_info['observable_indices']\n            cov[observable_indices, observable_indices] = standard_deviation**2\n            mean[observable_indices] = central_value\n\n        # get multivariate constraints\n        constraints_multivariate = Measurement.get_constraints(\n            observables,\n            distribution_types=['MultivariateNormalDistribution'],\n            include_measurements=include_measurements,\n        )\n        if constraints_multivariate:\n            # combine all covariance matrices and mean vectors using the weighted average\n            weights = [np.diag(1/np.diag(cov))]\n            means = [mean]\n            constraints_multivariate = constraints_multivariate['MultivariateNormalDistribution']\n            for i in range(len(constraints_multivariate['central_value'])):\n                weight_i = np.zeros((len(observables), len(observables)))\n                mean_i = np.zeros(len(observables))\n                observable_indices_i = constraints_multivariate['observable_indices'][i]\n                central_value_i = constraints_multivariate['central_value'][i]\n                standard_deviation_i = constraints_multivariate['standard_deviation'][i]\n                inverse_correlation_i = constraints_multivariate['inverse_correlation'][i]\n                weight_i[np.ix_(observable_indices_i, observable_indices_i)] = inverse_correlation_i / np.outer(standard_deviation_i, standard_deviation_i)\n                mean_i[observable_indices_i] = central_value_i\n                weights.append(weight_i)\n                means.append(mean_i)\n            inv_cov = np.sum(weights, axis=0)\n            # regularize inversion asuming inv_cov = D R D where R has unit diagonal, then invert R instead of inv_cov\n            d = np.sqrt(np.diag(inv_cov))\n            nonzero = d != 0  # unconstrained observables have zeros\n            inv_cov = inv_cov[np.ix_(nonzero, nonzero)]\n            d = d[nonzero]\n            d2 = np.outer(d, d)\n            R = inv_cov / d2\n            inv_R = np.linalg.inv(R)\n            cov = np.diag([np.nan] * len(nonzero))\n            cov[np.ix_(nonzero, nonzero)] = inv_R / d2\n            mean = cov @ np.sum([w @ m for w, m in zip(weights, means)], axis=0)\n        else:\n            cov[cov == np.inf] = np.nan\n        std = np.sqrt(np.diag(cov))\n        corr = cov / np.outer(std, std)\n\n        for i, row_names in enumerate(cls._observable_names):\n            row_measurements = Measurement.get_measurements(row_names, include_measurements=include_measurements)\n            row_idx = [observables.index(o) for o in row_names]\n            hash_val = hash_names(row_measurements, row_names)\n            cls(\n                hash_val=hash_val,\n                data_type='central',\n                data=jnp.array(mean[row_idx], dtype=jnp.float64),\n                row_names=row_names,\n                col_names=[],\n            )\n            cls(\n                hash_val=hash_val,\n                data_type='uncertainties',\n                data=jnp.array(std[row_idx], dtype=jnp.float64),\n                row_names=row_names,\n                col_names=[],\n            )\n            for j in range(i, len(cls._observable_names)):\n                col_names = cls._observable_names[j]\n                col_measurements = Measurement.get_measurements(col_names, include_measurements=include_measurements)\n                col_idx = [observables.index(o) for o in col_names]\n                hash_val = hash_names(row_measurements, col_measurements, row_names, col_names)\n                cls(\n                    hash_val=hash_val,\n                    data_type='correlations',\n                    data=jnp.array(corr[np.ix_(row_idx, col_idx)], dtype=jnp.float64),\n                    row_names=row_names,\n                    col_names=col_names,\n                )\n\n    @classmethod\n    def get_data(\n        cls,\n        data_type: str,\n        include_measurements: Iterable[str],\n        row_names: Iterable[str],\n        col_names: Optional[Iterable[str]] = []\n    ):\n        '''\n        Retrieve the data array for the specified data type and combination of observables and measurements.\n\n        Parameters\n        ----------\n        data_type : str\n            The type of data to retrieve. It can be `correlations`, `central`, or `uncertainties`.\n        include_measurements : Iterable[str]\n            A list of measurement names to include in the retrieval.\n        row_names : Iterable[str]\n            The names of the observables corresponding to the rows of the data array.\n        col_names : Iterable[str], optional\n            The names of the observables corresponding to the columns of the data array. Default is an empty list.\n\n        Returns\n        -------\n        Optional[jnp.ndarray]\n            The data array for the specified data type and combination of observables and measurements, or `None` if not found.\n\n        Examples\n        --------\n        Retrieve the data array for a specific data type and combination of observables and measurements:\n\n        &gt;&gt;&gt; data = ExperimentalCorrelations.get_data(\n        ...     data_type='correlations',\n        ...     include_measurements=['measurement1'],\n        ...     row_names=['observable1'],\n        ...     col_names=['observable2']\n        ... )\n        '''\n        row_measurements = Measurement.get_measurements(row_names, include_measurements=include_measurements)\n        col_measurements = Measurement.get_measurements(col_names, include_measurements=include_measurements)\n        hash_val = hash_names(row_measurements, col_measurements, row_names, col_names)\n        hash_val_transposed = hash_names(col_measurements, row_measurements, col_names, row_names)\n        if hash_val not in cls._instances[data_type] and hash_val_transposed not in cls._instances[data_type]:\n            cls.compute(include_measurements)\n        if hash_val in cls._instances[data_type]:\n            return cls._instances[data_type][hash_val].data\n        elif hash_val_transposed in cls._instances[data_type]:\n            return cls._instances[data_type][hash_val_transposed].data.T\n        else:\n            return None\n\n    @classmethod\n    def get_cov_scaled(\n        cls,\n        include_measurements: Iterable[str],\n        row_names: Iterable[str],\n        col_names: Iterable[str],\n        std_exp_scaled_row: np.ndarray,\n        std_exp_scaled_col: np.ndarray,\n    ):\n        '''\n        Retrieve the scaled covariance matrix for the specified combination of observables and measurements.\n\n        Parameters\n        ----------\n        include_measurements : Iterable[str]\n            A list of measurement names to include in the retrieval.\n        row_names : Iterable[str]\n            The names of the observables corresponding to the rows of the covariance matrix.\n        col_names : Iterable[str]\n            The names of the observables corresponding to the columns of the covariance matrix.\n        std_exp_scaled_row : np.ndarray\n            The experimental standard deviations for the row observables, scaled by any additional factors.\n        std_exp_scaled_col : np.ndarray\n            The experimental standard deviations for the column observables, scaled by any additional factors.\n\n        Returns\n        -------\n        jnp.ndarray\n            The scaled covariance matrix for the specified combination of observables and measurements.\n\n        Examples\n        --------\n        Retrieve the scaled covariance matrix for a specific combination of observables and measurements:\n\n        &gt;&gt;&gt; cov_scaled = ExperimentalCorrelations.get_cov_scaled(\n        ...     include_measurements=['measurement1'],\n        ...     row_names=['observable1'],\n        ...     col_names=['observable2'],\n        ...     std_exp_scaled_row=np.array([0.5]),\n        ...     std_exp_scaled_col=np.array([1.2])\n        ... )\n        '''\n        row_measurements = Measurement.get_measurements(row_names, include_measurements=include_measurements)\n        col_measurements = Measurement.get_measurements(col_names, include_measurements=include_measurements)\n        hash_val = hash_names(row_measurements, col_measurements, row_names, col_names)\n        if hash_val in cls._covariance_scaled:\n            cov_scaled = cls._covariance_scaled[hash_val]\n        else:\n            corr = cls.get_data('correlations', include_measurements, row_names, col_names)\n            if corr is None:\n                raise ValueError(f\"Correlation data for {row_names} and {col_names} not found.\")\n            cov_scaled = corr * np.outer(std_exp_scaled_row, std_exp_scaled_col)\n            cov_scaled = jnp.array(cov_scaled, dtype=jnp.float64)\n            cls._covariance_scaled[hash_val] = cov_scaled\n        return cov_scaled\n</code></pre>"},{"location":"jelli/core/experimental_correlations/#jelli.core.experimental_correlations.ExperimentalCorrelations.__init__","title":"<code>__init__(hash_val, data_type, data, row_names, col_names)</code>","text":"<p>Initialize an instance of the ExperimentalCorrelations class.</p> <p>Parameters:</p> Name Type Description Default <code>hash_val</code> <code>str</code> <p>A unique hash value representing the combination of measurements and observables.</p> required <code>data_type</code> <code>str</code> <p>The type of data stored in the instance. It can be <code>correlations</code>, <code>central</code>, or <code>uncertainties</code>.</p> required <code>data</code> <code>ndarray</code> <p>The data array containing the correlation matrix, central values, or uncertainties.</p> required <code>row_names</code> <code>Iterable[str]</code> <p>The names of the observables corresponding to the rows of the data array.</p> required <code>col_names</code> <code>Iterable[str]</code> <p>The names of the observables corresponding to the columns of the data array.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Initialize an instance of the ExperimentalCorrelations class:</p> <pre><code>&gt;&gt;&gt; exp_corr = ExperimentalCorrelations(\n...     hash_val='unique_hash_value',\n...     data_type='correlations',\n...     data=np.array([[1.0, 0.5], [0.5, 1.0]]),\n...     row_names=['observable1', 'observable2'],\n...     col_names=['observable1', 'observable2']\n... )\n</code></pre> Source code in <code>jelli/core/experimental_correlations.py</code> <pre><code>def __init__(\n    self,\n    hash_val: str,\n    data_type: str,\n    data: np.ndarray,\n    row_names: Iterable[str],\n    col_names: Iterable[str],\n) -&gt; None:\n    '''\n    Initialize an instance of the ExperimentalCorrelations class.\n\n    Parameters\n    ----------\n    hash_val : str\n        A unique hash value representing the combination of measurements and observables.\n    data_type : str\n        The type of data stored in the instance. It can be `correlations`, `central`, or `uncertainties`.\n    data : np.ndarray\n        The data array containing the correlation matrix, central values, or uncertainties.\n    row_names : Iterable[str]\n        The names of the observables corresponding to the rows of the data array.\n    col_names : Iterable[str]\n        The names of the observables corresponding to the columns of the data array.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Initialize an instance of the ExperimentalCorrelations class:\n\n    &gt;&gt;&gt; exp_corr = ExperimentalCorrelations(\n    ...     hash_val='unique_hash_value',\n    ...     data_type='correlations',\n    ...     data=np.array([[1.0, 0.5], [0.5, 1.0]]),\n    ...     row_names=['observable1', 'observable2'],\n    ...     col_names=['observable1', 'observable2']\n    ... )\n    '''\n    self.hash_val = hash_val\n    self.data_type = data_type\n    self.data = data\n    self.row_names = row_names\n    self.col_names = col_names\n    self._instances[data_type][hash_val] = self\n</code></pre>"},{"location":"jelli/core/experimental_correlations/#jelli.core.experimental_correlations.ExperimentalCorrelations.compute","title":"<code>compute(include_measurements, n_samples=int(1000000.0), seed=None)</code>  <code>classmethod</code>","text":"<p>Compute the correlation matrices, central values, and uncertainties from the specified measurements.</p> <p>Parameters:</p> Name Type Description Default <code>include_measurements</code> <code>Iterable[str]</code> <p>A list of measurement names to include in the computation.</p> required <code>n_samples</code> <code>int</code> <p>The number of samples to draw for numerical approximations of non-Gaussian distributions. Default is 1e6.</p> <code>int(1000000.0)</code> <code>seed</code> <code>int</code> <p>A random seed for reproducibility of the samples. Default is None.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Compute the correlation matrices, central values, and uncertainties from the specified measurements:</p> <pre><code>&gt;&gt;&gt; ExperimentalCorrelations.compute(include_measurements=['measurement1', 'measurement2'])\n</code></pre> Source code in <code>jelli/core/experimental_correlations.py</code> <pre><code>@classmethod\ndef compute(\n    cls,\n    include_measurements: Iterable[str],\n    n_samples: int = int(1e6),\n    seed: int = None\n) -&gt; None:\n    '''\n    Compute the correlation matrices, central values, and uncertainties from the specified measurements.\n\n    Parameters\n    ----------\n    include_measurements : Iterable[str]\n        A list of measurement names to include in the computation.\n    n_samples : int, optional\n        The number of samples to draw for numerical approximations of non-Gaussian distributions. Default is 1e6.\n    seed : int, optional\n        A random seed for reproducibility of the samples. Default is None.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Compute the correlation matrices, central values, and uncertainties from the specified measurements:\n\n    &gt;&gt;&gt; ExperimentalCorrelations.compute(include_measurements=['measurement1', 'measurement2'])\n    '''\n\n    observables = list(chain.from_iterable(cls._observable_names))\n\n    # get univariate constraints and combine them for each observable\n    constraints_univariate = Measurement.get_constraints(\n        observables,\n        distribution_types=[k for k in logpdf_functions.keys() if k not in ['MultivariateNormalDistribution']],\n        include_measurements=include_measurements,\n        )\n    constraints_list = []\n    for observable in observables:\n        constraints_observable = {}\n        for dist_type, dist_info in constraints_univariate.items():\n            mask = dist_info['observables'] == observable\n            if np.any(mask):\n                constraints_observable[dist_type] = {\n                    k: v[mask] for k, v in dist_info.items()\n                }\n        if constraints_observable:\n            constraints_list.append(constraints_observable)\n    constraints_univariate = Measurement.combine_constraints(constraints_list)\n\n    # construct covariance matrix and mean vector from univariate constraints\n    cov = np.diag([np.inf] * len(observables))\n    mean = np.zeros(len(observables))\n    for dist_type, dist_info in constraints_univariate.items():\n        if dist_type == 'NormalDistribution' or dist_type == 'HalfNormalDistribution':  # replace HalfNormalDistribution with zero-mean NormalDistribution\n            central_value = dist_info['central_value']\n            standard_deviation = dist_info['standard_deviation']\n            observable_indices = dist_info['observable_indices']\n        else:  # numerically obtain the Gaussian approximation\n            samples = get_distribution_samples(dist_type, dist_info, n_samples, seed)\n            central_value = np.mean(samples, axis=1)\n            standard_deviation = np.std(samples, axis=1)\n            observable_indices = dist_info['observable_indices']\n        cov[observable_indices, observable_indices] = standard_deviation**2\n        mean[observable_indices] = central_value\n\n    # get multivariate constraints\n    constraints_multivariate = Measurement.get_constraints(\n        observables,\n        distribution_types=['MultivariateNormalDistribution'],\n        include_measurements=include_measurements,\n    )\n    if constraints_multivariate:\n        # combine all covariance matrices and mean vectors using the weighted average\n        weights = [np.diag(1/np.diag(cov))]\n        means = [mean]\n        constraints_multivariate = constraints_multivariate['MultivariateNormalDistribution']\n        for i in range(len(constraints_multivariate['central_value'])):\n            weight_i = np.zeros((len(observables), len(observables)))\n            mean_i = np.zeros(len(observables))\n            observable_indices_i = constraints_multivariate['observable_indices'][i]\n            central_value_i = constraints_multivariate['central_value'][i]\n            standard_deviation_i = constraints_multivariate['standard_deviation'][i]\n            inverse_correlation_i = constraints_multivariate['inverse_correlation'][i]\n            weight_i[np.ix_(observable_indices_i, observable_indices_i)] = inverse_correlation_i / np.outer(standard_deviation_i, standard_deviation_i)\n            mean_i[observable_indices_i] = central_value_i\n            weights.append(weight_i)\n            means.append(mean_i)\n        inv_cov = np.sum(weights, axis=0)\n        # regularize inversion asuming inv_cov = D R D where R has unit diagonal, then invert R instead of inv_cov\n        d = np.sqrt(np.diag(inv_cov))\n        nonzero = d != 0  # unconstrained observables have zeros\n        inv_cov = inv_cov[np.ix_(nonzero, nonzero)]\n        d = d[nonzero]\n        d2 = np.outer(d, d)\n        R = inv_cov / d2\n        inv_R = np.linalg.inv(R)\n        cov = np.diag([np.nan] * len(nonzero))\n        cov[np.ix_(nonzero, nonzero)] = inv_R / d2\n        mean = cov @ np.sum([w @ m for w, m in zip(weights, means)], axis=0)\n    else:\n        cov[cov == np.inf] = np.nan\n    std = np.sqrt(np.diag(cov))\n    corr = cov / np.outer(std, std)\n\n    for i, row_names in enumerate(cls._observable_names):\n        row_measurements = Measurement.get_measurements(row_names, include_measurements=include_measurements)\n        row_idx = [observables.index(o) for o in row_names]\n        hash_val = hash_names(row_measurements, row_names)\n        cls(\n            hash_val=hash_val,\n            data_type='central',\n            data=jnp.array(mean[row_idx], dtype=jnp.float64),\n            row_names=row_names,\n            col_names=[],\n        )\n        cls(\n            hash_val=hash_val,\n            data_type='uncertainties',\n            data=jnp.array(std[row_idx], dtype=jnp.float64),\n            row_names=row_names,\n            col_names=[],\n        )\n        for j in range(i, len(cls._observable_names)):\n            col_names = cls._observable_names[j]\n            col_measurements = Measurement.get_measurements(col_names, include_measurements=include_measurements)\n            col_idx = [observables.index(o) for o in col_names]\n            hash_val = hash_names(row_measurements, col_measurements, row_names, col_names)\n            cls(\n                hash_val=hash_val,\n                data_type='correlations',\n                data=jnp.array(corr[np.ix_(row_idx, col_idx)], dtype=jnp.float64),\n                row_names=row_names,\n                col_names=col_names,\n            )\n</code></pre>"},{"location":"jelli/core/experimental_correlations/#jelli.core.experimental_correlations.ExperimentalCorrelations.get_cov_scaled","title":"<code>get_cov_scaled(include_measurements, row_names, col_names, std_exp_scaled_row, std_exp_scaled_col)</code>  <code>classmethod</code>","text":"<p>Retrieve the scaled covariance matrix for the specified combination of observables and measurements.</p> <p>Parameters:</p> Name Type Description Default <code>include_measurements</code> <code>Iterable[str]</code> <p>A list of measurement names to include in the retrieval.</p> required <code>row_names</code> <code>Iterable[str]</code> <p>The names of the observables corresponding to the rows of the covariance matrix.</p> required <code>col_names</code> <code>Iterable[str]</code> <p>The names of the observables corresponding to the columns of the covariance matrix.</p> required <code>std_exp_scaled_row</code> <code>ndarray</code> <p>The experimental standard deviations for the row observables, scaled by any additional factors.</p> required <code>std_exp_scaled_col</code> <code>ndarray</code> <p>The experimental standard deviations for the column observables, scaled by any additional factors.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The scaled covariance matrix for the specified combination of observables and measurements.</p> <p>Examples:</p> <p>Retrieve the scaled covariance matrix for a specific combination of observables and measurements:</p> <pre><code>&gt;&gt;&gt; cov_scaled = ExperimentalCorrelations.get_cov_scaled(\n...     include_measurements=['measurement1'],\n...     row_names=['observable1'],\n...     col_names=['observable2'],\n...     std_exp_scaled_row=np.array([0.5]),\n...     std_exp_scaled_col=np.array([1.2])\n... )\n</code></pre> Source code in <code>jelli/core/experimental_correlations.py</code> <pre><code>@classmethod\ndef get_cov_scaled(\n    cls,\n    include_measurements: Iterable[str],\n    row_names: Iterable[str],\n    col_names: Iterable[str],\n    std_exp_scaled_row: np.ndarray,\n    std_exp_scaled_col: np.ndarray,\n):\n    '''\n    Retrieve the scaled covariance matrix for the specified combination of observables and measurements.\n\n    Parameters\n    ----------\n    include_measurements : Iterable[str]\n        A list of measurement names to include in the retrieval.\n    row_names : Iterable[str]\n        The names of the observables corresponding to the rows of the covariance matrix.\n    col_names : Iterable[str]\n        The names of the observables corresponding to the columns of the covariance matrix.\n    std_exp_scaled_row : np.ndarray\n        The experimental standard deviations for the row observables, scaled by any additional factors.\n    std_exp_scaled_col : np.ndarray\n        The experimental standard deviations for the column observables, scaled by any additional factors.\n\n    Returns\n    -------\n    jnp.ndarray\n        The scaled covariance matrix for the specified combination of observables and measurements.\n\n    Examples\n    --------\n    Retrieve the scaled covariance matrix for a specific combination of observables and measurements:\n\n    &gt;&gt;&gt; cov_scaled = ExperimentalCorrelations.get_cov_scaled(\n    ...     include_measurements=['measurement1'],\n    ...     row_names=['observable1'],\n    ...     col_names=['observable2'],\n    ...     std_exp_scaled_row=np.array([0.5]),\n    ...     std_exp_scaled_col=np.array([1.2])\n    ... )\n    '''\n    row_measurements = Measurement.get_measurements(row_names, include_measurements=include_measurements)\n    col_measurements = Measurement.get_measurements(col_names, include_measurements=include_measurements)\n    hash_val = hash_names(row_measurements, col_measurements, row_names, col_names)\n    if hash_val in cls._covariance_scaled:\n        cov_scaled = cls._covariance_scaled[hash_val]\n    else:\n        corr = cls.get_data('correlations', include_measurements, row_names, col_names)\n        if corr is None:\n            raise ValueError(f\"Correlation data for {row_names} and {col_names} not found.\")\n        cov_scaled = corr * np.outer(std_exp_scaled_row, std_exp_scaled_col)\n        cov_scaled = jnp.array(cov_scaled, dtype=jnp.float64)\n        cls._covariance_scaled[hash_val] = cov_scaled\n    return cov_scaled\n</code></pre>"},{"location":"jelli/core/experimental_correlations/#jelli.core.experimental_correlations.ExperimentalCorrelations.get_data","title":"<code>get_data(data_type, include_measurements, row_names, col_names=[])</code>  <code>classmethod</code>","text":"<p>Retrieve the data array for the specified data type and combination of observables and measurements.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>str</code> <p>The type of data to retrieve. It can be <code>correlations</code>, <code>central</code>, or <code>uncertainties</code>.</p> required <code>include_measurements</code> <code>Iterable[str]</code> <p>A list of measurement names to include in the retrieval.</p> required <code>row_names</code> <code>Iterable[str]</code> <p>The names of the observables corresponding to the rows of the data array.</p> required <code>col_names</code> <code>Iterable[str]</code> <p>The names of the observables corresponding to the columns of the data array. Default is an empty list.</p> <code>[]</code> <p>Returns:</p> Type Description <code>Optional[ndarray]</code> <p>The data array for the specified data type and combination of observables and measurements, or <code>None</code> if not found.</p> <p>Examples:</p> <p>Retrieve the data array for a specific data type and combination of observables and measurements:</p> <pre><code>&gt;&gt;&gt; data = ExperimentalCorrelations.get_data(\n...     data_type='correlations',\n...     include_measurements=['measurement1'],\n...     row_names=['observable1'],\n...     col_names=['observable2']\n... )\n</code></pre> Source code in <code>jelli/core/experimental_correlations.py</code> <pre><code>@classmethod\ndef get_data(\n    cls,\n    data_type: str,\n    include_measurements: Iterable[str],\n    row_names: Iterable[str],\n    col_names: Optional[Iterable[str]] = []\n):\n    '''\n    Retrieve the data array for the specified data type and combination of observables and measurements.\n\n    Parameters\n    ----------\n    data_type : str\n        The type of data to retrieve. It can be `correlations`, `central`, or `uncertainties`.\n    include_measurements : Iterable[str]\n        A list of measurement names to include in the retrieval.\n    row_names : Iterable[str]\n        The names of the observables corresponding to the rows of the data array.\n    col_names : Iterable[str], optional\n        The names of the observables corresponding to the columns of the data array. Default is an empty list.\n\n    Returns\n    -------\n    Optional[jnp.ndarray]\n        The data array for the specified data type and combination of observables and measurements, or `None` if not found.\n\n    Examples\n    --------\n    Retrieve the data array for a specific data type and combination of observables and measurements:\n\n    &gt;&gt;&gt; data = ExperimentalCorrelations.get_data(\n    ...     data_type='correlations',\n    ...     include_measurements=['measurement1'],\n    ...     row_names=['observable1'],\n    ...     col_names=['observable2']\n    ... )\n    '''\n    row_measurements = Measurement.get_measurements(row_names, include_measurements=include_measurements)\n    col_measurements = Measurement.get_measurements(col_names, include_measurements=include_measurements)\n    hash_val = hash_names(row_measurements, col_measurements, row_names, col_names)\n    hash_val_transposed = hash_names(col_measurements, row_measurements, col_names, row_names)\n    if hash_val not in cls._instances[data_type] and hash_val_transposed not in cls._instances[data_type]:\n        cls.compute(include_measurements)\n    if hash_val in cls._instances[data_type]:\n        return cls._instances[data_type][hash_val].data\n    elif hash_val_transposed in cls._instances[data_type]:\n        return cls._instances[data_type][hash_val_transposed].data.T\n    else:\n        return None\n</code></pre>"},{"location":"jelli/core/experimental_correlations/#jelli.core.experimental_correlations.ExperimentalCorrelations.load","title":"<code>load()</code>  <code>classmethod</code>","text":"<p>Load observable names from all correlated observable sectors and initialize class-level attributes.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Load observable names from all correlated observable sectors:</p> <pre><code>&gt;&gt;&gt; ExperimentalCorrelations.load()\n</code></pre> Source code in <code>jelli/core/experimental_correlations.py</code> <pre><code>@classmethod\ndef load(cls) -&gt; None:\n    '''\n    Load observable names from all correlated observable sectors and initialize class-level attributes.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Load observable names from all correlated observable sectors:\n\n    &gt;&gt;&gt; ExperimentalCorrelations.load()\n    '''\n    observable_names = []\n    for observable_sector in ObservableSector.get_all():\n        if observable_sector.observable_uncertainties is not None:\n            observable_names.append(observable_sector.observable_names)\n    cls._observable_names = observable_names\n    cls._instances = {\n        'correlations': {},\n        'central': {},\n        'uncertainties': {}\n    }\n    cls._covariance_scaled = {}\n</code></pre>"},{"location":"jelli/core/global_likelihood/","title":"jelli.core.global_likelihood","text":""},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood","title":"<code>GlobalLikelihood</code>","text":"<p>A class to represent the global likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The EFT name (e.g., <code>SMEFT</code>, <code>WET</code>). Required if <code>custom_basis</code> is not provided.</p> <code>None</code> <code>basis</code> <code>str</code> <p>The basis name (e.g., <code>Warsaw</code>, <code>JMS</code>). Required if <code>custom_basis</code> is not provided.</p> <code>None</code> <code>custom_basis</code> <code>str</code> <p>The name of a custom basis defined using the <code>CustomBasis</code> class. Required if <code>eft</code> and <code>basis</code> are not provided.</p> <code>None</code> <code>include_observable_sectors</code> <code>list[str]</code> <p>A list of observable sector names to include in the likelihood. If not provided, all loaded observable sectors are included.</p> <code>None</code> <code>exclude_observable_sectors</code> <code>list[str]</code> <p>A list of observable sector names to exclude from the likelihood. If not provided, no sectors are excluded.</p> <code>None</code> <code>include_measurements</code> <code>list[str]</code> <p>A list of measurement names to include in the likelihood. If not provided, all loaded measurements are included.</p> <code>None</code> <code>exclude_measurements</code> <code>list[str]</code> <p>A list of measurement names to exclude from the likelihood. If not provided, no measurements are excluded.</p> <code>None</code> <code>custom_likelihoods</code> <code>dict[str, list[str]]</code> <p>A dictionary defining custom likelihoods. The keys are the names of the custom likelihoods, and the values are lists of observable names to include in each custom likelihood.</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>eft</code> <code>str</code> <p>The EFT name (e.g., <code>SMEFT</code>, <code>WET</code>).</p> <code>basis</code> <code>str</code> <p>The basis name (e.g., <code>Warsaw</code>, <code>JMS</code>).</p> <code>custom_basis</code> <code>str</code> <p>The name of the custom basis defined using the <code>CustomBasis</code> class.</p> <code>observable_sectors_gaussian</code> <code>list[str]</code> <p>The list of observable sector names containing observables with Gaussian theory uncertainties.</p> <code>observable_sectors_no_theory_uncertainty</code> <code>list[str]</code> <p>The list of observable sector names containing observables with no theory uncertainty.</p> <code>basis_mode</code> <code>str</code> <p>The basis mode, either <code>rgevolve</code>, <code>wcxf</code>, or <code>custom</code>.</p> <code>observable_sectors</code> <code>list[str]</code> <p>The list of observable sector names included in the likelihood.</p> <code>parameter_basis</code> <code>list[str]</code> <p>The list of parameter names in the basis.</p> <code>parameter_basis_split_re_im</code> <code>list[Tuple[str, str or None]]</code> <p>The list of parameter names in the basis, split into real and imaginary parts. Each entry is a tuple where the first element is the parameter name and the second element is <code>R</code> for real parameters or <code>I</code> for imaginary parameters.</p> <code>include_measurements</code> <code>dict[str, Measurement]</code> <p>The measurements included in the likelihood.</p> <code>observables_constrained</code> <code>set[str]</code> <p>The set of observables constrained by the included measurements.</p> <code>observables_no_theory_uncertainty</code> <code>list[str]</code> <p>The list of observables with no theory uncertainty.</p> <code>observables_gaussian</code> <code>list[str]</code> <p>The list of observables with Gaussian theory uncertainties.</p> <code>observables_correlated</code> <code>list[list[str]]</code> <p>The list of lists of observables in correlated observable sectors.</p> <code>prediction_data_no_theory_uncertainty</code> <code>list[list[array]]</code> <p>The prediction data for observables with no theory uncertainty.</p> <code>prediction_function_no_theory_uncertainty</code> <code>callable</code> <p>The prediction function for observables with no theory uncertainty.</p> <code>prediction_data_correlated</code> <code>list[list[list[array]]]</code> <p>The prediction data for observables in correlated sectors.</p> <code>prediction_function_correlated</code> <code>list[callable]</code> <p>The list of prediction functions for correlated observable sectors.</p> <code>custom_likelihoods_gaussian</code> <code>dict[str, list[str]]</code> <p>The custom likelihoods containing observables with Gaussian theory uncertainties.</p> <code>custom_likelihoods_no_theory_uncertainty</code> <code>dict[str, list[str]]</code> <p>The custom likelihoods containing observables with no theory uncertainty.</p> <code>likelihoods</code> <code>list[str]</code> <p>The list of all likelihood names, including custom likelihoods and 'global'.</p> <code>constraints_no_theory_uncertainty</code> <code>dict</code> <p>The constraints for observables with no theory uncertainty.</p> <code>constraints_no_theory_uncertainty_no_corr</code> <code>dict</code> <p>The constraints for observables with no theory uncertainty, neglecting experimental correlations (used for observable table).</p> <code>selector_matrix_no_th_unc_univariate</code> <code>array</code> <p>The selector matrix mapping observables with no theory uncertainty to likelihoods for univariate distributions.</p> <code>selector_matrix_no_th_unc_multivariate</code> <code>array</code> <p>The selector matrix mapping unique multivariate normal contributions to likelihoods for observables with no theory uncertainty.</p> <code>constraints_correlated_par_indep_cov</code> <code>dict</code> <p>The constraints for observables in correlated sectors with parameter-independent covariance.</p> <code>constraints_correlated_par_dep_cov</code> <code>dict</code> <p>The constraints for observables in correlated sectors with parameter-dependent covariance.</p> <code>selector_matrix_correlated</code> <code>List[array]</code> <p>The selector matrices mapping unique multivariate normal contributions to likelihoods for observables in correlated sectors.</p> <code>sm_log_likelihood_summed</code> <code>array</code> <p>The Standard Model log-likelihood summed over all observables.</p> <code>sm_log_likelihood_correlated</code> <code>array</code> <p>The Standard Model log-likelihood values for correlated observables.</p> <code>sm_log_likelihood_correlated_no_corr</code> <code>array</code> <p>The Standard Model log-likelihood values for correlated observables, neglecting correlations (used for observable table).</p> <code>sm_log_likelihood_no_theory_uncertainty</code> <code>array</code> <p>The Standard Model log-likelihood values for observables with no theory uncertainty.</p> <code>sm_log_likelihood_no_theory_uncertainty_no_corr</code> <code>array</code> <p>The Standard Model log-likelihood values for observables with no theory uncertainty, neglecting correlations (used for observable table).</p> <code>experimental_values_no_theory_uncertainty</code> <code>dict[str, list[float]]</code> <p>A dictionary mapping observable names to their experimental values and uncertainties for observables with no theory uncertainty (used for observable table).</p> <code>_observables_per_likelihood_no_theory_uncertainty</code> <code>dict[str, list[str]]</code> <p>A dictionary mapping likelihood names to lists of observables with no theory uncertainty.</p> <code>_observables_per_likelihood_correlated</code> <code>dict[str, list[str]]</code> <p>A dictionary mapping likelihood names to lists of observables in correlated sectors.</p> <code>_likelihood_indices_no_theory_uncertainty</code> <code>array</code> <p>The indices of the likelihoods with no theory uncertainty in the full likelihood list.</p> <code>_likelihood_indices_correlated</code> <code>array</code> <p>The indices of the correlated likelihoods in the full likelihood list.</p> <code>_likelihood_indices_global</code> <code>array</code> <p>The indices of the likelihoods included in the global likelihood (i.e., not custom likelihoods).</p> <code>_reference_scale</code> <code>float</code> <p>The reference scale for the likelihood.</p> <code>_indices_mvn_not_custom</code> <code>array</code> <p>The indices of multivariate normal contributions not included in custom likelihoods.</p> <code>_log_likelihood_point_function</code> <code>callable</code> <p>The JIT-compiled function to compute the information needed for <code>GlobalLikelihoodPoint</code> instances.</p> <code>_log_likelihood_point</code> <code>callable</code> <p>A partial function wrapping <code>_log_likelihood_point_function</code> with fixed arguments.</p> <code>_obstable</code> <code>callable</code> <p>The JIT-compiled function to compute the observable table information.</p> <code>_cache_compiled_likelihood</code> <code>dict</code> <p>A cache for <code>CompiledLikelihood</code> instances to avoid redundant computations.</p> <p>Methods:</p> Name Description <code>load</code> <p>Initializes <code>ObservableSector</code>, <code>Measurement</code>, <code>TheoryCorrelations</code>, and <code>ExperimentalCorrelations</code> classes by loading data from the specified path.</p> <code>get_negative_log_likelihood</code> <p>Returns a function to compute the negative log-likelihood for given parameters and likelihood.</p> <code>parameter_point</code> <p>Returns a <code>GlobalLikelihoodPoint</code> instance for the specified parameter values.</p> <code>get_compiled_likelihood</code> <p>Returns an instance of <code>CompiledLikelihood</code> for the specified parameters and likelihood.</p> <code>plot_data_2d</code> <p>Computes a grid of chi-squared values over a 2D parameter space for plotting. Returns a dictionary containing the parameter grid and the corresponding chi-squared values.</p> <code>_get_observable_sectors</code> <p>Determines the observable sectors to include in the likelihood based on inclusion/exclusion lists.</p> <code>_get_observable_sectors_correlated</code> <p>Determines and returns useful information about correlated observable sectors.</p> <code>_get_custom_likelihoods</code> <p>Processes custom likelihoods.</p> <code>_get_observables_per_likelihood</code> <p>Constructs dictionaries mapping likelihood names to lists of observables for both no theory uncertainty and correlated sectors.</p> <code>_get_prediction_function_gaussian</code> <p>Returns a prediction function for the Gaussian observable sectors.</p> <code>_get_prediction_function_no_theory_uncertainty</code> <p>Returns a prediction function for observables with no theory uncertainty.</p> <code>_get_constraints_no_theory_uncertainty</code> <p>Returns the constraints and selector matrices for observables with no theory uncertainty.</p> <code>_get_constraints_correlated</code> <p>Returns the constraints and selector matrices for correlated observable sectors.</p> <code>_get_log_likelihood_point_function</code> <p>Returns a JIT-compiled function to compute the information needed for <code>GlobalLikelihoodPoint</code> instances.</p> <code>_get_obstable_function</code> <p>Returns a JIT-compiled function to compute the observable table information.</p> <code>_get_parameter_basis</code> <p>Determines the parameter basis and splits parameters into real and imaginary parts.</p> <code>_get_par_array</code> <p>Converts a parameter dictionary into a JAX array.</p> <code>_get_reference_scale</code> <p>Determines the reference scale for the likelihood.</p> <p>Examples:</p> <p>Load all observable sectors, measurements, and correlations from the specified path:</p> <pre><code>&gt;&gt;&gt; GlobalLikelihood.load('path/to/data')\n</code></pre> <p>Create a global likelihood instance for the SMEFT in the Warsaw basis, including all observable sectors and measurements:</p> <pre><code>&gt;&gt;&gt; gl = GlobalLikelihood(eft='SMEFT', basis='Warsaw')\n</code></pre> <p>Create a global likelihood instance for a custom basis named 'my_basis', including only specific observable sectors and measurements:</p> <pre><code>&gt;&gt;&gt; gl = GlobalLikelihood(custom_basis='my_basis', include_observable_sectors=['sector1', 'sector2'], include_measurements=['measurement1', 'measurement2'])\n</code></pre> <p>Create a global likelihood instance for the SMEFT in the Warsaw basis, defining a custom likelihood that includes specific observables:</p> <pre><code>&gt;&gt;&gt; custom_likelihoods = {'my_likelihood': ['observable1', 'observable2']}\n&gt;&gt;&gt; gl = GlobalLikelihood(eft='SMEFT', basis='Warsaw', custom_likelihoods=custom_likelihoods)\n</code></pre> <p>Define a <code>GlobalLikelihoodPoint</code> instance for specific parameter values at the scale of 1000.0 GeV:</p> <pre><code>&gt;&gt;&gt; def par_func(x, y):\n...     return {'lq1_1111': x, 'lq3_1111': y}\n&gt;&gt;&gt; glp = gl.parameter_point(par_func(1e-8, 1e-8), 1000.0)\n</code></pre> <p>Obtain the 2D chi-squared grid for two parameters over specified ranges:</p> <pre><code>&gt;&gt;&gt; plot_data = gl.plot_data_2d(par_func, scale=1000.0, x_min=-1e-8, x_max=1e-8, y_min=-1e-8, y_max=1e-8, steps=50)\n</code></pre> <p>Get the negative log-likelihood function and data for specific parameters and a likelihood:</p> <pre><code>&gt;&gt;&gt; negative_log_likelihood, log_likelihood_data = gl.get_negative_log_likelihood(par_list=[('lq1_1111', 'R'), ('lq3_1111', 'R')], likelihood='global', par_dep_cov=False)\n</code></pre> <p>Get an instance of <code>CompiledLikelihood</code> for specific parameters and a likelihood:</p> <pre><code>&gt;&gt;&gt; compiled_likelihood = gl.get_compiled_likelihood(par_list=[('lq1_1111', 'R'), ('lq3_1111', 'R')], likelihood='global', par_dep_cov=False)\n</code></pre> <p>Access the parameter basis:</p> <pre><code>&gt;&gt;&gt; parameter_basis = gl.parameter_basis\n&gt;&gt;&gt; parameter_basis_split_re_im = gl.parameter_basis_split_re_im\n</code></pre> <p>Access the basis mode:</p> <pre><code>&gt;&gt;&gt; basis_mode = gl.basis_mode\n</code></pre> <p>Access the observables included in the likelihood:</p> <pre><code>&gt;&gt;&gt; observables_gaussian = gl.observables_gaussian\n&gt;&gt;&gt; observables_no_theory_uncertainty = gl.observables_no_theory_uncertainty\n</code></pre> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>class GlobalLikelihood():\n    '''\n    A class to represent the global likelihood.\n\n    Parameters\n    ----------\n    eft : str, optional\n        The EFT name (e.g., `SMEFT`, `WET`). Required if `custom_basis` is not provided.\n    basis : str, optional\n        The basis name (e.g., `Warsaw`, `JMS`). Required if `custom_basis` is not provided.\n    custom_basis : str, optional\n        The name of a custom basis defined using the `CustomBasis` class. Required if `eft` and `basis` are not provided.\n    include_observable_sectors : list[str], optional\n        A list of observable sector names to include in the likelihood. If not provided, all loaded observable sectors are included.\n    exclude_observable_sectors : list[str], optional\n        A list of observable sector names to exclude from the likelihood. If not provided, no sectors are excluded.\n    include_measurements : list[str], optional\n        A list of measurement names to include in the likelihood. If not provided, all loaded measurements are included.\n    exclude_measurements : list[str], optional\n        A list of measurement names to exclude from the likelihood. If not provided, no measurements are excluded.\n    custom_likelihoods : dict[str, list[str]], optional\n        A dictionary defining custom likelihoods. The keys are the names of the custom likelihoods, and the values are lists of observable names to include in each custom likelihood.\n\n    Attributes\n    ----------\n    eft : str\n        The EFT name (e.g., `SMEFT`, `WET`).\n    basis : str\n        The basis name (e.g., `Warsaw`, `JMS`).\n    custom_basis : str\n        The name of the custom basis defined using the `CustomBasis` class.\n    observable_sectors_gaussian : list[str]\n        The list of observable sector names containing observables with Gaussian theory uncertainties.\n    observable_sectors_no_theory_uncertainty : list[str]\n        The list of observable sector names containing observables with no theory uncertainty.\n    basis_mode : str\n        The basis mode, either `rgevolve`, `wcxf`, or `custom`.\n    observable_sectors : list[str]\n        The list of observable sector names included in the likelihood.\n    parameter_basis : list[str]\n        The list of parameter names in the basis.\n    parameter_basis_split_re_im : list[Tuple[str, str or None]]\n        The list of parameter names in the basis, split into real and imaginary parts. Each entry is a tuple where the first element is the parameter name and the second element is `R` for real parameters or `I` for imaginary parameters.\n    include_measurements : dict[str, Measurement]\n        The measurements included in the likelihood.\n    observables_constrained : set[str]\n        The set of observables constrained by the included measurements.\n    observables_no_theory_uncertainty : list[str]\n        The list of observables with no theory uncertainty.\n    observables_gaussian : list[str]\n        The list of observables with Gaussian theory uncertainties.\n    observables_correlated : list[list[str]]\n        The list of lists of observables in correlated observable sectors.\n    prediction_data_no_theory_uncertainty : list[list[jnp.array]]\n        The prediction data for observables with no theory uncertainty.\n    prediction_function_no_theory_uncertainty : callable\n        The prediction function for observables with no theory uncertainty.\n    prediction_data_correlated : list[list[list[jnp.array]]]\n        The prediction data for observables in correlated sectors.\n    prediction_function_correlated : list[callable]\n        The list of prediction functions for correlated observable sectors.\n    custom_likelihoods_gaussian : dict[str, list[str]]\n        The custom likelihoods containing observables with Gaussian theory uncertainties.\n    custom_likelihoods_no_theory_uncertainty : dict[str, list[str]]\n        The custom likelihoods containing observables with no theory uncertainty.\n    likelihoods : list[str]\n        The list of all likelihood names, including custom likelihoods and 'global'.\n    constraints_no_theory_uncertainty : dict\n        The constraints for observables with no theory uncertainty.\n    constraints_no_theory_uncertainty_no_corr : dict\n        The constraints for observables with no theory uncertainty, neglecting experimental correlations (used for observable table).\n    selector_matrix_no_th_unc_univariate : jnp.array\n        The selector matrix mapping observables with no theory uncertainty to likelihoods for univariate distributions.\n    selector_matrix_no_th_unc_multivariate : jnp.array\n        The selector matrix mapping unique multivariate normal contributions to likelihoods for observables with no theory uncertainty.\n    constraints_correlated_par_indep_cov : dict\n        The constraints for observables in correlated sectors with parameter-independent covariance.\n    constraints_correlated_par_dep_cov : dict\n        The constraints for observables in correlated sectors with parameter-dependent covariance.\n    selector_matrix_correlated : List[jnp.array]\n        The selector matrices mapping unique multivariate normal contributions to likelihoods for observables in correlated sectors.\n    sm_log_likelihood_summed : jnp.array\n        The Standard Model log-likelihood summed over all observables.\n    sm_log_likelihood_correlated : jnp.array\n        The Standard Model log-likelihood values for correlated observables.\n    sm_log_likelihood_correlated_no_corr : jnp.array\n        The Standard Model log-likelihood values for correlated observables, neglecting correlations (used for observable table).\n    sm_log_likelihood_no_theory_uncertainty : jnp.array\n        The Standard Model log-likelihood values for observables with no theory uncertainty.\n    sm_log_likelihood_no_theory_uncertainty_no_corr : jnp.array\n        The Standard Model log-likelihood values for observables with no theory uncertainty, neglecting correlations (used for observable table).\n    experimental_values_no_theory_uncertainty : dict[str, list[float]]\n        A dictionary mapping observable names to their experimental values and uncertainties for observables with no theory uncertainty (used for observable table).\n    _observables_per_likelihood_no_theory_uncertainty : dict[str, list[str]]\n        A dictionary mapping likelihood names to lists of observables with no theory uncertainty.\n    _observables_per_likelihood_correlated : dict[str, list[str]]\n        A dictionary mapping likelihood names to lists of observables in correlated sectors.\n    _likelihood_indices_no_theory_uncertainty : jnp.array\n        The indices of the likelihoods with no theory uncertainty in the full likelihood list.\n    _likelihood_indices_correlated : jnp.array\n        The indices of the correlated likelihoods in the full likelihood list.\n    _likelihood_indices_global : jnp.array\n        The indices of the likelihoods included in the global likelihood (i.e., not custom likelihoods).\n    _reference_scale : float\n        The reference scale for the likelihood.\n    _indices_mvn_not_custom : jnp.array\n        The indices of multivariate normal contributions not included in custom likelihoods.\n    _log_likelihood_point_function : callable\n        The JIT-compiled function to compute the information needed for `GlobalLikelihoodPoint` instances.\n    _log_likelihood_point : callable\n        A partial function wrapping `_log_likelihood_point_function` with fixed arguments.\n    _obstable : callable\n        The JIT-compiled function to compute the observable table information.\n    _cache_compiled_likelihood : dict\n        A cache for `CompiledLikelihood` instances to avoid redundant computations.\n\n    Methods\n    -------\n    load(path: str) -&gt; None\n        Initializes `ObservableSector`, `Measurement`, `TheoryCorrelations`, and `ExperimentalCorrelations` classes by loading data from the specified path.\n    get_negative_log_likelihood(par_list: List[Tuple[str, str]], likelihood: Union[str, Tuple[str, ...]], par_dep_cov: bool) -&gt; Tuple[Callable, List]\n        Returns a function to compute the negative log-likelihood for given parameters and likelihood.\n    parameter_point(*args, par_dep_cov: bool = False) -&gt; GlobalLikelihoodPoint\n        Returns a `GlobalLikelihoodPoint` instance for the specified parameter values.\n    get_compiled_likelihood(par_list: List[Tuple[str, str]], likelihood: Union[str, Tuple[str, ...]], par_dep_cov: bool = False) -&gt; CompiledLikelihood\n        Returns an instance of `CompiledLikelihood` for the specified parameters and likelihood.\n    plot_data_2d(par_fct, scale, x_min, x_max, y_min, y_max, x_log=False, y_log=False, steps=20, par_dep_cov=False) -&gt; Dict\n        Computes a grid of chi-squared values over a 2D parameter space for plotting. Returns a dictionary containing the parameter grid and the corresponding chi-squared values.\n    _get_observable_sectors(include_observable_sectors, exclude_observable_sectors) -&gt; Tuple[List[str], List[str], str]\n        Determines the observable sectors to include in the likelihood based on inclusion/exclusion lists.\n    _get_observable_sectors_correlated() -&gt; Tuple[List[List[str]], List[List[jnp.array]], List[jnp.array], List[jnp.array], List[jnp.array], List[jnp.array]]\n        Determines and returns useful information about correlated observable sectors.\n    _get_custom_likelihoods(custom_likelihoods) -&gt; Tuple[Dict[str, List[str]], Dict[str, List[str]]]\n        Processes custom likelihoods.\n    _get_observables_per_likelihood() -&gt; Tuple[Dict[str, List[str]], Dict[str, List[str]]]\n        Constructs dictionaries mapping likelihood names to lists of observables for both no theory uncertainty and correlated sectors.\n    _get_prediction_function_gaussian(observable_sectors_gaussian) -&gt; Callable\n        Returns a prediction function for the Gaussian observable sectors.\n    _get_prediction_function_no_theory_uncertainty() -&gt; Callable\n        Returns a prediction function for observables with no theory uncertainty.\n    _get_constraints_no_theory_uncertainty(observables, observable_lists_per_likelihood=None) -&gt; Tuple[Dict, Dict, jnp.array, jnp.array, jnp.array]\n        Returns the constraints and selector matrices for observables with no theory uncertainty.\n    _get_constraints_correlated() -&gt; Tuple[Dict, Dict, List[jnp.array]]\n        Returns the constraints and selector matrices for correlated observable sectors.\n    _get_log_likelihood_point_function() -&gt; Callable\n        Returns a JIT-compiled function to compute the information needed for `GlobalLikelihoodPoint` instances.\n    _get_obstable_function() -&gt; Callable\n        Returns a JIT-compiled function to compute the observable table information.\n    _get_parameter_basis() -&gt; Tuple[Dict, Dict]\n        Determines the parameter basis and splits parameters into real and imaginary parts.\n    _get_par_array(par_dict: Dict) -&gt; jnp.array\n        Converts a parameter dictionary into a JAX array.\n    _get_reference_scale() -&gt; float\n        Determines the reference scale for the likelihood.\n\n    Examples\n    --------\n    Load all observable sectors, measurements, and correlations from the specified path:\n\n    &gt;&gt;&gt; GlobalLikelihood.load('path/to/data')\n\n    Create a global likelihood instance for the SMEFT in the Warsaw basis, including all observable sectors and measurements:\n\n    &gt;&gt;&gt; gl = GlobalLikelihood(eft='SMEFT', basis='Warsaw')\n\n    Create a global likelihood instance for a custom basis named 'my_basis', including only specific observable sectors and measurements:\n\n    &gt;&gt;&gt; gl = GlobalLikelihood(custom_basis='my_basis', include_observable_sectors=['sector1', 'sector2'], include_measurements=['measurement1', 'measurement2'])\n\n    Create a global likelihood instance for the SMEFT in the Warsaw basis, defining a custom likelihood that includes specific observables:\n\n    &gt;&gt;&gt; custom_likelihoods = {'my_likelihood': ['observable1', 'observable2']}\n    &gt;&gt;&gt; gl = GlobalLikelihood(eft='SMEFT', basis='Warsaw', custom_likelihoods=custom_likelihoods)\n\n    Define a `GlobalLikelihoodPoint` instance for specific parameter values at the scale of 1000.0 GeV:\n\n    &gt;&gt;&gt; def par_func(x, y):\n    ...     return {'lq1_1111': x, 'lq3_1111': y}\n    &gt;&gt;&gt; glp = gl.parameter_point(par_func(1e-8, 1e-8), 1000.0)\n\n    Obtain the 2D chi-squared grid for two parameters over specified ranges:\n\n    &gt;&gt;&gt; plot_data = gl.plot_data_2d(par_func, scale=1000.0, x_min=-1e-8, x_max=1e-8, y_min=-1e-8, y_max=1e-8, steps=50)\n\n    Get the negative log-likelihood function and data for specific parameters and a likelihood:\n\n    &gt;&gt;&gt; negative_log_likelihood, log_likelihood_data = gl.get_negative_log_likelihood(par_list=[('lq1_1111', 'R'), ('lq3_1111', 'R')], likelihood='global', par_dep_cov=False)\n\n    Get an instance of `CompiledLikelihood` for specific parameters and a likelihood:\n\n    &gt;&gt;&gt; compiled_likelihood = gl.get_compiled_likelihood(par_list=[('lq1_1111', 'R'), ('lq3_1111', 'R')], likelihood='global', par_dep_cov=False)\n\n    Access the parameter basis:\n\n    &gt;&gt;&gt; parameter_basis = gl.parameter_basis\n    &gt;&gt;&gt; parameter_basis_split_re_im = gl.parameter_basis_split_re_im\n\n    Access the basis mode:\n\n    &gt;&gt;&gt; basis_mode = gl.basis_mode\n\n    Access the observables included in the likelihood:\n\n    &gt;&gt;&gt; observables_gaussian = gl.observables_gaussian\n    &gt;&gt;&gt; observables_no_theory_uncertainty = gl.observables_no_theory_uncertainty\n\n    '''\n\n    def __init__(\n        self,\n        eft=None,\n        basis=None,\n        custom_basis=None,\n        include_observable_sectors=None,\n        exclude_observable_sectors=None,\n        include_measurements=None,\n        exclude_measurements=None,\n        custom_likelihoods=None,\n    ):\n        '''\n        Initialize the GlobalLikelihood instance.\n\n        Parameters\n        ----------\n        eft : str, optional\n            The EFT name (e.g., `SMEFT`, `WET`). Required if `custom_basis` is not provided.\n        basis : str, optional\n            The basis name (e.g., `Warsaw`, `JMS`). Required if `custom_basis` is not provided.\n        custom_basis : str, optional\n            The name of a custom basis defined using the `CustomBasis` class. Required if `eft` and `basis` are not provided.\n        include_observable_sectors : list[str], optional\n            A list of observable sector names to include in the likelihood. If not provided, all loaded observable sectors are included.\n        exclude_observable_sectors : list[str], optional\n            A list of observable sector names to exclude from the likelihood. If not provided, no sectors are excluded.\n        include_measurements : list[str], optional\n            A list of measurement names to include in the likelihood. If not provided, all loaded measurements are included.\n        exclude_measurements : list[str], optional\n            A list of measurement names to exclude from the likelihood. If not provided, no measurements are excluded.\n        custom_likelihoods : dict[str, list[str]], optional\n            A dictionary defining custom likelihoods. The keys are the names of the custom likelihoods, and the values are lists of observable names to include in each custom likelihood.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Initialize a global likelihood instance for the SMEFT in the Warsaw basis, including all observable sectors and measurements:\n\n        &gt;&gt;&gt; gl = GlobalLikelihood(eft='SMEFT', basis='Warsaw')\n\n        Initialize a global likelihood instance for a custom basis named 'my_basis', including only specific observable sectors and measurements:\n\n        &gt;&gt;&gt; gl = GlobalLikelihood(custom_basis='my_basis', include_observable_sectors=['sector1', 'sector2'], include_measurements=['measurement1', 'measurement2'])\n\n        Initialize a global likelihood instance for the SMEFT in the Warsaw basis, defining a custom likelihood that includes specific observables:\n\n        &gt;&gt;&gt; custom_likelihoods = {'my_likelihood': ['observable1', 'observable2']}\n        &gt;&gt;&gt; gl = GlobalLikelihood(eft='SMEFT', basis='Warsaw', custom_likelihoods=custom_likelihoods)\n        '''\n\n        if custom_basis is not None:\n            if eft is not None or basis is not None:\n                raise ValueError(\"Please provide either `custom_basis`, or both `eft` and `basis`, but not both.\")\n        elif eft is not None and basis is None or basis is not None and eft is None:\n            raise ValueError(\"Please provide the `eft` when using the `basis` and vice versa.\")\n\n\n        # define attributes from arguments\n\n        self.eft = eft\n        self.basis = basis\n        self.custom_basis = custom_basis\n\n\n        # get names of all observable sectors and the basis mode, basis parameters, and reference scale\n\n        (\n            self.observable_sectors_gaussian,\n            self.observable_sectors_no_theory_uncertainty,\n            self.basis_mode\n        ) = self._get_observable_sectors(\n            include_observable_sectors,\n            exclude_observable_sectors\n        )\n        self.observable_sectors = self.observable_sectors_gaussian + self.observable_sectors_no_theory_uncertainty\n        self.parameter_basis_split_re_im, self.parameter_basis = self._get_parameter_basis()\n        self._reference_scale = self._get_reference_scale()\n\n        # get all measurements\n        observables_all = list(chain.from_iterable(\n            ObservableSector.get(observable_sector).observable_names\n            for observable_sector in self.observable_sectors\n        ))\n        self.include_measurements = Measurement.get_measurements(\n            observables=observables_all,\n            include_measurements=include_measurements,\n            exclude_measurements=exclude_measurements,\n        )\n        self.observables_constrained = set(chain.from_iterable(\n            measurement.constrained_observables\n            for measurement in self.include_measurements.values()\n        ))\n\n        # define attributes for observable sectors with no theory uncertainty\n\n        self.observables_no_theory_uncertainty = list(chain.from_iterable(\n            ObservableSector.get(observable_sector).observable_names\n            for observable_sector in self.observable_sectors_no_theory_uncertainty\n        ))\n        self.prediction_data_no_theory_uncertainty = [\n            ObservableSector.get(observable_sector).get_prediction_data(self.eft, self.basis)\n            for observable_sector in self.observable_sectors_no_theory_uncertainty\n        ]\n        self.prediction_function_no_theory_uncertainty = self._get_prediction_function_no_theory_uncertainty()\n\n\n        # define attributes for correlated observable sectors\n\n        (\n            self.observable_sectors_correlated,\n            self.cov_coeff_th_scaled,\n            self.cov_exp_scaled,\n            self.exp_central_scaled,\n            self.std_sm_exp,\n            self.std_exp,\n        ) = self._get_observable_sectors_correlated()\n\n        self.observables_correlated = [\n            list(chain.from_iterable(\n                ObservableSector.get(observable_sector).observable_names\n                for observable_sector in observable_sectors\n            ))\n            for observable_sectors in self.observable_sectors_correlated\n        ]\n        self.prediction_data_correlated = [\n            [\n                ObservableSector.get(observable_sector).get_prediction_data(self.eft, self.basis)\n                for observable_sector in observable_sectors\n            ]\n            for observable_sectors in self.observable_sectors_correlated\n        ]\n        self.prediction_function_correlated = [\n            self._get_prediction_function_gaussian(observable_sectors)\n            for observable_sectors in self.observable_sectors_correlated\n        ]\n\n        self.observables_gaussian = list(chain.from_iterable(\n            self.observables_correlated\n            ))\n\n        self.custom_likelihoods_gaussian, self.custom_likelihoods_no_theory_uncertainty = self._get_custom_likelihoods(custom_likelihoods)\n        self._observables_per_likelihood_no_theory_uncertainty, self._observables_per_likelihood_correlated = self._get_observables_per_likelihood()\n\n        _likelihoods_no_theory_uncertainty = sorted(self._observables_per_likelihood_no_theory_uncertainty.keys())\n        _likelihoods_correlated = sorted(self._observables_per_likelihood_correlated.keys())\n        _likelihoods_custom = sorted(set(self.custom_likelihoods_gaussian.keys()) | set(self.custom_likelihoods_no_theory_uncertainty.keys()))\n        _likelihoods = _likelihoods_correlated + _likelihoods_no_theory_uncertainty + _likelihoods_custom\n\n        self._observables_per_likelihood_no_theory_uncertainty.update(self.custom_likelihoods_no_theory_uncertainty)\n        self._observables_per_likelihood_correlated.update(self.custom_likelihoods_gaussian)\n        self._likelihood_indices_no_theory_uncertainty = jnp.array([\n            _likelihoods.index(likelihood)\n            for likelihood in list(self._observables_per_likelihood_no_theory_uncertainty.keys())\n        ], dtype=int)\n        self._likelihood_indices_correlated = jnp.array([\n            _likelihoods.index(likelihood)\n            for likelihood in list(self._observables_per_likelihood_correlated.keys())\n        ], dtype=int)\n\n        # add global likelihood\n        self._likelihood_indices_global = jnp.array([\n            i for i, likelihood in enumerate(_likelihoods)\n            if likelihood not in (\n                set(self.custom_likelihoods_gaussian) | set(self.custom_likelihoods_no_theory_uncertainty)\n            )\n        ], dtype=int)\n        self.likelihoods = _likelihoods + ['global']\n\n        (\n            self.constraints_no_theory_uncertainty,\n            self.constraints_no_theory_uncertainty_no_corr,\n            self.selector_matrix_no_th_unc_univariate,\n            self.selector_matrix_no_th_unc_multivariate,\n            self._indices_mvn_not_custom,\n        ) = self._get_constraints_no_theory_uncertainty(\n            self.observables_no_theory_uncertainty,\n            list(self._observables_per_likelihood_no_theory_uncertainty.values())\n        )\n\n        (\n            self.constraints_correlated_par_indep_cov,\n            self.constraints_correlated_par_dep_cov,\n            self.selector_matrix_correlated,\n        ) = self._get_constraints_correlated()\n\n        self._log_likelihood_point_function = self._get_log_likelihood_point_function()\n        self._log_likelihood_point = partial(\n            self._log_likelihood_point_function,\n            prediction_data_no_theory_uncertainty=self.prediction_data_no_theory_uncertainty,\n            prediction_data_correlated=self.prediction_data_correlated,\n            constraints_no_theory_uncertainty=self.constraints_no_theory_uncertainty,\n            constraints_correlated_par_indep_cov=self.constraints_correlated_par_indep_cov,\n            constraints_correlated_par_dep_cov=self.constraints_correlated_par_dep_cov,\n            selector_matrix_no_th_unc_univariate=self.selector_matrix_no_th_unc_univariate,\n            selector_matrix_no_th_unc_multivariate=self.selector_matrix_no_th_unc_multivariate,\n            selector_matrix_correlated=self.selector_matrix_correlated,\n            likelihood_indices_no_theory_uncertainty=self._likelihood_indices_no_theory_uncertainty,\n            likelihood_indices_correlated=self._likelihood_indices_correlated,\n            likelihood_indices_global=self._likelihood_indices_global,\n        )\n        (\n            sm_prediction_no_theory_uncertainty,\n            sm_prediction_correlated,\n            sm_log_likelihood_no_th_unc_univariate,\n            sm_log_likelihood_no_th_unc_multivariate,\n            sm_log_likelihood_correlated,\n            self.sm_log_likelihood_summed,\n            std_sm_exp_correlated_scaled,\n        ) = self._log_likelihood_point(\n            self._get_par_array({}),\n            self._reference_scale,\n            par_dep_cov=False,\n        )\n\n        self._obstable = partial(\n            self._get_obstable_function(),\n            constraints_no_theory_uncertainty_no_corr=self.constraints_no_theory_uncertainty_no_corr,\n            indices_mvn_not_custom=self._indices_mvn_not_custom,\n            exp_central_scaled=self.exp_central_scaled,\n            std_sm_exp=self.std_sm_exp,\n        )\n        (\n            sm_log_likelihood_no_th_unc_multivariate,\n            sm_log_likelihood_no_th_unc_multivariate_no_corr,\n            self.sm_log_likelihood_correlated,\n            self.sm_log_likelihood_correlated_no_corr,\n            _,\n            _,\n        ) = self._obstable(\n            sm_prediction_no_theory_uncertainty,\n            sm_prediction_correlated,\n            sm_log_likelihood_no_th_unc_multivariate,\n            sm_log_likelihood_correlated,\n            std_sm_exp_correlated_scaled,\n        )\n        self.sm_log_likelihood_no_theory_uncertainty = sm_log_likelihood_no_th_unc_univariate + sm_log_likelihood_no_th_unc_multivariate\n        self.sm_log_likelihood_no_theory_uncertainty_no_corr = sm_log_likelihood_no_th_unc_univariate + sm_log_likelihood_no_th_unc_multivariate_no_corr\n\n        combined_constraints = Measurement.get_combined_constraints(\n            self.observables_no_theory_uncertainty\n        )\n        experimental_values = {}\n        for dist_type, dist_info in combined_constraints.items():\n            observable_indices = dist_info['observable_indices']\n            mode, uncertainty = get_mode_and_uncertainty(dist_type, dist_info)\n            experimental_values.update({\n                self.observables_no_theory_uncertainty[ind]: [mode[i], uncertainty[i]]\n                for i, ind in enumerate(observable_indices)\n            })\n        self.experimental_values_no_theory_uncertainty = experimental_values\n\n        self._cache_compiled_likelihood = {}\n\n    @classmethod\n    def load(cls, path):\n        '''\n        Initialize `ObservableSector`, `Measurement`, `TheoryCorrelations`, and `ExperimentalCorrelations` classes by loading data from the specified path.\n\n        Parameters\n        ----------\n        path : str\n            The path to the directory containing the data files.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n\n        Load all observable sectors, measurements, and correlations from the specified path:\n\n        &gt;&gt;&gt; GlobalLikelihood.load('path/to/data')\n        '''\n        # load all observable sectors\n        ObservableSector.load(path)\n        # load all measurements\n        Measurement.load(path)\n        # load all theory correlations\n        TheoryCorrelations.load(path)\n        # load all experimental correlations\n        ExperimentalCorrelations.load()\n\n    def _get_observable_sectors(self, include_observable_sectors, exclude_observable_sectors):\n        '''\n        Determines the observable sectors to include in the likelihood based on inclusion/exclusion lists.\n\n        Parameters\n        ----------\n        include_observable_sectors : list[str] or None\n            A list of observable sector names to include in the likelihood. If None, all loaded observable sectors are included.\n        exclude_observable_sectors : list[str] or None\n            A list of observable sector names to exclude from the likelihood. If None, no sectors are excluded.\n\n        Returns\n        -------\n        observable_sectors_gaussian : list[str]\n            The list of observable sector names containing observables with Gaussian theory uncertainties.\n        observable_sectors_no_theory_uncertainty : list[str]\n            The list of observable sector names containing observables with no theory uncertainty.\n        basis_mode : str\n            The basis mode, either `rgevolve`, `wcxf`, or `custom`.\n        '''\n        if include_observable_sectors is not None and exclude_observable_sectors is not None:\n            raise ValueError(\"Please provide either `include_observable_sectors` or `exclude_observable_sectors`, not both.\")\n        available_observable_sectors = set(ObservableSector.get_all_names(eft=self.eft, basis=self.basis, custom_basis=self.custom_basis))\n        if include_observable_sectors is not None:\n            if set(include_observable_sectors)-available_observable_sectors:\n                raise ValueError(f\"Observable sectors {set(include_observable_sectors)-available_observable_sectors} provided in `include_observable_sectors` but not found in loaded observable sectors\")\n            observable_sectors = sorted(\n                include_observable_sectors\n            )\n        elif exclude_observable_sectors is not None:\n            if set(exclude_observable_sectors)-available_observable_sectors:\n                raise ValueError(f\"Observable sectors {set(exclude_observable_sectors)-available_observable_sectors} provided in `exclude_observable_sectors` but not found in loaded observable sectors\")\n            observable_sectors = sorted(\n                available_observable_sectors - set(exclude_observable_sectors)\n            )\n        else:\n            observable_sectors = sorted(available_observable_sectors)\n        if observable_sectors:\n            basis_mode = ObservableSector.get(observable_sectors[0]).basis_mode\n            if basis_mode in ['wcxf', 'custom']:\n                scales = set(\n                    ObservableSector.get(observable_sector).scale\n                    for observable_sector in observable_sectors\n                )\n                if len(scales) &gt; 1:\n                    raise ValueError(\n                        f\"Observable sectors for basis {self.custom_basis or (self.eft, self.basis)} are defined at different scales. Please use `include_observable_sectors` or `exclude_observable_sectors` to select observable sectors at the same scale.\"\n                    )\n        observable_sectors_gaussian = []\n        observable_sectors_no_theory_uncertainty = []\n        for observable_sector in observable_sectors:\n            if ObservableSector.get(observable_sector).observable_uncertainties is None:\n                observable_sectors_no_theory_uncertainty.append(observable_sector)\n            else:\n                observable_sectors_gaussian.append(observable_sector)\n        return observable_sectors_gaussian, observable_sectors_no_theory_uncertainty, basis_mode\n\n    def _get_observable_sectors_correlated(self):\n        '''\n        Determines and returns useful information about correlated observable sectors.\n\n        Returns\n        -------\n        observable_sectors_correlated : list[list[str]]\n            The list of lists of observable sector names in correlated groups.\n        cov_coeff_th_scaled : list[list[list[jnp.array]]]\n            The list of lists of theory correlation coefficient matrices for each correlated group, scaled by the combined SM and experimental uncertainties.\n        cov_exp_scaled : list[jnp.array]\n            The list of experimental covariance matrices for each correlated group, scaled by the combined SM and experimental uncertainties.\n        exp_central_scaled : list[jnp.array]\n            The list of experimental central values for each correlated group, scaled by the combined SM and experimental uncertainties.\n        std_sm_exp : list[jnp.array]\n            The list of combined SM and experimental uncertainties for each correlated group.\n        std_exp_list : list[jnp.array]\n            The list of experimental uncertainties for each correlated group.\n        '''\n\n        # get correlations for all gaussian observable sectors\n\n        correlations_th =  []\n        correlations_exp =  []\n        for i, row_sector in enumerate(self.observable_sectors_gaussian):\n            row_th = []\n            row_exp = []\n            for j, col_sector in enumerate(self.observable_sectors_gaussian[:i+1]):\n                obs_row = ObservableSector.get(row_sector).observable_names\n                obs_col = ObservableSector.get(col_sector).observable_names\n                row_th.append(TheoryCorrelations.get_data(obs_row, obs_col))\n                row_exp.append(ExperimentalCorrelations.get_data('correlations', self.include_measurements, obs_row, obs_col))\n            correlations_th.append(row_th)\n            correlations_exp.append(row_exp)\n\n\n        # find connected components of the correlation graph\n\n        G = nx.Graph()\n        G.add_nodes_from(self.observable_sectors_gaussian)\n        for i, name_i in enumerate(self.observable_sectors_gaussian):\n            for j, name_j in enumerate(self.observable_sectors_gaussian[:i+1]):\n                if correlations_th[i][j] is not None or correlations_exp[i][j] is not None:\n                    G.add_edge(name_i, name_j)\n        components = list(nx.connected_components(G))\n        components = [sorted(list(group)) for group in components]\n        components = sorted(components, key=lambda c: self.observable_sectors_gaussian.index(c[0]))\n        observable_sectors_correlated = components\n\n\n        # get combined sm and exp standard deviations and scaled uncertainties for connected components\n\n        std_th_scaled = []\n        std_exp_scaled = []\n        std_sm_exp = []\n        exp_central_scaled = []\n        std_exp_list = []\n        for group in components:\n            sub_std_th_scaled = []\n            sub_std_exp_scaled = []\n            sub_std_sm_exp = []\n            sub_exp_central_scaled = []\n            sub_std_exp = []\n            for i, row_sector in enumerate(group):\n                obs_row = ObservableSector.get(row_sector).observable_names\n                std_exp = ExperimentalCorrelations.get_data('uncertainties', self.include_measurements, obs_row)\n                exp_central = ExperimentalCorrelations.get_data('central', self.include_measurements, obs_row)\n                std_th = ObservableSector.get(row_sector).observable_uncertainties\n                std_sm = ObservableSector.get(row_sector).observable_uncertainties_SM\n                _std_sm_exp = std_exp * np.sqrt(1 + (std_sm / std_exp)**2) # combined sm + exp uncertainty\n                sub_std_th_scaled.append(std_th/_std_sm_exp)\n                sub_std_exp_scaled.append(std_exp/_std_sm_exp)\n                sub_std_sm_exp.append(_std_sm_exp)\n                sub_exp_central_scaled.append(exp_central/_std_sm_exp)\n                sub_std_exp.append(std_exp)\n            std_th_scaled.append(sub_std_th_scaled)\n            std_exp_scaled.append(sub_std_exp_scaled)\n            std_sm_exp.append(jnp.array(np.concatenate(sub_std_sm_exp)))\n            exp_central_scaled.append(jnp.array(np.concatenate(sub_exp_central_scaled)))\n            std_exp_list.append(jnp.array(np.concatenate(sub_std_exp)))\n\n\n        # get scaled covariance matrices for connected components\n\n        cov_coeff_th_scaled = []\n        cov_exp_scaled = []\n        for k, group in enumerate(components):\n            sub_th = []\n            sub_exp = []\n            for i, row_sector in enumerate(group):\n                row_th = []\n                row_exp = []\n                for j, col_sector in enumerate(group[:i+1]):\n                    obs_row = ObservableSector.get(row_sector).observable_names\n                    obs_col = ObservableSector.get(col_sector).observable_names\n                    row_th.append(TheoryCorrelations.get_cov_scaled(\n                        self.include_measurements, obs_row, obs_col, std_th_scaled[k][i], std_th_scaled[k][j]\n                    ))\n                    row_exp.append(ExperimentalCorrelations.get_cov_scaled(\n                        self.include_measurements, obs_row, obs_col, std_exp_scaled[k][i], std_exp_scaled[k][j]\n                    ))\n                sub_th.append(row_th)\n                sub_exp.append(row_exp)\n            cov_coeff_th_scaled.append(sub_th)\n\n            n_sectors = len(sub_exp)\n            cov_exp = np.empty((n_sectors, n_sectors), dtype=object).tolist()\n            for i in range(n_sectors):\n                for j in range(n_sectors):\n                    if i &gt;= j:\n                        cov_exp[i][j] = sub_exp[i][j]\n                    else:\n                        shape = sub_exp[j][i].shape\n                        cov_exp[i][j] = np.zeros((shape[1], shape[0]))\n            cov_exp_tril = np.tril(np.block(cov_exp))\n            sub_exp = cov_exp_tril + cov_exp_tril.T - np.diag(np.diag(cov_exp_tril))\n            cov_exp_scaled.append(jnp.array(sub_exp))\n\n        return (\n            observable_sectors_correlated,\n            cov_coeff_th_scaled,\n            cov_exp_scaled,\n            exp_central_scaled,\n            std_sm_exp,\n            std_exp_list,\n        )\n\n    def _get_custom_likelihoods(self, custom_likelihoods):\n        '''\n        Processes custom likelihoods.\n\n        Parameters\n        ----------\n        custom_likelihoods : dict[str, list[str]] or None\n            A dictionary defining custom likelihoods. The keys are the names of the custom likelihoods, and the values are lists of observable names to include in each custom likelihood.\n\n        Returns\n        -------\n        likelihoods_gaussian : dict[str, list[str]]\n            A dictionary mapping custom likelihood names to lists of observables with Gaussian theory uncertainties.\n        likelihoods_no_theory_uncertainty : dict[str, list[str]]\n            A dictionary mapping custom likelihood names to lists of observables with no theory uncertainty.\n        '''\n        if custom_likelihoods is None:\n            return {}, {}\n        if not isinstance(custom_likelihoods, dict) or not all([isinstance(k, str) and isinstance(v, list) for k, v in custom_likelihoods.items()]):\n            raise ValueError(\"The custom_likelihoods argument should be a dictionary with string names of custom likelihoods as keys and lists of observable names as values.\")\n\n        likelihoods_gaussian = {}\n        likelihoods_no_theory_uncertainty = {}\n\n        for name, observables in custom_likelihoods.items():\n            observables_gaussian = set()\n            observables_no_theory_uncertainty = set()\n            invalid_observables = set()\n            for observable in observables:\n                if observable in self.observables_gaussian:\n                    observables_gaussian.add(observable)\n                elif observable in self.observables_no_theory_uncertainty:\n                    observables_no_theory_uncertainty.add(observable)\n                else:\n                    invalid_observables.add(observable)\n            if invalid_observables:\n                raise ValueError(\n                    f\"Custom likelihood '{name}' contains observables not found in the loaded observable sectors: {sorted(invalid_observables)}\"\n                )\n            if observables_gaussian:\n                likelihoods_gaussian[f'custom_{name}'] = sorted(observables_gaussian)\n            if observables_no_theory_uncertainty:\n                likelihoods_no_theory_uncertainty[f'custom_{name}'] = sorted(observables_no_theory_uncertainty)\n\n        return likelihoods_gaussian, likelihoods_no_theory_uncertainty\n\n    def _get_observables_per_likelihood(self):\n        '''\n        Constructs dictionaries mapping likelihood names to lists of observables for both no theory uncertainty and correlated sectors.\n\n        Returns\n        -------\n        observables_per_likelihood_no_theory_uncertainty : dict[str, list[str]]\n            A dictionary mapping likelihood names to lists of observables with no theory uncertainty.\n        observables_per_likelihood_correlated : dict[str, list[str]]\n            A dictionary mapping likelihood names to lists of observables with Gaussian theory uncertainties.\n        '''\n\n        observables_per_likelihood_no_theory_uncertainty = {\n            observable_sector: ObservableSector.get(observable_sector).observable_names\n            for observable_sector in self.observable_sectors_no_theory_uncertainty\n        }\n\n        observables_per_likelihood_correlated = {\n            tuple(observable_sectors): self.observables_correlated[i]\n            for i, observable_sectors in enumerate(self.observable_sectors_correlated)\n            }\n\n        return observables_per_likelihood_no_theory_uncertainty, observables_per_likelihood_correlated\n\n    def _get_prediction_function_gaussian(self, observable_sectors_gaussian):\n        '''\n        Returns a prediction function for the Gaussian observable sectors.\n\n        Parameters\n        ----------\n        observable_sectors_gaussian : list[str]\n            A list of observable sector names containing observables with Gaussian theory uncertainties.\n\n        Returns\n        -------\n        prediction : Callable\n            A function that takes a parameter array, scale, and prediction data, and returns the polynomial predictions and parameter monomials.\n        '''\n\n        prediction_functions = [\n            ObservableSector.get(name).prediction\n            for name in observable_sectors_gaussian\n        ]\n\n        def prediction(\n            par_array: jnp.array, scale: Union[float, int, jnp.array],\n            prediction_data: List[List[jnp.array]]\n        ) -&gt; jnp.array:\n            polynomial_predictions = [jnp.empty(0)]\n            par_monomials = []\n            for prediction_function, data in zip(prediction_functions, prediction_data):\n                polynomial_prediction, par_monomial = prediction_function(\n                    par_array, scale, data\n                )\n                polynomial_predictions.append(polynomial_prediction)\n                par_monomials.append(par_monomial)\n            polynomial_predictions = jnp.concatenate(polynomial_predictions, axis=-1)\n            return polynomial_predictions, par_monomials\n\n        return prediction\n\n    def _get_prediction_function_no_theory_uncertainty(self):\n        '''\n        Returns a prediction function for observables with no theory uncertainty.\n\n        Returns\n        -------\n        prediction : Callable\n            A function that takes a parameter array, scale, and prediction data, and returns the polynomial predictions for observables with no theory uncertainty.\n        '''\n\n        prediction_functions = [\n            ObservableSector.get(name).prediction\n            for name in self.observable_sectors_no_theory_uncertainty\n        ]\n        def prediction(\n            par_array: jnp.array, scale: Union[float, int, jnp.array],\n            prediction_data: List[List[jnp.array]]\n        ) -&gt; jnp.array:\n            polynomial_predictions = [jnp.empty(0)]\n            for prediction_function, data in zip(prediction_functions, prediction_data):\n                polynomial_predictions.append(\n                    prediction_function(par_array, scale, data)[0]\n                )\n            polynomial_predictions = jnp.concatenate(polynomial_predictions, axis=-1)\n            return polynomial_predictions\n\n\n        return prediction\n\n    def _get_constraints_no_theory_uncertainty(self, observables, observable_lists_per_likelihood=None):\n        '''\n        Returns the constraints and selector matrices for observables with no theory uncertainty.\n\n        Parameters\n        ----------\n        observables : list[str]\n            A list of observable names with no theory uncertainty.\n        observable_lists_per_likelihood : list[list[str]] or None\n            A list of lists of observable names for each likelihood.\n\n        Returns\n        -------\n        constraint_dict : dict\n            A dictionary containing the constraints for different distribution types.\n        constraint_no_corr : list or None\n            A list containing the multivariate normal distribution constraints neglecting correlations, or None if no such constraints exist.\n        selector_matrix_univariate : jnp.array\n            A selector matrix for univariate distributions, with shape `(n_likelihoods, n_observables)`.\n        selector_matrix_multivariate : jnp.array\n            A selector matrix for multivariate normal distributions, with shape `(n_likelihoods, n_distributions)`.\n        indices_mvn_not_custom : jnp.array\n            Indices of multivariate normal distributions that contribute to non-custom likelihoods.\n        '''\n\n        constraint_dict = {}\n\n        constraints = Measurement.get_constraints(\n            observables,\n            include_measurements=self.include_measurements,\n            distribution_types=[\n                'NumericalDistribution',\n                'NormalDistribution',\n                'HalfNormalDistribution',\n                'GammaDistributionPositive',\n                'MultivariateNormalDistribution',\n            ]\n        )\n\n        # numerical distribution\n        if 'NumericalDistribution' in constraints:\n            constraint_dict['NumericalDistribution'] = [\n                jnp.asarray(constraints['NumericalDistribution']['observable_indices']),\n                jnp.asarray(constraints['NumericalDistribution']['x']),\n                jnp.asarray(constraints['NumericalDistribution']['log_y']),\n            ]\n\n        # normal distribution\n        if 'NormalDistribution' in constraints:\n            constraint_dict['NormalDistribution'] = [\n                jnp.asarray(constraints['NormalDistribution']['observable_indices']),\n                jnp.asarray(constraints['NormalDistribution']['central_value']),\n                jnp.asarray(constraints['NormalDistribution']['standard_deviation']),\n            ]\n\n        # half normal distribution\n        if 'HalfNormalDistribution' in constraints:\n            constraint_dict['HalfNormalDistribution'] = [\n                jnp.asarray(constraints['HalfNormalDistribution']['observable_indices']),\n                jnp.asarray(constraints['HalfNormalDistribution']['standard_deviation']),\n            ]\n\n        # gamma distribution positive\n        if 'GammaDistributionPositive' in constraints:\n            constraint_dict['GammaDistributionPositive'] = [\n                jnp.asarray(constraints['GammaDistributionPositive']['observable_indices']),\n                jnp.asarray(constraints['GammaDistributionPositive']['a']),\n                jnp.asarray(constraints['GammaDistributionPositive']['loc']),\n                jnp.asarray(constraints['GammaDistributionPositive']['scale']),\n            ]\n\n        # MVN constraints, neglecting correlations\n        if 'MultivariateNormalDistribution' in constraints:\n            constraint_no_corr = [\n                jnp.asarray(np.concatenate(constraints['MultivariateNormalDistribution']['observable_indices'])),\n                jnp.asarray(np.concatenate(constraints['MultivariateNormalDistribution']['central_value'])),\n                jnp.asarray(np.concatenate(constraints['MultivariateNormalDistribution']['standard_deviation'])),\n            ]\n        else:\n            constraint_no_corr = None\n\n        if observable_lists_per_likelihood is not None:  # if not only correlated likelihoods\n            # selector matrix for univariate distributions\n            selector_matrix_univariate = jnp.array([\n                np.isin(observables, likelihood_observables).astype(float)\n                for likelihood_observables in observable_lists_per_likelihood\n            ])\n        else:\n            selector_matrix_univariate = jnp.zeros((0, len(observables)), dtype=float)\n\n        # multivariate normal distribution\n\n        _observable_lists_per_likelihood = observable_lists_per_likelihood or [observables]\n        # Collect all unique MVN blocks into this dict\n        unique_mvnd_blocks = {}\n\n        # For each likelihood, keep track of which MVNs it uses (by key)\n        mvnd_keys_per_likelihood = [[] for _ in _observable_lists_per_likelihood]\n\n        # Loop over all likelihood definitions\n        for i, observable_list in enumerate(_observable_lists_per_likelihood):\n\n            mvnd_block_data = Measurement.get_constraints(\n                observable_list,\n                include_measurements=self.include_measurements,\n                observables_for_indices=observables,\n                distribution_types=['MultivariateNormalDistribution'],\n            )['MultivariateNormalDistribution']\n\n            for j in range(len(mvnd_block_data['measurement_name'])):\n                mvnd_entry = {k: mvnd_block_data[k][j] for k in mvnd_block_data.keys()}\n                mvnd_key = (mvnd_entry['measurement_name'], tuple(mvnd_entry['observables']))\n                unique_mvnd_blocks[mvnd_key] = mvnd_entry\n                mvnd_keys_per_likelihood[i].append(mvnd_key)\n\n        # Final ordered list of all unique MVN blocks\n        all_mvnd_keys = list(unique_mvnd_blocks.keys())\n\n        n_likelihoods = len(mvnd_keys_per_likelihood)\n        n_contributions = len(all_mvnd_keys)\n\n        # Map MVND key to its index in all_mvnd_keys for fast lookup\n        mvnd_key_to_index = {key: i for i, key in enumerate(all_mvnd_keys)}\n\n        # Construct the logpdf input data from the unique MVNs\n        if all_mvnd_keys:\n            constraint_dict['MultivariateNormalDistribution'] = [\n                [jnp.asarray(unique_mvnd_blocks[k]['observable_indices']) for k in all_mvnd_keys],\n                [jnp.asarray(unique_mvnd_blocks[k]['central_value']) for k in all_mvnd_keys],\n                [jnp.asarray(unique_mvnd_blocks[k]['standard_deviation']) for k in all_mvnd_keys],\n                [jnp.asarray(unique_mvnd_blocks[k]['inverse_correlation']) for k in all_mvnd_keys],\n            ]\n            # Create selector matrix (n_likelihoods x n_contributions)\n            selector_matrix_multivariate = np.zeros((n_likelihoods, n_contributions))\n            for i, mvnd_keys in enumerate(mvnd_keys_per_likelihood):\n                for key in mvnd_keys:\n                    selector_matrix_multivariate[i, mvnd_key_to_index[key]] = 1.0\n            selector_matrix_multivariate = jnp.array(selector_matrix_multivariate)\n        else:\n            selector_matrix_multivariate = jnp.zeros((n_likelihoods, 1), dtype=float)\n\n        # Get indices of MVNs that contribute to non-custom likelihoods\n        n_likelihoods_not_custom = len(self.observable_sectors_no_theory_uncertainty)\n        indices_mvn_not_custom = jnp.nonzero(\n            np.sum(\n                selector_matrix_multivariate[:n_likelihoods_not_custom],\n                axis=0\n            )\n        )[0]\n\n        return (\n            constraint_dict,\n            constraint_no_corr,\n            selector_matrix_univariate,\n            selector_matrix_multivariate,\n            indices_mvn_not_custom,\n        )\n\n    def _get_constraints_correlated(self):\n        '''\n        Returns the constraints and selector matrices for correlated observable sectors.\n\n        Returns\n        -------\n        constraints_correlated_par_indep_cov : list\n            A list containing the multivariate normal distribution constraints with parameter-independent covariance matrices.\n        constraints_correlated_par_dep_cov : list\n            A list containing the constraints for correlated observable sectors with parameter-dependent covariance matrices.\n        selector_matrix : list[jnp.array]\n            A list of selector matrices for each correlated observable sector, with shape `(n_likelihoods, n_distributions)`.\n        '''\n\n        # constraints for correlated observable sectors with parameter dependent covariance matrix\n\n        n_correlated_likelihoods = len(self._observables_per_likelihood_correlated)\n        unique_indices_list = []\n        selector_matrix = []\n        for i, observables_correlated in enumerate(self.observables_correlated):\n            unique_observable_indices = []\n            mvn_to_likelihood_map = defaultdict(list)  # maps indices of observables in the set of correlated sectors (MVNs) to likelihoods\n            for j, observables_in_likelihood in enumerate(self._observables_per_likelihood_correlated.values()):\n                if (\n                    j == i  # this is the set of correlated sectors selected in the i loop\n                    or j &gt;= len(self.observables_correlated)  # these are the custom likelihoods\n                ):\n                    obs_indices = tuple(\n                        observables_correlated.index(observable)\n                        for observable in observables_in_likelihood\n                        if (\n                            observable in observables_correlated  # a custom likelihood might contain no observable from this set of correlated sectors\n                            and observable in self.observables_constrained  # only consider observables that are constrained\n                        )\n                    )\n                    if obs_indices:\n                        if obs_indices not in unique_observable_indices:\n                            unique_observable_indices.append(\n                                obs_indices\n                            )\n                        mvn_to_likelihood_map[obs_indices].append(j)\n\n            # build selector matrix of (n_correlated_likelihoods, n_mvns)\n            sel_matrix = np.zeros((n_correlated_likelihoods, len(unique_observable_indices)))\n            for col, indices in enumerate(unique_observable_indices):\n                rows = mvn_to_likelihood_map.get(indices, [])\n                sel_matrix[rows, col] = 1  # set the entry to 1 if the likelihood depends on this MVN based on the mvn_to_likelihood_map\n\n            unique_indices_list.append([jnp.array(indices, dtype=int) for indices in unique_observable_indices])\n            selector_matrix.append(sel_matrix)\n\n        constraints_correlated_par_dep_cov = [\n            self.cov_coeff_th_scaled,\n            self.std_sm_exp,\n            unique_indices_list,\n            self.exp_central_scaled,\n            self.cov_exp_scaled,\n        ]\n\n        # constraints for correlated observable sectors with parameter independent covariance matrix\n\n        mean = []\n        standard_deviation = []\n        inverse_correlation = []\n        for i, unique_indices in enumerate(unique_indices_list):\n            mean.append([])\n            standard_deviation.append([])\n            inverse_correlation.append([])\n            cov_exp_scaled = self.cov_exp_scaled[i]\n            cov_coeff_th_scaled = self.cov_coeff_th_scaled[i]\n            par_monomials = []\n            for name in self.observable_sectors_correlated[i]:\n                sector = ObservableSector.get(name)\n                par_monomial = np.zeros(len(sector.keys_coeff_observable))\n                par_monomial[0] = 1.0\n                par_monomials.append(par_monomial)\n            cov_obs_th_scaled = cov_coeff_to_cov_obs(par_monomials, cov_coeff_th_scaled)\n            corr = cov_obs_th_scaled + cov_exp_scaled  # actually correlation matrix as it is parameter independent and rescaled with its own diagonal\n            std_sm_exp = self.std_sm_exp[i]\n            for index_array in unique_indices:\n                index_list = list(index_array)\n                mean[i].append(\n                    jnp.asarray(\n                        np.take(\n                            self.exp_central_scaled[i]*std_sm_exp,\n                            index_list\n                        ),\n                        dtype=jnp.float64\n                    )\n                )\n                std = np.take(\n                    std_sm_exp,\n                    index_list\n                )\n                standard_deviation[i].append(\n                    jnp.asarray(\n                        std,\n                        dtype=jnp.float64\n                    )\n                )\n                c = np.take(\n                    np.take(corr, index_list, axis=0),\n                    index_list,\n                    axis=1\n                )\n                inverse_correlation[i].append(\n                    jnp.asarray(\n                        np.linalg.inv(c),\n                        dtype=jnp.float64\n                    )\n                )\n\n        constraints_correlated_par_indep_cov = [\n            unique_indices_list,\n            mean,\n            standard_deviation,\n            inverse_correlation,\n        ]\n\n        return constraints_correlated_par_indep_cov, constraints_correlated_par_dep_cov, selector_matrix\n\n    def get_negative_log_likelihood(\n            self,\n            par_list: List[Tuple[str, str]],\n            likelihood: Union[str, Tuple[str, ...]],\n            par_dep_cov: bool,\n        ):\n        '''\n        Get a function that computes the negative log-likelihood for a given list of parameters and likelihood, and the corresponding likelihood data\n\n        Parameters\n        ----------\n        par_list : List[Tuple[str, str]]\n            List of tuples specifying the parameters to include in the likelihood evaluation. Each entry is a tuple where the first element is the parameter name and the second element is `R` for real parameters or `I` for imaginary parameters.\n        likelihood : Union[str, Tuple[str, ...]]\n            The likelihood to evaluate. This can be a string specifying a single likelihood (e.g., 'global' for the combined likelihood, or the name of a specific likelihood), or a tuple of strings specifying a correlated set of likelihoods.\n        par_dep_cov : bool\n            Whether to use the parameter-dependent covariance matrix for correlated likelihoods.\n\n        Returns\n        -------\n        negative_log_likelihood : Callable\n            A function that computes the negative log-likelihood given an array of parameter values, a scale, and the likelihood data.\n        log_likelihood_data : List\n            A list containing the data needed for the likelihood evaluation.\n\n        Examples\n        --------\n        Get the negative log-likelihood function and data for a specific set of parameters and the global likelihood:\n        &gt;&gt;&gt; negative_log_likelihood, log_likelihood_data = global_likelihood.get_negative_log_likelihood(par_list=[('lq1_1111', 'R'), ('lq3_1111', 'R')], likelihood='global', par_dep_cov=False\n        &gt;&gt;&gt; par_array = jnp.array([1e-8, 1e-8])\n        &gt;&gt;&gt; scale = 1000.0\n        &gt;&gt;&gt; nll_value = negative_log_likelihood(par_array, scale, log_likelihood_data)\n\n        '''\n        # prepare selector matrices for included likelihoods\n        if likelihood == 'global':  # for global likelihood, select all non-custom likelihoods\n            selector_matrix_no_th_unc_univariate  = self.selector_matrix_no_th_unc_univariate[:len(self.observable_sectors_no_theory_uncertainty)]\n            selector_matrix_no_th_unc_multivariate = self.selector_matrix_no_th_unc_multivariate[:len(self.observable_sectors_no_theory_uncertainty)]\n            selector_matrix_correlated = [selector_matrix[:len(self.observable_sectors_correlated)] for selector_matrix in self.selector_matrix_correlated]\n        else:  # for a specific likelihood, select just the corresponding rows in selector matrices\n            if likelihood in self._observables_per_likelihood_no_theory_uncertainty:\n                n = list(self._observables_per_likelihood_no_theory_uncertainty).index(likelihood)\n                selector_matrix_no_th_unc_univariate = self.selector_matrix_no_th_unc_univariate[[n], :]\n                selector_matrix_no_th_unc_multivariate = self.selector_matrix_no_th_unc_multivariate[[n], :]\n            else:\n                selector_matrix_no_th_unc_univariate = None\n                selector_matrix_no_th_unc_multivariate = None\n            if likelihood in self._observables_per_likelihood_correlated:\n                n = list(self._observables_per_likelihood_correlated).index(likelihood)\n                selector_matrix_correlated = [selector_matrix[[n], :] for selector_matrix in self.selector_matrix_correlated]\n            else:\n                selector_matrix_correlated = [None for _ in self.selector_matrix_correlated]\n\n        log_likelihood_data = [\n            self.prediction_data_no_theory_uncertainty,\n            self.prediction_data_correlated,\n            self.constraints_no_theory_uncertainty,\n            self.constraints_correlated_par_indep_cov,\n            self.constraints_correlated_par_dep_cov,\n            selector_matrix_no_th_unc_univariate,\n            selector_matrix_no_th_unc_multivariate,\n            selector_matrix_correlated,\n        ]\n\n        n_parameters = len(self.parameter_basis_split_re_im)\n        par_indices = jnp.array([self.parameter_basis_split_re_im[par] for par in par_list])\n\n        def negative_log_likelihood(\n            par_array: jnp.array,\n            scale: Union[float, int, jnp.array],\n            log_likelihood_data: List,\n        ) -&gt; float:\n\n            (\n                prediction_data_no_theory_uncertainty,\n                prediction_data_correlated,\n                constraints_no_theory_uncertainty,\n                constraints_correlated_par_indep_cov,\n                constraints_correlated_par_dep_cov,\n                selector_matrix_no_th_unc_univariate,\n                selector_matrix_no_th_unc_multivariate,\n                selector_matrix_correlated,\n            ) = log_likelihood_data\n\n            par_array_full = jnp.zeros(n_parameters)\n            par_array_full = par_array_full.at[par_indices].set(par_array)\n\n            # no theory uncertainty likelihoods\n            log_likelihood_no_th_unc_summed = 0.0\n            if selector_matrix_no_th_unc_univariate is not None:\n                prediction_no_theory_uncertainty = self.prediction_function_no_theory_uncertainty(\n                    par_array_full, scale, prediction_data_no_theory_uncertainty\n                )\n                for distribution_type in constraints_no_theory_uncertainty.keys():\n                    if distribution_type == 'MultivariateNormalDistribution':\n                        selector_matrix = selector_matrix_no_th_unc_multivariate\n                    else:\n                        selector_matrix = selector_matrix_no_th_unc_univariate\n                    log_likelihood_no_th_unc_summed += jnp.sum(\n                        logL_functions_summed[distribution_type](\n                            prediction_no_theory_uncertainty,\n                            selector_matrix,\n                            *constraints_no_theory_uncertainty[distribution_type]\n                        )\n                    )\n\n            # correlated likelihoods\n            prediction_correlated = [\n                prediction_function(\n                    par_array_full, scale, prediction_data_correlated[i]\n                ) for i, prediction_function in enumerate(self.prediction_function_correlated)  # includes predictions and par_monomials\n            ]\n            n_correlated_sectors = len(selector_matrix_correlated)\n            log_likelihood_correlated_summed = 0.0\n            if par_dep_cov:\n                (cov_coeff_th_scaled,\n                 std_sm_exp,\n                 observable_indices,\n                 exp_central_scaled,\n                 cov_exp_scaled,\n                ) = constraints_correlated_par_dep_cov\n                for i in range(n_correlated_sectors):\n                    selector_matrix = selector_matrix_correlated[i]\n                    if selector_matrix is not None:\n                        predictions, par_monomials = prediction_correlated[i]\n                        cov_obs_th_scaled = cov_coeff_to_cov_obs(par_monomials, cov_coeff_th_scaled[i])\n                        log_likelihood_correlated_summed += jnp.sum(\n                            logL_correlated_sectors_summed(\n                                predictions/std_sm_exp[i],\n                                selector_matrix,\n                                observable_indices[i],\n                                exp_central_scaled[i],\n                                cov_obs_th_scaled,\n                                cov_exp_scaled[i]\n                            )\n                        )\n            else:\n                (\n                 observable_indices,\n                 mean,\n                 standard_deviation,\n                 inverse_correlation,\n                ) = constraints_correlated_par_indep_cov\n                logL_function = logL_functions_summed['MultivariateNormalDistribution']\n                for i in range(n_correlated_sectors):\n                    selector_matrix = selector_matrix_correlated[i]\n                    if selector_matrix is not None:\n                        predictions, _ = prediction_correlated[i]\n                        log_likelihood_correlated_summed += jnp.sum(\n                            logL_function(\n                                predictions,\n                                selector_matrix,\n                                observable_indices[i],\n                                mean[i],\n                                standard_deviation[i],\n                                inverse_correlation[i],\n                            )\n                        )\n            return - (log_likelihood_no_th_unc_summed + log_likelihood_correlated_summed)\n\n        return negative_log_likelihood, log_likelihood_data\n\n    def _get_log_likelihood_point_function(self):\n        '''\n        Returns a JIT-compiled function to compute the information needed for `GlobalLikelihoodPoint` instances.\n\n        Returns\n        -------\n        log_likelihood_point : Callable\n            A function that computes the predictions and log-likelihood contributions for a given parameter array, scale, and likelihood data.\n        '''\n\n        n_likelihoods = len(self.likelihoods)\n\n        def log_likelihood_point(\n            par_array: jnp.array,\n            scale: Union[float, int, jnp.array],\n            par_dep_cov: bool,\n            prediction_data_no_theory_uncertainty: jnp.array,\n            prediction_data_correlated: jnp.array,\n            constraints_no_theory_uncertainty: Dict[str,Union[List[jnp.array],List[List[jnp.array]]]],\n            constraints_correlated_par_indep_cov: Union[List[jnp.array],List[List[jnp.array]]],\n            constraints_correlated_par_dep_cov: Union[List[jnp.array],List[List[jnp.array]]],\n            selector_matrix_no_th_unc_univariate: jnp.array,\n            selector_matrix_no_th_unc_multivariate: jnp.array,\n            selector_matrix_correlated: List[jnp.array],\n            likelihood_indices_no_theory_uncertainty: jnp.array,\n            likelihood_indices_correlated: jnp.array,\n            likelihood_indices_global: jnp.array,\n        ) -&gt; Tuple[jnp.array]:\n\n            # no theory uncertainty likelihoods and predictions\n            prediction_no_theory_uncertainty = self.prediction_function_no_theory_uncertainty(\n                par_array, scale, prediction_data_no_theory_uncertainty\n            )\n            log_likelihood_no_th_unc_univariate = jnp.zeros(len(prediction_no_theory_uncertainty))\n            log_likelihood_no_th_unc_multivariate = jnp.zeros((1, len(prediction_no_theory_uncertainty)))\n            for distribution_type in constraints_no_theory_uncertainty.keys():\n                if distribution_type == 'MultivariateNormalDistribution':\n                    log_likelihood_no_th_unc_multivariate = logL_functions[distribution_type](\n                        prediction_no_theory_uncertainty,\n                        *constraints_no_theory_uncertainty[distribution_type]\n                    )\n                else:\n                    log_likelihood_no_th_unc_univariate += logL_functions[distribution_type](\n                        prediction_no_theory_uncertainty,\n                        *constraints_no_theory_uncertainty[distribution_type]\n                    )\n\n            log_likelihood_no_theory_uncertainty_summed = (\n                selector_matrix_no_th_unc_univariate @ log_likelihood_no_th_unc_univariate\n                + selector_matrix_no_th_unc_multivariate @ jnp.sum(log_likelihood_no_th_unc_multivariate, axis=1)\n            )\n\n            # correlated likelihoods and predictions\n            prediction_correlated = [\n                prediction_function(\n                    par_array, scale, prediction_data_correlated[i]\n                ) for i, prediction_function in enumerate(self.prediction_function_correlated)  # includes predictions and par_monomials\n            ]\n            n_correlated_sectors = len(prediction_correlated)\n            log_likelihood_correlated = []\n            std_th_exp_correlated_scaled = []\n            if par_dep_cov:\n                (cov_coeff_th_scaled,\n                 std_sm_exp,\n                 observable_indices,\n                 exp_central_scaled,\n                 cov_exp_scaled,\n                ) = constraints_correlated_par_dep_cov\n                for i in range(n_correlated_sectors):\n                    predictions, par_monomials = prediction_correlated[i]\n                    cov_obs_th_scaled = cov_coeff_to_cov_obs(par_monomials, cov_coeff_th_scaled[i])\n                    std_th_exp_correlated_scaled.append(jnp.sqrt(jnp.diag(cov_obs_th_scaled) + jnp.diag(cov_exp_scaled[i])))\n                    log_likelihood_correlated.append(\n                        logL_correlated_sectors(\n                            predictions/std_sm_exp[i],\n                            observable_indices[i],\n                            exp_central_scaled[i],\n                            cov_obs_th_scaled,\n                            cov_exp_scaled[i]\n                        )\n                    )\n            else:\n                (\n                 observable_indices,\n                 mean,\n                 standard_deviation,\n                 inverse_correlation,\n                ) = constraints_correlated_par_indep_cov\n                logL_function = logL_functions['MultivariateNormalDistribution']\n                for i in range(n_correlated_sectors):\n                    predictions, _ = prediction_correlated[i]\n                    std_th_exp_correlated_scaled.append(jnp.ones_like(predictions))\n                    log_likelihood_correlated.append(\n                        logL_function(\n                            predictions,\n                            observable_indices[i],\n                            mean[i],\n                            standard_deviation[i],\n                            inverse_correlation[i],\n                        )\n                    )\n\n            n_correlated_likelihoods = len(likelihood_indices_correlated)\n            log_likelihood_correlated_summed = jnp.zeros(n_correlated_likelihoods)\n            for i in range(n_correlated_sectors):\n                logL = jnp.sum(log_likelihood_correlated[i], axis=1)\n                logL = jnp.where(jnp.isnan(logL), len(log_likelihood_correlated[i])*LOG_ZERO, logL)\n                log_likelihood_correlated_summed += selector_matrix_correlated[i] @ logL\n\n            log_likelihood_summed = jnp.zeros(n_likelihoods)\n            log_likelihood_summed = log_likelihood_summed.at[likelihood_indices_no_theory_uncertainty].add(log_likelihood_no_theory_uncertainty_summed)\n            log_likelihood_summed = log_likelihood_summed.at[likelihood_indices_correlated].add(log_likelihood_correlated_summed)\n            log_likelihood_global = jnp.sum(log_likelihood_summed[likelihood_indices_global])\n            log_likelihood_summed = log_likelihood_summed.at[-1].set(log_likelihood_global)\n            return (\n                prediction_no_theory_uncertainty,\n                prediction_correlated,\n                log_likelihood_no_th_unc_univariate,\n                log_likelihood_no_th_unc_multivariate,\n                log_likelihood_correlated,\n                log_likelihood_summed,\n                std_th_exp_correlated_scaled,\n            )\n        return jit(log_likelihood_point, static_argnames=[\"par_dep_cov\"])\n\n    def _get_obstable_function(self):\n        '''\n        Returns a JIT-compiled function to compute the observable table information.\n\n        Returns\n        -------\n        obstable : Callable\n            A function that computes the log-likelihood contributions and related information for a given set of predictions and constraints.\n        '''\n\n        @jit\n        def obstable(\n            prediction_no_theory_uncertainty: jnp.array,\n            prediction_correlated: List[jnp.array],\n            log_likelihood_no_th_unc_multivariate: jnp.array,\n            log_likelihood_correlated: List[jnp.array],\n            std_th_exp_correlated_scaled: List[jnp.array],\n            constraints_no_theory_uncertainty_no_corr: List[jnp.array],\n            indices_mvn_not_custom: jnp.array,\n            exp_central_scaled: List[jnp.array],\n            std_sm_exp: List[jnp.array],\n        ) -&gt; Tuple[jnp.array]:\n\n            # no theory uncertainty sectors\n            # including correlations\n            log_likelihood_no_th_unc_multivariate = jnp.sum(\n                jnp.take(\n                    log_likelihood_no_th_unc_multivariate,\n                    indices_mvn_not_custom,\n                    axis=0\n                ),\n                axis=0\n            )\n\n            # neglecting correlations\n            if constraints_no_theory_uncertainty_no_corr is not None:\n                log_likelihood_no_th_unc_multivariate_no_corr = logL_functions['NormalDistribution'](\n                    prediction_no_theory_uncertainty,\n                    *constraints_no_theory_uncertainty_no_corr,\n                )\n            else:\n                log_likelihood_no_th_unc_multivariate_no_corr = jnp.zeros(len(prediction_no_theory_uncertainty))\n\n            # correlated sectors\n            # including correlations\n            log_likelihood_correlated = [log_likelihood[0] for log_likelihood in log_likelihood_correlated]\n\n            # neglecting correlations\n            log_likelihood_correlated_no_corr = []\n            exp_central_correlated = []\n            std_th_exp_correlated = []\n            n_correlated_sectors = len(prediction_correlated)\n            for i in range(n_correlated_sectors):\n                std_th_exp = std_th_exp_correlated_scaled[i] * std_sm_exp[i]\n                exp_central = exp_central_scaled[i] * std_sm_exp[i]\n                observable_indices = jnp.arange(len(prediction_correlated[i][0]))\n                log_likelihood_correlated_no_corr.append(\n                    logL_functions['NormalDistribution'](\n                        prediction_correlated[i][0],\n                        observable_indices,\n                        exp_central,\n                        std_th_exp\n                    )\n                )\n                exp_central_correlated.append(exp_central)\n                std_th_exp_correlated.append(std_th_exp)\n\n            return (\n                log_likelihood_no_th_unc_multivariate,\n                log_likelihood_no_th_unc_multivariate_no_corr,\n                log_likelihood_correlated,\n                log_likelihood_correlated_no_corr,\n                exp_central_correlated,\n                std_th_exp_correlated,\n            )\n        return obstable\n\n    def _get_parameter_basis(self):\n        '''\n        Determines the parameter basis and splits parameters into real and imaginary parts.\n\n        Returns\n        -------\n        parameter_basis_split_re_im : Dict[Union[str, Tuple[str, str]], int]\n            A dictionary mapping parameter names (or tuples of parameter name and 'R'/'I') to their indices in the basis with real and imaginary parts split.\n        parameter_basis : Dict[str, int]\n            A dictionary mapping parameter names to their indices in the basis without splitting real and imaginary parts.\n        '''\n        if self.basis_mode == 'rgevolve':\n            parameter_basis_split_re_im = get_wc_basis(eft=self.eft, basis=self.basis, sector=None, split_re_im=True)\n            parameter_basis = get_wc_basis(eft=self.eft, basis=self.basis, sector=None, split_re_im=False)\n        elif self.basis_mode == 'wcxf':\n            parameter_basis_split_re_im = get_wc_basis_from_wcxf(eft=self.eft, basis=self.basis, sector=None, split_re_im=True)\n            parameter_basis = get_wc_basis_from_wcxf(eft=self.eft, basis=self.basis, sector=None, split_re_im=False)\n        else:\n            custom_basis = CustomBasis.get(\n                ObservableSector.get(self.observable_sectors[0]).custom_basis\n            )\n            parameter_basis_split_re_im = custom_basis.get_parameter_basis(split_re_im=True)\n            parameter_basis = custom_basis.get_parameter_basis(split_re_im=False)\n        parameter_basis_split_re_im = {par: i for i, par in enumerate(parameter_basis_split_re_im)}\n        parameter_basis = {par: i for i, par in enumerate(parameter_basis)}\n        return parameter_basis_split_re_im, parameter_basis\n\n    def _get_par_array(self, par_dict):\n        '''\n        Converts a parameter dictionary into a JAX array.\n\n        Parameters\n        ----------\n        par_dict : dict\n            A dictionary mapping parameter names (or tuples of parameter name and 'R'/'I') to their values.\n\n        Returns\n        -------\n        jnp.array\n            A JAX array containing the parameter values in the order defined by `parameter_basis_split_re_im`.\n        '''\n        if not par_dict:\n            return jnp.zeros(len(self.parameter_basis_split_re_im))\n        elif isinstance(list(par_dict.keys())[0], tuple):\n            par_array = np.zeros(len(self.parameter_basis_split_re_im))\n            for name, value in par_dict.items():\n                if name not in self.parameter_basis_split_re_im:\n                    raise ValueError(f\"Parameter {name} not found in the parameter basis.\")\n                par_array[self.parameter_basis_split_re_im[name]] = value\n            return jnp.array(par_array)\n        else:\n            par_array = np.zeros(len(self.parameter_basis_split_re_im))\n            for name, value in par_dict.items():\n                if (name,'R') not in self.parameter_basis_split_re_im:\n                    raise ValueError(f\"Parameter {name} not found in the parameter basis.\")\n                par_array[self.parameter_basis_split_re_im[(name, 'R')]] = value.real\n                if (name, 'I') in self.parameter_basis_split_re_im:\n                    par_array[self.parameter_basis_split_re_im[(name, 'I')]] = value.imag\n            return jnp.array(par_array)\n\n    def parameter_point(self, *args, par_dep_cov: bool = False):\n        \"\"\"\n        Create a `GlobalLikelihoodPoint` instance.\n\n        Parameters\n        ----------\n        *args : tuple\n            Positional arguments. The method dispatches\n            based on the number and types of these arguments. Accepted input signatures:\n\n              1. `parameter_point(par_dict: dict, scale: Union[float, int], *, par_dep_cov: bool = False)`\n                - Create a `GlobalLikelihoodPoint` from a dictionary of parameters and a scale.\n\n              2. `parameter_point(w: wilson.Wilson, *, par_dep_cov: bool = False)`\n                - Create a `GlobalLikelihoodPoint` from a `wilson.Wilson` object.\n\n              3. `parameter_point(wc: wilson.wcxf.WC, *, par_dep_cov: bool = False)`\n                - Create a `GlobalLikelihoodPoint` from a `wilson.wcxf.WC` object.\n\n              4. `parameter_point(filename: str, *, par_dep_cov: bool = False)`\n                - Create a `GlobalLikelihoodPoint` from the path to a WCxf file.\n\n        par_dep_cov : bool, optional\n            If `True`, use the parameter dependent covariance matrix for the likelihood point.\n            Default is `False`.\n\n        Returns\n        -------\n        GlobalLikelihoodPoint\n            An instance of GlobalLikelihoodPoint with the specified parameters.\n        \"\"\"\n\n        if len(args) == 2:\n            par_dict, scale = args\n            if not isinstance(par_dict, dict) or not isinstance(scale, (float, int)):\n                raise ValueError(\n                    \"Invalid types of the two positional arguments. Expected a dictionary and scale.\"\n                )\n        elif len(args) == 1:\n            arg = args[0]\n            if isinstance(arg, Wilson):\n                par_dict = arg.wc.dict\n                scale = arg.wc.scale\n            elif isinstance(arg, wcxf.WC):\n                par_dict = arg.dict\n                scale = arg.scale\n            elif isinstance(arg, str):\n                with open(arg, 'r') as f:\n                    wc = wcxf.WC.load(f)\n                par_dict = wc.dict\n                scale = wc.scale\n            else:\n                raise ValueError(\n                    \"Invalid type of the positional argument. Expected a Wilson or wcxf.WC object, or a filename.\"\n                )\n        else:\n            raise ValueError(\"Invalid number of positional arguments. Expected either two (a dictionary and scale) or one (a Wilson or wcxf.WC object, or a filename).\")\n        return GlobalLikelihoodPoint(self, self._get_par_array(par_dict), scale, par_dep_cov=par_dep_cov)\n\n    def get_compiled_likelihood(\n        self,\n        par_list: List[Tuple[str, str]],\n        likelihood: Union[str, Tuple[str, ...]],\n        par_dep_cov: bool = False,\n    ):\n        '''\n        Returns an instance of `CompiledLikelihood` for the specified parameters and likelihood.\n\n        Parameters\n        ----------\n        par_list : List[Tuple[str, str]]\n            List of tuples specifying the parameters to include in the likelihood evaluation. Each entry is a tuple where the first element is the parameter name and the second element is `R` for real parameters or `I` for imaginary parameters.\n        likelihood : Union[str, Tuple[str, ...]]\n            The likelihood to evaluate. This can be a string specifying a single likelihood (e.g., 'global' for the combined likelihood, or the name of a specific likelihood), or a tuple of strings specifying a correlated set of likelihoods.\n        par_dep_cov : bool, optional\n            Whether to use the parameter-dependent covariance matrix for correlated likelihoods. Default is `False`.\n\n        Returns\n        -------\n        CompiledLikelihood\n            An instance of `CompiledLikelihood` containing jitted functions for likelihood evaluation.\n\n        Examples\n        --------\n        Get a `CompiledLikelihood` instance for a specific set of parameters and the global likelihood:\n        &gt;&gt;&gt; compiled_likelihood = global_likelihood.get_compiled_likelihood(par_list=[('lq1_1111', 'R'), ('lq3_1111', 'R')], likelihood='global', par_dep_cov=False)\n        '''\n        if (tuple(par_list), likelihood, par_dep_cov) not in self._cache_compiled_likelihood:\n            compiled_likelihood = CompiledLikelihood(\n                self,\n                par_list,\n                likelihood,\n                par_dep_cov,\n            )\n            self._cache_compiled_likelihood[(tuple(par_list), likelihood, par_dep_cov)] = compiled_likelihood\n        return self._cache_compiled_likelihood[(tuple(par_list), likelihood, par_dep_cov)]\n\n    def _get_reference_scale(self):\n        '''\n        Determines the reference scale for the likelihood.\n\n        Returns\n        -------\n        float\n            The reference scale for the likelihood.\n        '''\n        if self.basis_mode == 'rgevolve':\n            return float(reference_scale[self.eft])\n        else:\n            return ObservableSector.get(self.observable_sectors[0]).scale\n\n    def plot_data_2d(self, par_fct, scale, x_min, x_max, y_min, y_max, x_log=False, y_log=False, steps=20, par_dep_cov=False):\n        '''\n        Computes a grid of chi-squared values over a 2D parameter space for plotting. Returns a dictionary containing the parameter grid and the corresponding chi-squared values.\n\n        Parameters\n        ----------\n        par_fct : Callable\n            A function that takes two arguments (x, y) and returns a dictionary of parameters.\n        scale : Union[float, int, Callable]\n            The scale at which to evaluate the parameters. This can be a fixed float or int, or a callable that takes (x, y) and returns a scale.\n        x_min : float\n            The minimum value of the x-axis parameter (in log10 if x_log is `True`).\n        x_max : float\n            The maximum value of the x-axis parameter (in log10 if x_log is `True`).\n        y_min : float\n            The minimum value of the y-axis parameter (in log10 if y_log is `True`).\n        y_max : float\n            The maximum value of the y-axis parameter (in log10 if y_log is `True`).\n        x_log : bool, optional\n            Whether to use a logarithmic scale for the x-axis. Default is `False`.\n        y_log : bool, optional\n            Whether to use a logarithmic scale for the y-axis. Default is `False`.\n        steps : int, optional\n            The number of steps in each dimension for the grid. Default is `20`.\n        par_dep_cov : bool, optional\n            Whether to use the parameter-dependent covariance matrix for correlated likelihoods. Default is `False`.\n\n        Returns\n        -------\n        plotdata : Dict\n            A dictionary containing the parameter grid and the corresponding chi-squared values for each likelihood. The keys are the names of the likelihoods, and the values are dictionaries with keys `x`, `y`, and `z`, where `x` and `y` are the parameter grids and `z` is the chi-squared grid.\n\n        Examples\n        --------\n        Define a function that maps (x, y) to a dictionary of parameters:\n        &gt;&gt;&gt; def par_func(x, y):\n        ...     return {'lq1_1111': x, 'lq3_1111': y}\n\n        Obtain the 2D chi-squared grid for two parameters over specified ranges:\n\n        &gt;&gt;&gt; plot_data = gl.plot_data_2d(par_func, scale=1000.0, x_min=-1e-8, x_max=1e-8, y_min=-1e-8, y_max=1e-8, steps=50)\n        '''\n        if x_log:\n            _x = jnp.logspace(x_min, x_max, steps)\n        else:\n            _x = jnp.linspace(x_min, x_max, steps)\n        if y_log:\n            _y = jnp.logspace(y_min, y_max, steps)\n        else:\n            _y = jnp.linspace(y_min, y_max, steps)\n        x, y = jnp.meshgrid(_x, _y)\n        xy = jnp.array([x, y]).reshape(2, steps**2).T\n        xy_enumerated = list(enumerate(xy))\n        if isinstance(scale, Number):\n            scale_fct = partial(_scale_fct_fixed, scale=scale)\n        else:\n            scale_fct = scale\n        ll = partial(_log_likelihood_2d, gl=self, par_fct=par_fct, scale_fct=scale_fct, par_dep_cov=par_dep_cov)\n        ll_dict_list_enumerated = map(ll, xy_enumerated)  # no multiprocessing for now\n        ll_dict_list = [\n            ll_dict[1] for ll_dict in\n            sorted(ll_dict_list_enumerated, key=itemgetter(0))\n        ]\n        plotdata = {}\n        keys = ll_dict_list[0].keys()  # look at first dict to fix keys\n        for k in keys:\n            z = -2 * np.array([ll_dict[k] for ll_dict in ll_dict_list]).reshape((steps, steps))\n            plotdata[k] = {'x': x, 'y': y, 'z': z}\n        return plotdata\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood.__init__","title":"<code>__init__(eft=None, basis=None, custom_basis=None, include_observable_sectors=None, exclude_observable_sectors=None, include_measurements=None, exclude_measurements=None, custom_likelihoods=None)</code>","text":"<p>Initialize the GlobalLikelihood instance.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The EFT name (e.g., <code>SMEFT</code>, <code>WET</code>). Required if <code>custom_basis</code> is not provided.</p> <code>None</code> <code>basis</code> <code>str</code> <p>The basis name (e.g., <code>Warsaw</code>, <code>JMS</code>). Required if <code>custom_basis</code> is not provided.</p> <code>None</code> <code>custom_basis</code> <code>str</code> <p>The name of a custom basis defined using the <code>CustomBasis</code> class. Required if <code>eft</code> and <code>basis</code> are not provided.</p> <code>None</code> <code>include_observable_sectors</code> <code>list[str]</code> <p>A list of observable sector names to include in the likelihood. If not provided, all loaded observable sectors are included.</p> <code>None</code> <code>exclude_observable_sectors</code> <code>list[str]</code> <p>A list of observable sector names to exclude from the likelihood. If not provided, no sectors are excluded.</p> <code>None</code> <code>include_measurements</code> <code>list[str]</code> <p>A list of measurement names to include in the likelihood. If not provided, all loaded measurements are included.</p> <code>None</code> <code>exclude_measurements</code> <code>list[str]</code> <p>A list of measurement names to exclude from the likelihood. If not provided, no measurements are excluded.</p> <code>None</code> <code>custom_likelihoods</code> <code>dict[str, list[str]]</code> <p>A dictionary defining custom likelihoods. The keys are the names of the custom likelihoods, and the values are lists of observable names to include in each custom likelihood.</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Initialize a global likelihood instance for the SMEFT in the Warsaw basis, including all observable sectors and measurements:</p> <pre><code>&gt;&gt;&gt; gl = GlobalLikelihood(eft='SMEFT', basis='Warsaw')\n</code></pre> <p>Initialize a global likelihood instance for a custom basis named 'my_basis', including only specific observable sectors and measurements:</p> <pre><code>&gt;&gt;&gt; gl = GlobalLikelihood(custom_basis='my_basis', include_observable_sectors=['sector1', 'sector2'], include_measurements=['measurement1', 'measurement2'])\n</code></pre> <p>Initialize a global likelihood instance for the SMEFT in the Warsaw basis, defining a custom likelihood that includes specific observables:</p> <pre><code>&gt;&gt;&gt; custom_likelihoods = {'my_likelihood': ['observable1', 'observable2']}\n&gt;&gt;&gt; gl = GlobalLikelihood(eft='SMEFT', basis='Warsaw', custom_likelihoods=custom_likelihoods)\n</code></pre> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def __init__(\n    self,\n    eft=None,\n    basis=None,\n    custom_basis=None,\n    include_observable_sectors=None,\n    exclude_observable_sectors=None,\n    include_measurements=None,\n    exclude_measurements=None,\n    custom_likelihoods=None,\n):\n    '''\n    Initialize the GlobalLikelihood instance.\n\n    Parameters\n    ----------\n    eft : str, optional\n        The EFT name (e.g., `SMEFT`, `WET`). Required if `custom_basis` is not provided.\n    basis : str, optional\n        The basis name (e.g., `Warsaw`, `JMS`). Required if `custom_basis` is not provided.\n    custom_basis : str, optional\n        The name of a custom basis defined using the `CustomBasis` class. Required if `eft` and `basis` are not provided.\n    include_observable_sectors : list[str], optional\n        A list of observable sector names to include in the likelihood. If not provided, all loaded observable sectors are included.\n    exclude_observable_sectors : list[str], optional\n        A list of observable sector names to exclude from the likelihood. If not provided, no sectors are excluded.\n    include_measurements : list[str], optional\n        A list of measurement names to include in the likelihood. If not provided, all loaded measurements are included.\n    exclude_measurements : list[str], optional\n        A list of measurement names to exclude from the likelihood. If not provided, no measurements are excluded.\n    custom_likelihoods : dict[str, list[str]], optional\n        A dictionary defining custom likelihoods. The keys are the names of the custom likelihoods, and the values are lists of observable names to include in each custom likelihood.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Initialize a global likelihood instance for the SMEFT in the Warsaw basis, including all observable sectors and measurements:\n\n    &gt;&gt;&gt; gl = GlobalLikelihood(eft='SMEFT', basis='Warsaw')\n\n    Initialize a global likelihood instance for a custom basis named 'my_basis', including only specific observable sectors and measurements:\n\n    &gt;&gt;&gt; gl = GlobalLikelihood(custom_basis='my_basis', include_observable_sectors=['sector1', 'sector2'], include_measurements=['measurement1', 'measurement2'])\n\n    Initialize a global likelihood instance for the SMEFT in the Warsaw basis, defining a custom likelihood that includes specific observables:\n\n    &gt;&gt;&gt; custom_likelihoods = {'my_likelihood': ['observable1', 'observable2']}\n    &gt;&gt;&gt; gl = GlobalLikelihood(eft='SMEFT', basis='Warsaw', custom_likelihoods=custom_likelihoods)\n    '''\n\n    if custom_basis is not None:\n        if eft is not None or basis is not None:\n            raise ValueError(\"Please provide either `custom_basis`, or both `eft` and `basis`, but not both.\")\n    elif eft is not None and basis is None or basis is not None and eft is None:\n        raise ValueError(\"Please provide the `eft` when using the `basis` and vice versa.\")\n\n\n    # define attributes from arguments\n\n    self.eft = eft\n    self.basis = basis\n    self.custom_basis = custom_basis\n\n\n    # get names of all observable sectors and the basis mode, basis parameters, and reference scale\n\n    (\n        self.observable_sectors_gaussian,\n        self.observable_sectors_no_theory_uncertainty,\n        self.basis_mode\n    ) = self._get_observable_sectors(\n        include_observable_sectors,\n        exclude_observable_sectors\n    )\n    self.observable_sectors = self.observable_sectors_gaussian + self.observable_sectors_no_theory_uncertainty\n    self.parameter_basis_split_re_im, self.parameter_basis = self._get_parameter_basis()\n    self._reference_scale = self._get_reference_scale()\n\n    # get all measurements\n    observables_all = list(chain.from_iterable(\n        ObservableSector.get(observable_sector).observable_names\n        for observable_sector in self.observable_sectors\n    ))\n    self.include_measurements = Measurement.get_measurements(\n        observables=observables_all,\n        include_measurements=include_measurements,\n        exclude_measurements=exclude_measurements,\n    )\n    self.observables_constrained = set(chain.from_iterable(\n        measurement.constrained_observables\n        for measurement in self.include_measurements.values()\n    ))\n\n    # define attributes for observable sectors with no theory uncertainty\n\n    self.observables_no_theory_uncertainty = list(chain.from_iterable(\n        ObservableSector.get(observable_sector).observable_names\n        for observable_sector in self.observable_sectors_no_theory_uncertainty\n    ))\n    self.prediction_data_no_theory_uncertainty = [\n        ObservableSector.get(observable_sector).get_prediction_data(self.eft, self.basis)\n        for observable_sector in self.observable_sectors_no_theory_uncertainty\n    ]\n    self.prediction_function_no_theory_uncertainty = self._get_prediction_function_no_theory_uncertainty()\n\n\n    # define attributes for correlated observable sectors\n\n    (\n        self.observable_sectors_correlated,\n        self.cov_coeff_th_scaled,\n        self.cov_exp_scaled,\n        self.exp_central_scaled,\n        self.std_sm_exp,\n        self.std_exp,\n    ) = self._get_observable_sectors_correlated()\n\n    self.observables_correlated = [\n        list(chain.from_iterable(\n            ObservableSector.get(observable_sector).observable_names\n            for observable_sector in observable_sectors\n        ))\n        for observable_sectors in self.observable_sectors_correlated\n    ]\n    self.prediction_data_correlated = [\n        [\n            ObservableSector.get(observable_sector).get_prediction_data(self.eft, self.basis)\n            for observable_sector in observable_sectors\n        ]\n        for observable_sectors in self.observable_sectors_correlated\n    ]\n    self.prediction_function_correlated = [\n        self._get_prediction_function_gaussian(observable_sectors)\n        for observable_sectors in self.observable_sectors_correlated\n    ]\n\n    self.observables_gaussian = list(chain.from_iterable(\n        self.observables_correlated\n        ))\n\n    self.custom_likelihoods_gaussian, self.custom_likelihoods_no_theory_uncertainty = self._get_custom_likelihoods(custom_likelihoods)\n    self._observables_per_likelihood_no_theory_uncertainty, self._observables_per_likelihood_correlated = self._get_observables_per_likelihood()\n\n    _likelihoods_no_theory_uncertainty = sorted(self._observables_per_likelihood_no_theory_uncertainty.keys())\n    _likelihoods_correlated = sorted(self._observables_per_likelihood_correlated.keys())\n    _likelihoods_custom = sorted(set(self.custom_likelihoods_gaussian.keys()) | set(self.custom_likelihoods_no_theory_uncertainty.keys()))\n    _likelihoods = _likelihoods_correlated + _likelihoods_no_theory_uncertainty + _likelihoods_custom\n\n    self._observables_per_likelihood_no_theory_uncertainty.update(self.custom_likelihoods_no_theory_uncertainty)\n    self._observables_per_likelihood_correlated.update(self.custom_likelihoods_gaussian)\n    self._likelihood_indices_no_theory_uncertainty = jnp.array([\n        _likelihoods.index(likelihood)\n        for likelihood in list(self._observables_per_likelihood_no_theory_uncertainty.keys())\n    ], dtype=int)\n    self._likelihood_indices_correlated = jnp.array([\n        _likelihoods.index(likelihood)\n        for likelihood in list(self._observables_per_likelihood_correlated.keys())\n    ], dtype=int)\n\n    # add global likelihood\n    self._likelihood_indices_global = jnp.array([\n        i for i, likelihood in enumerate(_likelihoods)\n        if likelihood not in (\n            set(self.custom_likelihoods_gaussian) | set(self.custom_likelihoods_no_theory_uncertainty)\n        )\n    ], dtype=int)\n    self.likelihoods = _likelihoods + ['global']\n\n    (\n        self.constraints_no_theory_uncertainty,\n        self.constraints_no_theory_uncertainty_no_corr,\n        self.selector_matrix_no_th_unc_univariate,\n        self.selector_matrix_no_th_unc_multivariate,\n        self._indices_mvn_not_custom,\n    ) = self._get_constraints_no_theory_uncertainty(\n        self.observables_no_theory_uncertainty,\n        list(self._observables_per_likelihood_no_theory_uncertainty.values())\n    )\n\n    (\n        self.constraints_correlated_par_indep_cov,\n        self.constraints_correlated_par_dep_cov,\n        self.selector_matrix_correlated,\n    ) = self._get_constraints_correlated()\n\n    self._log_likelihood_point_function = self._get_log_likelihood_point_function()\n    self._log_likelihood_point = partial(\n        self._log_likelihood_point_function,\n        prediction_data_no_theory_uncertainty=self.prediction_data_no_theory_uncertainty,\n        prediction_data_correlated=self.prediction_data_correlated,\n        constraints_no_theory_uncertainty=self.constraints_no_theory_uncertainty,\n        constraints_correlated_par_indep_cov=self.constraints_correlated_par_indep_cov,\n        constraints_correlated_par_dep_cov=self.constraints_correlated_par_dep_cov,\n        selector_matrix_no_th_unc_univariate=self.selector_matrix_no_th_unc_univariate,\n        selector_matrix_no_th_unc_multivariate=self.selector_matrix_no_th_unc_multivariate,\n        selector_matrix_correlated=self.selector_matrix_correlated,\n        likelihood_indices_no_theory_uncertainty=self._likelihood_indices_no_theory_uncertainty,\n        likelihood_indices_correlated=self._likelihood_indices_correlated,\n        likelihood_indices_global=self._likelihood_indices_global,\n    )\n    (\n        sm_prediction_no_theory_uncertainty,\n        sm_prediction_correlated,\n        sm_log_likelihood_no_th_unc_univariate,\n        sm_log_likelihood_no_th_unc_multivariate,\n        sm_log_likelihood_correlated,\n        self.sm_log_likelihood_summed,\n        std_sm_exp_correlated_scaled,\n    ) = self._log_likelihood_point(\n        self._get_par_array({}),\n        self._reference_scale,\n        par_dep_cov=False,\n    )\n\n    self._obstable = partial(\n        self._get_obstable_function(),\n        constraints_no_theory_uncertainty_no_corr=self.constraints_no_theory_uncertainty_no_corr,\n        indices_mvn_not_custom=self._indices_mvn_not_custom,\n        exp_central_scaled=self.exp_central_scaled,\n        std_sm_exp=self.std_sm_exp,\n    )\n    (\n        sm_log_likelihood_no_th_unc_multivariate,\n        sm_log_likelihood_no_th_unc_multivariate_no_corr,\n        self.sm_log_likelihood_correlated,\n        self.sm_log_likelihood_correlated_no_corr,\n        _,\n        _,\n    ) = self._obstable(\n        sm_prediction_no_theory_uncertainty,\n        sm_prediction_correlated,\n        sm_log_likelihood_no_th_unc_multivariate,\n        sm_log_likelihood_correlated,\n        std_sm_exp_correlated_scaled,\n    )\n    self.sm_log_likelihood_no_theory_uncertainty = sm_log_likelihood_no_th_unc_univariate + sm_log_likelihood_no_th_unc_multivariate\n    self.sm_log_likelihood_no_theory_uncertainty_no_corr = sm_log_likelihood_no_th_unc_univariate + sm_log_likelihood_no_th_unc_multivariate_no_corr\n\n    combined_constraints = Measurement.get_combined_constraints(\n        self.observables_no_theory_uncertainty\n    )\n    experimental_values = {}\n    for dist_type, dist_info in combined_constraints.items():\n        observable_indices = dist_info['observable_indices']\n        mode, uncertainty = get_mode_and_uncertainty(dist_type, dist_info)\n        experimental_values.update({\n            self.observables_no_theory_uncertainty[ind]: [mode[i], uncertainty[i]]\n            for i, ind in enumerate(observable_indices)\n        })\n    self.experimental_values_no_theory_uncertainty = experimental_values\n\n    self._cache_compiled_likelihood = {}\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_constraints_correlated","title":"<code>_get_constraints_correlated()</code>","text":"<p>Returns the constraints and selector matrices for correlated observable sectors.</p> <p>Returns:</p> Name Type Description <code>constraints_correlated_par_indep_cov</code> <code>list</code> <p>A list containing the multivariate normal distribution constraints with parameter-independent covariance matrices.</p> <code>constraints_correlated_par_dep_cov</code> <code>list</code> <p>A list containing the constraints for correlated observable sectors with parameter-dependent covariance matrices.</p> <code>selector_matrix</code> <code>list[array]</code> <p>A list of selector matrices for each correlated observable sector, with shape <code>(n_likelihoods, n_distributions)</code>.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_constraints_correlated(self):\n    '''\n    Returns the constraints and selector matrices for correlated observable sectors.\n\n    Returns\n    -------\n    constraints_correlated_par_indep_cov : list\n        A list containing the multivariate normal distribution constraints with parameter-independent covariance matrices.\n    constraints_correlated_par_dep_cov : list\n        A list containing the constraints for correlated observable sectors with parameter-dependent covariance matrices.\n    selector_matrix : list[jnp.array]\n        A list of selector matrices for each correlated observable sector, with shape `(n_likelihoods, n_distributions)`.\n    '''\n\n    # constraints for correlated observable sectors with parameter dependent covariance matrix\n\n    n_correlated_likelihoods = len(self._observables_per_likelihood_correlated)\n    unique_indices_list = []\n    selector_matrix = []\n    for i, observables_correlated in enumerate(self.observables_correlated):\n        unique_observable_indices = []\n        mvn_to_likelihood_map = defaultdict(list)  # maps indices of observables in the set of correlated sectors (MVNs) to likelihoods\n        for j, observables_in_likelihood in enumerate(self._observables_per_likelihood_correlated.values()):\n            if (\n                j == i  # this is the set of correlated sectors selected in the i loop\n                or j &gt;= len(self.observables_correlated)  # these are the custom likelihoods\n            ):\n                obs_indices = tuple(\n                    observables_correlated.index(observable)\n                    for observable in observables_in_likelihood\n                    if (\n                        observable in observables_correlated  # a custom likelihood might contain no observable from this set of correlated sectors\n                        and observable in self.observables_constrained  # only consider observables that are constrained\n                    )\n                )\n                if obs_indices:\n                    if obs_indices not in unique_observable_indices:\n                        unique_observable_indices.append(\n                            obs_indices\n                        )\n                    mvn_to_likelihood_map[obs_indices].append(j)\n\n        # build selector matrix of (n_correlated_likelihoods, n_mvns)\n        sel_matrix = np.zeros((n_correlated_likelihoods, len(unique_observable_indices)))\n        for col, indices in enumerate(unique_observable_indices):\n            rows = mvn_to_likelihood_map.get(indices, [])\n            sel_matrix[rows, col] = 1  # set the entry to 1 if the likelihood depends on this MVN based on the mvn_to_likelihood_map\n\n        unique_indices_list.append([jnp.array(indices, dtype=int) for indices in unique_observable_indices])\n        selector_matrix.append(sel_matrix)\n\n    constraints_correlated_par_dep_cov = [\n        self.cov_coeff_th_scaled,\n        self.std_sm_exp,\n        unique_indices_list,\n        self.exp_central_scaled,\n        self.cov_exp_scaled,\n    ]\n\n    # constraints for correlated observable sectors with parameter independent covariance matrix\n\n    mean = []\n    standard_deviation = []\n    inverse_correlation = []\n    for i, unique_indices in enumerate(unique_indices_list):\n        mean.append([])\n        standard_deviation.append([])\n        inverse_correlation.append([])\n        cov_exp_scaled = self.cov_exp_scaled[i]\n        cov_coeff_th_scaled = self.cov_coeff_th_scaled[i]\n        par_monomials = []\n        for name in self.observable_sectors_correlated[i]:\n            sector = ObservableSector.get(name)\n            par_monomial = np.zeros(len(sector.keys_coeff_observable))\n            par_monomial[0] = 1.0\n            par_monomials.append(par_monomial)\n        cov_obs_th_scaled = cov_coeff_to_cov_obs(par_monomials, cov_coeff_th_scaled)\n        corr = cov_obs_th_scaled + cov_exp_scaled  # actually correlation matrix as it is parameter independent and rescaled with its own diagonal\n        std_sm_exp = self.std_sm_exp[i]\n        for index_array in unique_indices:\n            index_list = list(index_array)\n            mean[i].append(\n                jnp.asarray(\n                    np.take(\n                        self.exp_central_scaled[i]*std_sm_exp,\n                        index_list\n                    ),\n                    dtype=jnp.float64\n                )\n            )\n            std = np.take(\n                std_sm_exp,\n                index_list\n            )\n            standard_deviation[i].append(\n                jnp.asarray(\n                    std,\n                    dtype=jnp.float64\n                )\n            )\n            c = np.take(\n                np.take(corr, index_list, axis=0),\n                index_list,\n                axis=1\n            )\n            inverse_correlation[i].append(\n                jnp.asarray(\n                    np.linalg.inv(c),\n                    dtype=jnp.float64\n                )\n            )\n\n    constraints_correlated_par_indep_cov = [\n        unique_indices_list,\n        mean,\n        standard_deviation,\n        inverse_correlation,\n    ]\n\n    return constraints_correlated_par_indep_cov, constraints_correlated_par_dep_cov, selector_matrix\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_constraints_no_theory_uncertainty","title":"<code>_get_constraints_no_theory_uncertainty(observables, observable_lists_per_likelihood=None)</code>","text":"<p>Returns the constraints and selector matrices for observables with no theory uncertainty.</p> <p>Parameters:</p> Name Type Description Default <code>observables</code> <code>list[str]</code> <p>A list of observable names with no theory uncertainty.</p> required <code>observable_lists_per_likelihood</code> <code>list[list[str]] or None</code> <p>A list of lists of observable names for each likelihood.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>constraint_dict</code> <code>dict</code> <p>A dictionary containing the constraints for different distribution types.</p> <code>constraint_no_corr</code> <code>list or None</code> <p>A list containing the multivariate normal distribution constraints neglecting correlations, or None if no such constraints exist.</p> <code>selector_matrix_univariate</code> <code>array</code> <p>A selector matrix for univariate distributions, with shape <code>(n_likelihoods, n_observables)</code>.</p> <code>selector_matrix_multivariate</code> <code>array</code> <p>A selector matrix for multivariate normal distributions, with shape <code>(n_likelihoods, n_distributions)</code>.</p> <code>indices_mvn_not_custom</code> <code>array</code> <p>Indices of multivariate normal distributions that contribute to non-custom likelihoods.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_constraints_no_theory_uncertainty(self, observables, observable_lists_per_likelihood=None):\n    '''\n    Returns the constraints and selector matrices for observables with no theory uncertainty.\n\n    Parameters\n    ----------\n    observables : list[str]\n        A list of observable names with no theory uncertainty.\n    observable_lists_per_likelihood : list[list[str]] or None\n        A list of lists of observable names for each likelihood.\n\n    Returns\n    -------\n    constraint_dict : dict\n        A dictionary containing the constraints for different distribution types.\n    constraint_no_corr : list or None\n        A list containing the multivariate normal distribution constraints neglecting correlations, or None if no such constraints exist.\n    selector_matrix_univariate : jnp.array\n        A selector matrix for univariate distributions, with shape `(n_likelihoods, n_observables)`.\n    selector_matrix_multivariate : jnp.array\n        A selector matrix for multivariate normal distributions, with shape `(n_likelihoods, n_distributions)`.\n    indices_mvn_not_custom : jnp.array\n        Indices of multivariate normal distributions that contribute to non-custom likelihoods.\n    '''\n\n    constraint_dict = {}\n\n    constraints = Measurement.get_constraints(\n        observables,\n        include_measurements=self.include_measurements,\n        distribution_types=[\n            'NumericalDistribution',\n            'NormalDistribution',\n            'HalfNormalDistribution',\n            'GammaDistributionPositive',\n            'MultivariateNormalDistribution',\n        ]\n    )\n\n    # numerical distribution\n    if 'NumericalDistribution' in constraints:\n        constraint_dict['NumericalDistribution'] = [\n            jnp.asarray(constraints['NumericalDistribution']['observable_indices']),\n            jnp.asarray(constraints['NumericalDistribution']['x']),\n            jnp.asarray(constraints['NumericalDistribution']['log_y']),\n        ]\n\n    # normal distribution\n    if 'NormalDistribution' in constraints:\n        constraint_dict['NormalDistribution'] = [\n            jnp.asarray(constraints['NormalDistribution']['observable_indices']),\n            jnp.asarray(constraints['NormalDistribution']['central_value']),\n            jnp.asarray(constraints['NormalDistribution']['standard_deviation']),\n        ]\n\n    # half normal distribution\n    if 'HalfNormalDistribution' in constraints:\n        constraint_dict['HalfNormalDistribution'] = [\n            jnp.asarray(constraints['HalfNormalDistribution']['observable_indices']),\n            jnp.asarray(constraints['HalfNormalDistribution']['standard_deviation']),\n        ]\n\n    # gamma distribution positive\n    if 'GammaDistributionPositive' in constraints:\n        constraint_dict['GammaDistributionPositive'] = [\n            jnp.asarray(constraints['GammaDistributionPositive']['observable_indices']),\n            jnp.asarray(constraints['GammaDistributionPositive']['a']),\n            jnp.asarray(constraints['GammaDistributionPositive']['loc']),\n            jnp.asarray(constraints['GammaDistributionPositive']['scale']),\n        ]\n\n    # MVN constraints, neglecting correlations\n    if 'MultivariateNormalDistribution' in constraints:\n        constraint_no_corr = [\n            jnp.asarray(np.concatenate(constraints['MultivariateNormalDistribution']['observable_indices'])),\n            jnp.asarray(np.concatenate(constraints['MultivariateNormalDistribution']['central_value'])),\n            jnp.asarray(np.concatenate(constraints['MultivariateNormalDistribution']['standard_deviation'])),\n        ]\n    else:\n        constraint_no_corr = None\n\n    if observable_lists_per_likelihood is not None:  # if not only correlated likelihoods\n        # selector matrix for univariate distributions\n        selector_matrix_univariate = jnp.array([\n            np.isin(observables, likelihood_observables).astype(float)\n            for likelihood_observables in observable_lists_per_likelihood\n        ])\n    else:\n        selector_matrix_univariate = jnp.zeros((0, len(observables)), dtype=float)\n\n    # multivariate normal distribution\n\n    _observable_lists_per_likelihood = observable_lists_per_likelihood or [observables]\n    # Collect all unique MVN blocks into this dict\n    unique_mvnd_blocks = {}\n\n    # For each likelihood, keep track of which MVNs it uses (by key)\n    mvnd_keys_per_likelihood = [[] for _ in _observable_lists_per_likelihood]\n\n    # Loop over all likelihood definitions\n    for i, observable_list in enumerate(_observable_lists_per_likelihood):\n\n        mvnd_block_data = Measurement.get_constraints(\n            observable_list,\n            include_measurements=self.include_measurements,\n            observables_for_indices=observables,\n            distribution_types=['MultivariateNormalDistribution'],\n        )['MultivariateNormalDistribution']\n\n        for j in range(len(mvnd_block_data['measurement_name'])):\n            mvnd_entry = {k: mvnd_block_data[k][j] for k in mvnd_block_data.keys()}\n            mvnd_key = (mvnd_entry['measurement_name'], tuple(mvnd_entry['observables']))\n            unique_mvnd_blocks[mvnd_key] = mvnd_entry\n            mvnd_keys_per_likelihood[i].append(mvnd_key)\n\n    # Final ordered list of all unique MVN blocks\n    all_mvnd_keys = list(unique_mvnd_blocks.keys())\n\n    n_likelihoods = len(mvnd_keys_per_likelihood)\n    n_contributions = len(all_mvnd_keys)\n\n    # Map MVND key to its index in all_mvnd_keys for fast lookup\n    mvnd_key_to_index = {key: i for i, key in enumerate(all_mvnd_keys)}\n\n    # Construct the logpdf input data from the unique MVNs\n    if all_mvnd_keys:\n        constraint_dict['MultivariateNormalDistribution'] = [\n            [jnp.asarray(unique_mvnd_blocks[k]['observable_indices']) for k in all_mvnd_keys],\n            [jnp.asarray(unique_mvnd_blocks[k]['central_value']) for k in all_mvnd_keys],\n            [jnp.asarray(unique_mvnd_blocks[k]['standard_deviation']) for k in all_mvnd_keys],\n            [jnp.asarray(unique_mvnd_blocks[k]['inverse_correlation']) for k in all_mvnd_keys],\n        ]\n        # Create selector matrix (n_likelihoods x n_contributions)\n        selector_matrix_multivariate = np.zeros((n_likelihoods, n_contributions))\n        for i, mvnd_keys in enumerate(mvnd_keys_per_likelihood):\n            for key in mvnd_keys:\n                selector_matrix_multivariate[i, mvnd_key_to_index[key]] = 1.0\n        selector_matrix_multivariate = jnp.array(selector_matrix_multivariate)\n    else:\n        selector_matrix_multivariate = jnp.zeros((n_likelihoods, 1), dtype=float)\n\n    # Get indices of MVNs that contribute to non-custom likelihoods\n    n_likelihoods_not_custom = len(self.observable_sectors_no_theory_uncertainty)\n    indices_mvn_not_custom = jnp.nonzero(\n        np.sum(\n            selector_matrix_multivariate[:n_likelihoods_not_custom],\n            axis=0\n        )\n    )[0]\n\n    return (\n        constraint_dict,\n        constraint_no_corr,\n        selector_matrix_univariate,\n        selector_matrix_multivariate,\n        indices_mvn_not_custom,\n    )\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_custom_likelihoods","title":"<code>_get_custom_likelihoods(custom_likelihoods)</code>","text":"<p>Processes custom likelihoods.</p> <p>Parameters:</p> Name Type Description Default <code>custom_likelihoods</code> <code>dict[str, list[str]] or None</code> <p>A dictionary defining custom likelihoods. The keys are the names of the custom likelihoods, and the values are lists of observable names to include in each custom likelihood.</p> required <p>Returns:</p> Name Type Description <code>likelihoods_gaussian</code> <code>dict[str, list[str]]</code> <p>A dictionary mapping custom likelihood names to lists of observables with Gaussian theory uncertainties.</p> <code>likelihoods_no_theory_uncertainty</code> <code>dict[str, list[str]]</code> <p>A dictionary mapping custom likelihood names to lists of observables with no theory uncertainty.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_custom_likelihoods(self, custom_likelihoods):\n    '''\n    Processes custom likelihoods.\n\n    Parameters\n    ----------\n    custom_likelihoods : dict[str, list[str]] or None\n        A dictionary defining custom likelihoods. The keys are the names of the custom likelihoods, and the values are lists of observable names to include in each custom likelihood.\n\n    Returns\n    -------\n    likelihoods_gaussian : dict[str, list[str]]\n        A dictionary mapping custom likelihood names to lists of observables with Gaussian theory uncertainties.\n    likelihoods_no_theory_uncertainty : dict[str, list[str]]\n        A dictionary mapping custom likelihood names to lists of observables with no theory uncertainty.\n    '''\n    if custom_likelihoods is None:\n        return {}, {}\n    if not isinstance(custom_likelihoods, dict) or not all([isinstance(k, str) and isinstance(v, list) for k, v in custom_likelihoods.items()]):\n        raise ValueError(\"The custom_likelihoods argument should be a dictionary with string names of custom likelihoods as keys and lists of observable names as values.\")\n\n    likelihoods_gaussian = {}\n    likelihoods_no_theory_uncertainty = {}\n\n    for name, observables in custom_likelihoods.items():\n        observables_gaussian = set()\n        observables_no_theory_uncertainty = set()\n        invalid_observables = set()\n        for observable in observables:\n            if observable in self.observables_gaussian:\n                observables_gaussian.add(observable)\n            elif observable in self.observables_no_theory_uncertainty:\n                observables_no_theory_uncertainty.add(observable)\n            else:\n                invalid_observables.add(observable)\n        if invalid_observables:\n            raise ValueError(\n                f\"Custom likelihood '{name}' contains observables not found in the loaded observable sectors: {sorted(invalid_observables)}\"\n            )\n        if observables_gaussian:\n            likelihoods_gaussian[f'custom_{name}'] = sorted(observables_gaussian)\n        if observables_no_theory_uncertainty:\n            likelihoods_no_theory_uncertainty[f'custom_{name}'] = sorted(observables_no_theory_uncertainty)\n\n    return likelihoods_gaussian, likelihoods_no_theory_uncertainty\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_log_likelihood_point_function","title":"<code>_get_log_likelihood_point_function()</code>","text":"<p>Returns a JIT-compiled function to compute the information needed for <code>GlobalLikelihoodPoint</code> instances.</p> <p>Returns:</p> Name Type Description <code>log_likelihood_point</code> <code>Callable</code> <p>A function that computes the predictions and log-likelihood contributions for a given parameter array, scale, and likelihood data.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_log_likelihood_point_function(self):\n    '''\n    Returns a JIT-compiled function to compute the information needed for `GlobalLikelihoodPoint` instances.\n\n    Returns\n    -------\n    log_likelihood_point : Callable\n        A function that computes the predictions and log-likelihood contributions for a given parameter array, scale, and likelihood data.\n    '''\n\n    n_likelihoods = len(self.likelihoods)\n\n    def log_likelihood_point(\n        par_array: jnp.array,\n        scale: Union[float, int, jnp.array],\n        par_dep_cov: bool,\n        prediction_data_no_theory_uncertainty: jnp.array,\n        prediction_data_correlated: jnp.array,\n        constraints_no_theory_uncertainty: Dict[str,Union[List[jnp.array],List[List[jnp.array]]]],\n        constraints_correlated_par_indep_cov: Union[List[jnp.array],List[List[jnp.array]]],\n        constraints_correlated_par_dep_cov: Union[List[jnp.array],List[List[jnp.array]]],\n        selector_matrix_no_th_unc_univariate: jnp.array,\n        selector_matrix_no_th_unc_multivariate: jnp.array,\n        selector_matrix_correlated: List[jnp.array],\n        likelihood_indices_no_theory_uncertainty: jnp.array,\n        likelihood_indices_correlated: jnp.array,\n        likelihood_indices_global: jnp.array,\n    ) -&gt; Tuple[jnp.array]:\n\n        # no theory uncertainty likelihoods and predictions\n        prediction_no_theory_uncertainty = self.prediction_function_no_theory_uncertainty(\n            par_array, scale, prediction_data_no_theory_uncertainty\n        )\n        log_likelihood_no_th_unc_univariate = jnp.zeros(len(prediction_no_theory_uncertainty))\n        log_likelihood_no_th_unc_multivariate = jnp.zeros((1, len(prediction_no_theory_uncertainty)))\n        for distribution_type in constraints_no_theory_uncertainty.keys():\n            if distribution_type == 'MultivariateNormalDistribution':\n                log_likelihood_no_th_unc_multivariate = logL_functions[distribution_type](\n                    prediction_no_theory_uncertainty,\n                    *constraints_no_theory_uncertainty[distribution_type]\n                )\n            else:\n                log_likelihood_no_th_unc_univariate += logL_functions[distribution_type](\n                    prediction_no_theory_uncertainty,\n                    *constraints_no_theory_uncertainty[distribution_type]\n                )\n\n        log_likelihood_no_theory_uncertainty_summed = (\n            selector_matrix_no_th_unc_univariate @ log_likelihood_no_th_unc_univariate\n            + selector_matrix_no_th_unc_multivariate @ jnp.sum(log_likelihood_no_th_unc_multivariate, axis=1)\n        )\n\n        # correlated likelihoods and predictions\n        prediction_correlated = [\n            prediction_function(\n                par_array, scale, prediction_data_correlated[i]\n            ) for i, prediction_function in enumerate(self.prediction_function_correlated)  # includes predictions and par_monomials\n        ]\n        n_correlated_sectors = len(prediction_correlated)\n        log_likelihood_correlated = []\n        std_th_exp_correlated_scaled = []\n        if par_dep_cov:\n            (cov_coeff_th_scaled,\n             std_sm_exp,\n             observable_indices,\n             exp_central_scaled,\n             cov_exp_scaled,\n            ) = constraints_correlated_par_dep_cov\n            for i in range(n_correlated_sectors):\n                predictions, par_monomials = prediction_correlated[i]\n                cov_obs_th_scaled = cov_coeff_to_cov_obs(par_monomials, cov_coeff_th_scaled[i])\n                std_th_exp_correlated_scaled.append(jnp.sqrt(jnp.diag(cov_obs_th_scaled) + jnp.diag(cov_exp_scaled[i])))\n                log_likelihood_correlated.append(\n                    logL_correlated_sectors(\n                        predictions/std_sm_exp[i],\n                        observable_indices[i],\n                        exp_central_scaled[i],\n                        cov_obs_th_scaled,\n                        cov_exp_scaled[i]\n                    )\n                )\n        else:\n            (\n             observable_indices,\n             mean,\n             standard_deviation,\n             inverse_correlation,\n            ) = constraints_correlated_par_indep_cov\n            logL_function = logL_functions['MultivariateNormalDistribution']\n            for i in range(n_correlated_sectors):\n                predictions, _ = prediction_correlated[i]\n                std_th_exp_correlated_scaled.append(jnp.ones_like(predictions))\n                log_likelihood_correlated.append(\n                    logL_function(\n                        predictions,\n                        observable_indices[i],\n                        mean[i],\n                        standard_deviation[i],\n                        inverse_correlation[i],\n                    )\n                )\n\n        n_correlated_likelihoods = len(likelihood_indices_correlated)\n        log_likelihood_correlated_summed = jnp.zeros(n_correlated_likelihoods)\n        for i in range(n_correlated_sectors):\n            logL = jnp.sum(log_likelihood_correlated[i], axis=1)\n            logL = jnp.where(jnp.isnan(logL), len(log_likelihood_correlated[i])*LOG_ZERO, logL)\n            log_likelihood_correlated_summed += selector_matrix_correlated[i] @ logL\n\n        log_likelihood_summed = jnp.zeros(n_likelihoods)\n        log_likelihood_summed = log_likelihood_summed.at[likelihood_indices_no_theory_uncertainty].add(log_likelihood_no_theory_uncertainty_summed)\n        log_likelihood_summed = log_likelihood_summed.at[likelihood_indices_correlated].add(log_likelihood_correlated_summed)\n        log_likelihood_global = jnp.sum(log_likelihood_summed[likelihood_indices_global])\n        log_likelihood_summed = log_likelihood_summed.at[-1].set(log_likelihood_global)\n        return (\n            prediction_no_theory_uncertainty,\n            prediction_correlated,\n            log_likelihood_no_th_unc_univariate,\n            log_likelihood_no_th_unc_multivariate,\n            log_likelihood_correlated,\n            log_likelihood_summed,\n            std_th_exp_correlated_scaled,\n        )\n    return jit(log_likelihood_point, static_argnames=[\"par_dep_cov\"])\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_observable_sectors","title":"<code>_get_observable_sectors(include_observable_sectors, exclude_observable_sectors)</code>","text":"<p>Determines the observable sectors to include in the likelihood based on inclusion/exclusion lists.</p> <p>Parameters:</p> Name Type Description Default <code>include_observable_sectors</code> <code>list[str] or None</code> <p>A list of observable sector names to include in the likelihood. If None, all loaded observable sectors are included.</p> required <code>exclude_observable_sectors</code> <code>list[str] or None</code> <p>A list of observable sector names to exclude from the likelihood. If None, no sectors are excluded.</p> required <p>Returns:</p> Name Type Description <code>observable_sectors_gaussian</code> <code>list[str]</code> <p>The list of observable sector names containing observables with Gaussian theory uncertainties.</p> <code>observable_sectors_no_theory_uncertainty</code> <code>list[str]</code> <p>The list of observable sector names containing observables with no theory uncertainty.</p> <code>basis_mode</code> <code>str</code> <p>The basis mode, either <code>rgevolve</code>, <code>wcxf</code>, or <code>custom</code>.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_observable_sectors(self, include_observable_sectors, exclude_observable_sectors):\n    '''\n    Determines the observable sectors to include in the likelihood based on inclusion/exclusion lists.\n\n    Parameters\n    ----------\n    include_observable_sectors : list[str] or None\n        A list of observable sector names to include in the likelihood. If None, all loaded observable sectors are included.\n    exclude_observable_sectors : list[str] or None\n        A list of observable sector names to exclude from the likelihood. If None, no sectors are excluded.\n\n    Returns\n    -------\n    observable_sectors_gaussian : list[str]\n        The list of observable sector names containing observables with Gaussian theory uncertainties.\n    observable_sectors_no_theory_uncertainty : list[str]\n        The list of observable sector names containing observables with no theory uncertainty.\n    basis_mode : str\n        The basis mode, either `rgevolve`, `wcxf`, or `custom`.\n    '''\n    if include_observable_sectors is not None and exclude_observable_sectors is not None:\n        raise ValueError(\"Please provide either `include_observable_sectors` or `exclude_observable_sectors`, not both.\")\n    available_observable_sectors = set(ObservableSector.get_all_names(eft=self.eft, basis=self.basis, custom_basis=self.custom_basis))\n    if include_observable_sectors is not None:\n        if set(include_observable_sectors)-available_observable_sectors:\n            raise ValueError(f\"Observable sectors {set(include_observable_sectors)-available_observable_sectors} provided in `include_observable_sectors` but not found in loaded observable sectors\")\n        observable_sectors = sorted(\n            include_observable_sectors\n        )\n    elif exclude_observable_sectors is not None:\n        if set(exclude_observable_sectors)-available_observable_sectors:\n            raise ValueError(f\"Observable sectors {set(exclude_observable_sectors)-available_observable_sectors} provided in `exclude_observable_sectors` but not found in loaded observable sectors\")\n        observable_sectors = sorted(\n            available_observable_sectors - set(exclude_observable_sectors)\n        )\n    else:\n        observable_sectors = sorted(available_observable_sectors)\n    if observable_sectors:\n        basis_mode = ObservableSector.get(observable_sectors[0]).basis_mode\n        if basis_mode in ['wcxf', 'custom']:\n            scales = set(\n                ObservableSector.get(observable_sector).scale\n                for observable_sector in observable_sectors\n            )\n            if len(scales) &gt; 1:\n                raise ValueError(\n                    f\"Observable sectors for basis {self.custom_basis or (self.eft, self.basis)} are defined at different scales. Please use `include_observable_sectors` or `exclude_observable_sectors` to select observable sectors at the same scale.\"\n                )\n    observable_sectors_gaussian = []\n    observable_sectors_no_theory_uncertainty = []\n    for observable_sector in observable_sectors:\n        if ObservableSector.get(observable_sector).observable_uncertainties is None:\n            observable_sectors_no_theory_uncertainty.append(observable_sector)\n        else:\n            observable_sectors_gaussian.append(observable_sector)\n    return observable_sectors_gaussian, observable_sectors_no_theory_uncertainty, basis_mode\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_observable_sectors_correlated","title":"<code>_get_observable_sectors_correlated()</code>","text":"<p>Determines and returns useful information about correlated observable sectors.</p> <p>Returns:</p> Name Type Description <code>observable_sectors_correlated</code> <code>list[list[str]]</code> <p>The list of lists of observable sector names in correlated groups.</p> <code>cov_coeff_th_scaled</code> <code>list[list[list[array]]]</code> <p>The list of lists of theory correlation coefficient matrices for each correlated group, scaled by the combined SM and experimental uncertainties.</p> <code>cov_exp_scaled</code> <code>list[array]</code> <p>The list of experimental covariance matrices for each correlated group, scaled by the combined SM and experimental uncertainties.</p> <code>exp_central_scaled</code> <code>list[array]</code> <p>The list of experimental central values for each correlated group, scaled by the combined SM and experimental uncertainties.</p> <code>std_sm_exp</code> <code>list[array]</code> <p>The list of combined SM and experimental uncertainties for each correlated group.</p> <code>std_exp_list</code> <code>list[array]</code> <p>The list of experimental uncertainties for each correlated group.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_observable_sectors_correlated(self):\n    '''\n    Determines and returns useful information about correlated observable sectors.\n\n    Returns\n    -------\n    observable_sectors_correlated : list[list[str]]\n        The list of lists of observable sector names in correlated groups.\n    cov_coeff_th_scaled : list[list[list[jnp.array]]]\n        The list of lists of theory correlation coefficient matrices for each correlated group, scaled by the combined SM and experimental uncertainties.\n    cov_exp_scaled : list[jnp.array]\n        The list of experimental covariance matrices for each correlated group, scaled by the combined SM and experimental uncertainties.\n    exp_central_scaled : list[jnp.array]\n        The list of experimental central values for each correlated group, scaled by the combined SM and experimental uncertainties.\n    std_sm_exp : list[jnp.array]\n        The list of combined SM and experimental uncertainties for each correlated group.\n    std_exp_list : list[jnp.array]\n        The list of experimental uncertainties for each correlated group.\n    '''\n\n    # get correlations for all gaussian observable sectors\n\n    correlations_th =  []\n    correlations_exp =  []\n    for i, row_sector in enumerate(self.observable_sectors_gaussian):\n        row_th = []\n        row_exp = []\n        for j, col_sector in enumerate(self.observable_sectors_gaussian[:i+1]):\n            obs_row = ObservableSector.get(row_sector).observable_names\n            obs_col = ObservableSector.get(col_sector).observable_names\n            row_th.append(TheoryCorrelations.get_data(obs_row, obs_col))\n            row_exp.append(ExperimentalCorrelations.get_data('correlations', self.include_measurements, obs_row, obs_col))\n        correlations_th.append(row_th)\n        correlations_exp.append(row_exp)\n\n\n    # find connected components of the correlation graph\n\n    G = nx.Graph()\n    G.add_nodes_from(self.observable_sectors_gaussian)\n    for i, name_i in enumerate(self.observable_sectors_gaussian):\n        for j, name_j in enumerate(self.observable_sectors_gaussian[:i+1]):\n            if correlations_th[i][j] is not None or correlations_exp[i][j] is not None:\n                G.add_edge(name_i, name_j)\n    components = list(nx.connected_components(G))\n    components = [sorted(list(group)) for group in components]\n    components = sorted(components, key=lambda c: self.observable_sectors_gaussian.index(c[0]))\n    observable_sectors_correlated = components\n\n\n    # get combined sm and exp standard deviations and scaled uncertainties for connected components\n\n    std_th_scaled = []\n    std_exp_scaled = []\n    std_sm_exp = []\n    exp_central_scaled = []\n    std_exp_list = []\n    for group in components:\n        sub_std_th_scaled = []\n        sub_std_exp_scaled = []\n        sub_std_sm_exp = []\n        sub_exp_central_scaled = []\n        sub_std_exp = []\n        for i, row_sector in enumerate(group):\n            obs_row = ObservableSector.get(row_sector).observable_names\n            std_exp = ExperimentalCorrelations.get_data('uncertainties', self.include_measurements, obs_row)\n            exp_central = ExperimentalCorrelations.get_data('central', self.include_measurements, obs_row)\n            std_th = ObservableSector.get(row_sector).observable_uncertainties\n            std_sm = ObservableSector.get(row_sector).observable_uncertainties_SM\n            _std_sm_exp = std_exp * np.sqrt(1 + (std_sm / std_exp)**2) # combined sm + exp uncertainty\n            sub_std_th_scaled.append(std_th/_std_sm_exp)\n            sub_std_exp_scaled.append(std_exp/_std_sm_exp)\n            sub_std_sm_exp.append(_std_sm_exp)\n            sub_exp_central_scaled.append(exp_central/_std_sm_exp)\n            sub_std_exp.append(std_exp)\n        std_th_scaled.append(sub_std_th_scaled)\n        std_exp_scaled.append(sub_std_exp_scaled)\n        std_sm_exp.append(jnp.array(np.concatenate(sub_std_sm_exp)))\n        exp_central_scaled.append(jnp.array(np.concatenate(sub_exp_central_scaled)))\n        std_exp_list.append(jnp.array(np.concatenate(sub_std_exp)))\n\n\n    # get scaled covariance matrices for connected components\n\n    cov_coeff_th_scaled = []\n    cov_exp_scaled = []\n    for k, group in enumerate(components):\n        sub_th = []\n        sub_exp = []\n        for i, row_sector in enumerate(group):\n            row_th = []\n            row_exp = []\n            for j, col_sector in enumerate(group[:i+1]):\n                obs_row = ObservableSector.get(row_sector).observable_names\n                obs_col = ObservableSector.get(col_sector).observable_names\n                row_th.append(TheoryCorrelations.get_cov_scaled(\n                    self.include_measurements, obs_row, obs_col, std_th_scaled[k][i], std_th_scaled[k][j]\n                ))\n                row_exp.append(ExperimentalCorrelations.get_cov_scaled(\n                    self.include_measurements, obs_row, obs_col, std_exp_scaled[k][i], std_exp_scaled[k][j]\n                ))\n            sub_th.append(row_th)\n            sub_exp.append(row_exp)\n        cov_coeff_th_scaled.append(sub_th)\n\n        n_sectors = len(sub_exp)\n        cov_exp = np.empty((n_sectors, n_sectors), dtype=object).tolist()\n        for i in range(n_sectors):\n            for j in range(n_sectors):\n                if i &gt;= j:\n                    cov_exp[i][j] = sub_exp[i][j]\n                else:\n                    shape = sub_exp[j][i].shape\n                    cov_exp[i][j] = np.zeros((shape[1], shape[0]))\n        cov_exp_tril = np.tril(np.block(cov_exp))\n        sub_exp = cov_exp_tril + cov_exp_tril.T - np.diag(np.diag(cov_exp_tril))\n        cov_exp_scaled.append(jnp.array(sub_exp))\n\n    return (\n        observable_sectors_correlated,\n        cov_coeff_th_scaled,\n        cov_exp_scaled,\n        exp_central_scaled,\n        std_sm_exp,\n        std_exp_list,\n    )\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_observables_per_likelihood","title":"<code>_get_observables_per_likelihood()</code>","text":"<p>Constructs dictionaries mapping likelihood names to lists of observables for both no theory uncertainty and correlated sectors.</p> <p>Returns:</p> Name Type Description <code>observables_per_likelihood_no_theory_uncertainty</code> <code>dict[str, list[str]]</code> <p>A dictionary mapping likelihood names to lists of observables with no theory uncertainty.</p> <code>observables_per_likelihood_correlated</code> <code>dict[str, list[str]]</code> <p>A dictionary mapping likelihood names to lists of observables with Gaussian theory uncertainties.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_observables_per_likelihood(self):\n    '''\n    Constructs dictionaries mapping likelihood names to lists of observables for both no theory uncertainty and correlated sectors.\n\n    Returns\n    -------\n    observables_per_likelihood_no_theory_uncertainty : dict[str, list[str]]\n        A dictionary mapping likelihood names to lists of observables with no theory uncertainty.\n    observables_per_likelihood_correlated : dict[str, list[str]]\n        A dictionary mapping likelihood names to lists of observables with Gaussian theory uncertainties.\n    '''\n\n    observables_per_likelihood_no_theory_uncertainty = {\n        observable_sector: ObservableSector.get(observable_sector).observable_names\n        for observable_sector in self.observable_sectors_no_theory_uncertainty\n    }\n\n    observables_per_likelihood_correlated = {\n        tuple(observable_sectors): self.observables_correlated[i]\n        for i, observable_sectors in enumerate(self.observable_sectors_correlated)\n        }\n\n    return observables_per_likelihood_no_theory_uncertainty, observables_per_likelihood_correlated\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_obstable_function","title":"<code>_get_obstable_function()</code>","text":"<p>Returns a JIT-compiled function to compute the observable table information.</p> <p>Returns:</p> Name Type Description <code>obstable</code> <code>Callable</code> <p>A function that computes the log-likelihood contributions and related information for a given set of predictions and constraints.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_obstable_function(self):\n    '''\n    Returns a JIT-compiled function to compute the observable table information.\n\n    Returns\n    -------\n    obstable : Callable\n        A function that computes the log-likelihood contributions and related information for a given set of predictions and constraints.\n    '''\n\n    @jit\n    def obstable(\n        prediction_no_theory_uncertainty: jnp.array,\n        prediction_correlated: List[jnp.array],\n        log_likelihood_no_th_unc_multivariate: jnp.array,\n        log_likelihood_correlated: List[jnp.array],\n        std_th_exp_correlated_scaled: List[jnp.array],\n        constraints_no_theory_uncertainty_no_corr: List[jnp.array],\n        indices_mvn_not_custom: jnp.array,\n        exp_central_scaled: List[jnp.array],\n        std_sm_exp: List[jnp.array],\n    ) -&gt; Tuple[jnp.array]:\n\n        # no theory uncertainty sectors\n        # including correlations\n        log_likelihood_no_th_unc_multivariate = jnp.sum(\n            jnp.take(\n                log_likelihood_no_th_unc_multivariate,\n                indices_mvn_not_custom,\n                axis=0\n            ),\n            axis=0\n        )\n\n        # neglecting correlations\n        if constraints_no_theory_uncertainty_no_corr is not None:\n            log_likelihood_no_th_unc_multivariate_no_corr = logL_functions['NormalDistribution'](\n                prediction_no_theory_uncertainty,\n                *constraints_no_theory_uncertainty_no_corr,\n            )\n        else:\n            log_likelihood_no_th_unc_multivariate_no_corr = jnp.zeros(len(prediction_no_theory_uncertainty))\n\n        # correlated sectors\n        # including correlations\n        log_likelihood_correlated = [log_likelihood[0] for log_likelihood in log_likelihood_correlated]\n\n        # neglecting correlations\n        log_likelihood_correlated_no_corr = []\n        exp_central_correlated = []\n        std_th_exp_correlated = []\n        n_correlated_sectors = len(prediction_correlated)\n        for i in range(n_correlated_sectors):\n            std_th_exp = std_th_exp_correlated_scaled[i] * std_sm_exp[i]\n            exp_central = exp_central_scaled[i] * std_sm_exp[i]\n            observable_indices = jnp.arange(len(prediction_correlated[i][0]))\n            log_likelihood_correlated_no_corr.append(\n                logL_functions['NormalDistribution'](\n                    prediction_correlated[i][0],\n                    observable_indices,\n                    exp_central,\n                    std_th_exp\n                )\n            )\n            exp_central_correlated.append(exp_central)\n            std_th_exp_correlated.append(std_th_exp)\n\n        return (\n            log_likelihood_no_th_unc_multivariate,\n            log_likelihood_no_th_unc_multivariate_no_corr,\n            log_likelihood_correlated,\n            log_likelihood_correlated_no_corr,\n            exp_central_correlated,\n            std_th_exp_correlated,\n        )\n    return obstable\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_par_array","title":"<code>_get_par_array(par_dict)</code>","text":"<p>Converts a parameter dictionary into a JAX array.</p> <p>Parameters:</p> Name Type Description Default <code>par_dict</code> <code>dict</code> <p>A dictionary mapping parameter names (or tuples of parameter name and 'R'/'I') to their values.</p> required <p>Returns:</p> Type Description <code>array</code> <p>A JAX array containing the parameter values in the order defined by <code>parameter_basis_split_re_im</code>.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_par_array(self, par_dict):\n    '''\n    Converts a parameter dictionary into a JAX array.\n\n    Parameters\n    ----------\n    par_dict : dict\n        A dictionary mapping parameter names (or tuples of parameter name and 'R'/'I') to their values.\n\n    Returns\n    -------\n    jnp.array\n        A JAX array containing the parameter values in the order defined by `parameter_basis_split_re_im`.\n    '''\n    if not par_dict:\n        return jnp.zeros(len(self.parameter_basis_split_re_im))\n    elif isinstance(list(par_dict.keys())[0], tuple):\n        par_array = np.zeros(len(self.parameter_basis_split_re_im))\n        for name, value in par_dict.items():\n            if name not in self.parameter_basis_split_re_im:\n                raise ValueError(f\"Parameter {name} not found in the parameter basis.\")\n            par_array[self.parameter_basis_split_re_im[name]] = value\n        return jnp.array(par_array)\n    else:\n        par_array = np.zeros(len(self.parameter_basis_split_re_im))\n        for name, value in par_dict.items():\n            if (name,'R') not in self.parameter_basis_split_re_im:\n                raise ValueError(f\"Parameter {name} not found in the parameter basis.\")\n            par_array[self.parameter_basis_split_re_im[(name, 'R')]] = value.real\n            if (name, 'I') in self.parameter_basis_split_re_im:\n                par_array[self.parameter_basis_split_re_im[(name, 'I')]] = value.imag\n        return jnp.array(par_array)\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_parameter_basis","title":"<code>_get_parameter_basis()</code>","text":"<p>Determines the parameter basis and splits parameters into real and imaginary parts.</p> <p>Returns:</p> Name Type Description <code>parameter_basis_split_re_im</code> <code>Dict[Union[str, Tuple[str, str]], int]</code> <p>A dictionary mapping parameter names (or tuples of parameter name and 'R'/'I') to their indices in the basis with real and imaginary parts split.</p> <code>parameter_basis</code> <code>Dict[str, int]</code> <p>A dictionary mapping parameter names to their indices in the basis without splitting real and imaginary parts.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_parameter_basis(self):\n    '''\n    Determines the parameter basis and splits parameters into real and imaginary parts.\n\n    Returns\n    -------\n    parameter_basis_split_re_im : Dict[Union[str, Tuple[str, str]], int]\n        A dictionary mapping parameter names (or tuples of parameter name and 'R'/'I') to their indices in the basis with real and imaginary parts split.\n    parameter_basis : Dict[str, int]\n        A dictionary mapping parameter names to their indices in the basis without splitting real and imaginary parts.\n    '''\n    if self.basis_mode == 'rgevolve':\n        parameter_basis_split_re_im = get_wc_basis(eft=self.eft, basis=self.basis, sector=None, split_re_im=True)\n        parameter_basis = get_wc_basis(eft=self.eft, basis=self.basis, sector=None, split_re_im=False)\n    elif self.basis_mode == 'wcxf':\n        parameter_basis_split_re_im = get_wc_basis_from_wcxf(eft=self.eft, basis=self.basis, sector=None, split_re_im=True)\n        parameter_basis = get_wc_basis_from_wcxf(eft=self.eft, basis=self.basis, sector=None, split_re_im=False)\n    else:\n        custom_basis = CustomBasis.get(\n            ObservableSector.get(self.observable_sectors[0]).custom_basis\n        )\n        parameter_basis_split_re_im = custom_basis.get_parameter_basis(split_re_im=True)\n        parameter_basis = custom_basis.get_parameter_basis(split_re_im=False)\n    parameter_basis_split_re_im = {par: i for i, par in enumerate(parameter_basis_split_re_im)}\n    parameter_basis = {par: i for i, par in enumerate(parameter_basis)}\n    return parameter_basis_split_re_im, parameter_basis\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_prediction_function_gaussian","title":"<code>_get_prediction_function_gaussian(observable_sectors_gaussian)</code>","text":"<p>Returns a prediction function for the Gaussian observable sectors.</p> <p>Parameters:</p> Name Type Description Default <code>observable_sectors_gaussian</code> <code>list[str]</code> <p>A list of observable sector names containing observables with Gaussian theory uncertainties.</p> required <p>Returns:</p> Name Type Description <code>prediction</code> <code>Callable</code> <p>A function that takes a parameter array, scale, and prediction data, and returns the polynomial predictions and parameter monomials.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_prediction_function_gaussian(self, observable_sectors_gaussian):\n    '''\n    Returns a prediction function for the Gaussian observable sectors.\n\n    Parameters\n    ----------\n    observable_sectors_gaussian : list[str]\n        A list of observable sector names containing observables with Gaussian theory uncertainties.\n\n    Returns\n    -------\n    prediction : Callable\n        A function that takes a parameter array, scale, and prediction data, and returns the polynomial predictions and parameter monomials.\n    '''\n\n    prediction_functions = [\n        ObservableSector.get(name).prediction\n        for name in observable_sectors_gaussian\n    ]\n\n    def prediction(\n        par_array: jnp.array, scale: Union[float, int, jnp.array],\n        prediction_data: List[List[jnp.array]]\n    ) -&gt; jnp.array:\n        polynomial_predictions = [jnp.empty(0)]\n        par_monomials = []\n        for prediction_function, data in zip(prediction_functions, prediction_data):\n            polynomial_prediction, par_monomial = prediction_function(\n                par_array, scale, data\n            )\n            polynomial_predictions.append(polynomial_prediction)\n            par_monomials.append(par_monomial)\n        polynomial_predictions = jnp.concatenate(polynomial_predictions, axis=-1)\n        return polynomial_predictions, par_monomials\n\n    return prediction\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_prediction_function_no_theory_uncertainty","title":"<code>_get_prediction_function_no_theory_uncertainty()</code>","text":"<p>Returns a prediction function for observables with no theory uncertainty.</p> <p>Returns:</p> Name Type Description <code>prediction</code> <code>Callable</code> <p>A function that takes a parameter array, scale, and prediction data, and returns the polynomial predictions for observables with no theory uncertainty.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_prediction_function_no_theory_uncertainty(self):\n    '''\n    Returns a prediction function for observables with no theory uncertainty.\n\n    Returns\n    -------\n    prediction : Callable\n        A function that takes a parameter array, scale, and prediction data, and returns the polynomial predictions for observables with no theory uncertainty.\n    '''\n\n    prediction_functions = [\n        ObservableSector.get(name).prediction\n        for name in self.observable_sectors_no_theory_uncertainty\n    ]\n    def prediction(\n        par_array: jnp.array, scale: Union[float, int, jnp.array],\n        prediction_data: List[List[jnp.array]]\n    ) -&gt; jnp.array:\n        polynomial_predictions = [jnp.empty(0)]\n        for prediction_function, data in zip(prediction_functions, prediction_data):\n            polynomial_predictions.append(\n                prediction_function(par_array, scale, data)[0]\n            )\n        polynomial_predictions = jnp.concatenate(polynomial_predictions, axis=-1)\n        return polynomial_predictions\n\n\n    return prediction\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood._get_reference_scale","title":"<code>_get_reference_scale()</code>","text":"<p>Determines the reference scale for the likelihood.</p> <p>Returns:</p> Type Description <code>float</code> <p>The reference scale for the likelihood.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _get_reference_scale(self):\n    '''\n    Determines the reference scale for the likelihood.\n\n    Returns\n    -------\n    float\n        The reference scale for the likelihood.\n    '''\n    if self.basis_mode == 'rgevolve':\n        return float(reference_scale[self.eft])\n    else:\n        return ObservableSector.get(self.observable_sectors[0]).scale\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood.get_compiled_likelihood","title":"<code>get_compiled_likelihood(par_list, likelihood, par_dep_cov=False)</code>","text":"<p>Returns an instance of <code>CompiledLikelihood</code> for the specified parameters and likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>par_list</code> <code>List[Tuple[str, str]]</code> <p>List of tuples specifying the parameters to include in the likelihood evaluation. Each entry is a tuple where the first element is the parameter name and the second element is <code>R</code> for real parameters or <code>I</code> for imaginary parameters.</p> required <code>likelihood</code> <code>Union[str, Tuple[str, ...]]</code> <p>The likelihood to evaluate. This can be a string specifying a single likelihood (e.g., 'global' for the combined likelihood, or the name of a specific likelihood), or a tuple of strings specifying a correlated set of likelihoods.</p> required <code>par_dep_cov</code> <code>bool</code> <p>Whether to use the parameter-dependent covariance matrix for correlated likelihoods. Default is <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>CompiledLikelihood</code> <p>An instance of <code>CompiledLikelihood</code> containing jitted functions for likelihood evaluation.</p> <p>Examples:</p> <p>Get a <code>CompiledLikelihood</code> instance for a specific set of parameters and the global likelihood:</p> <pre><code>&gt;&gt;&gt; compiled_likelihood = global_likelihood.get_compiled_likelihood(par_list=[('lq1_1111', 'R'), ('lq3_1111', 'R')], likelihood='global', par_dep_cov=False)\n</code></pre> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def get_compiled_likelihood(\n    self,\n    par_list: List[Tuple[str, str]],\n    likelihood: Union[str, Tuple[str, ...]],\n    par_dep_cov: bool = False,\n):\n    '''\n    Returns an instance of `CompiledLikelihood` for the specified parameters and likelihood.\n\n    Parameters\n    ----------\n    par_list : List[Tuple[str, str]]\n        List of tuples specifying the parameters to include in the likelihood evaluation. Each entry is a tuple where the first element is the parameter name and the second element is `R` for real parameters or `I` for imaginary parameters.\n    likelihood : Union[str, Tuple[str, ...]]\n        The likelihood to evaluate. This can be a string specifying a single likelihood (e.g., 'global' for the combined likelihood, or the name of a specific likelihood), or a tuple of strings specifying a correlated set of likelihoods.\n    par_dep_cov : bool, optional\n        Whether to use the parameter-dependent covariance matrix for correlated likelihoods. Default is `False`.\n\n    Returns\n    -------\n    CompiledLikelihood\n        An instance of `CompiledLikelihood` containing jitted functions for likelihood evaluation.\n\n    Examples\n    --------\n    Get a `CompiledLikelihood` instance for a specific set of parameters and the global likelihood:\n    &gt;&gt;&gt; compiled_likelihood = global_likelihood.get_compiled_likelihood(par_list=[('lq1_1111', 'R'), ('lq3_1111', 'R')], likelihood='global', par_dep_cov=False)\n    '''\n    if (tuple(par_list), likelihood, par_dep_cov) not in self._cache_compiled_likelihood:\n        compiled_likelihood = CompiledLikelihood(\n            self,\n            par_list,\n            likelihood,\n            par_dep_cov,\n        )\n        self._cache_compiled_likelihood[(tuple(par_list), likelihood, par_dep_cov)] = compiled_likelihood\n    return self._cache_compiled_likelihood[(tuple(par_list), likelihood, par_dep_cov)]\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood.get_negative_log_likelihood","title":"<code>get_negative_log_likelihood(par_list, likelihood, par_dep_cov)</code>","text":"<p>Get a function that computes the negative log-likelihood for a given list of parameters and likelihood, and the corresponding likelihood data</p> <p>Parameters:</p> Name Type Description Default <code>par_list</code> <code>List[Tuple[str, str]]</code> <p>List of tuples specifying the parameters to include in the likelihood evaluation. Each entry is a tuple where the first element is the parameter name and the second element is <code>R</code> for real parameters or <code>I</code> for imaginary parameters.</p> required <code>likelihood</code> <code>Union[str, Tuple[str, ...]]</code> <p>The likelihood to evaluate. This can be a string specifying a single likelihood (e.g., 'global' for the combined likelihood, or the name of a specific likelihood), or a tuple of strings specifying a correlated set of likelihoods.</p> required <code>par_dep_cov</code> <code>bool</code> <p>Whether to use the parameter-dependent covariance matrix for correlated likelihoods.</p> required <p>Returns:</p> Name Type Description <code>negative_log_likelihood</code> <code>Callable</code> <p>A function that computes the negative log-likelihood given an array of parameter values, a scale, and the likelihood data.</p> <code>log_likelihood_data</code> <code>List</code> <p>A list containing the data needed for the likelihood evaluation.</p> <p>Examples:</p> <p>Get the negative log-likelihood function and data for a specific set of parameters and the global likelihood:</p> <pre><code>&gt;&gt;&gt; negative_log_likelihood, log_likelihood_data = global_likelihood.get_negative_log_likelihood(par_list=[('lq1_1111', 'R'), ('lq3_1111', 'R')], likelihood='global', par_dep_cov=False\n&gt;&gt;&gt; par_array = jnp.array([1e-8, 1e-8])\n&gt;&gt;&gt; scale = 1000.0\n&gt;&gt;&gt; nll_value = negative_log_likelihood(par_array, scale, log_likelihood_data)\n</code></pre> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def get_negative_log_likelihood(\n        self,\n        par_list: List[Tuple[str, str]],\n        likelihood: Union[str, Tuple[str, ...]],\n        par_dep_cov: bool,\n    ):\n    '''\n    Get a function that computes the negative log-likelihood for a given list of parameters and likelihood, and the corresponding likelihood data\n\n    Parameters\n    ----------\n    par_list : List[Tuple[str, str]]\n        List of tuples specifying the parameters to include in the likelihood evaluation. Each entry is a tuple where the first element is the parameter name and the second element is `R` for real parameters or `I` for imaginary parameters.\n    likelihood : Union[str, Tuple[str, ...]]\n        The likelihood to evaluate. This can be a string specifying a single likelihood (e.g., 'global' for the combined likelihood, or the name of a specific likelihood), or a tuple of strings specifying a correlated set of likelihoods.\n    par_dep_cov : bool\n        Whether to use the parameter-dependent covariance matrix for correlated likelihoods.\n\n    Returns\n    -------\n    negative_log_likelihood : Callable\n        A function that computes the negative log-likelihood given an array of parameter values, a scale, and the likelihood data.\n    log_likelihood_data : List\n        A list containing the data needed for the likelihood evaluation.\n\n    Examples\n    --------\n    Get the negative log-likelihood function and data for a specific set of parameters and the global likelihood:\n    &gt;&gt;&gt; negative_log_likelihood, log_likelihood_data = global_likelihood.get_negative_log_likelihood(par_list=[('lq1_1111', 'R'), ('lq3_1111', 'R')], likelihood='global', par_dep_cov=False\n    &gt;&gt;&gt; par_array = jnp.array([1e-8, 1e-8])\n    &gt;&gt;&gt; scale = 1000.0\n    &gt;&gt;&gt; nll_value = negative_log_likelihood(par_array, scale, log_likelihood_data)\n\n    '''\n    # prepare selector matrices for included likelihoods\n    if likelihood == 'global':  # for global likelihood, select all non-custom likelihoods\n        selector_matrix_no_th_unc_univariate  = self.selector_matrix_no_th_unc_univariate[:len(self.observable_sectors_no_theory_uncertainty)]\n        selector_matrix_no_th_unc_multivariate = self.selector_matrix_no_th_unc_multivariate[:len(self.observable_sectors_no_theory_uncertainty)]\n        selector_matrix_correlated = [selector_matrix[:len(self.observable_sectors_correlated)] for selector_matrix in self.selector_matrix_correlated]\n    else:  # for a specific likelihood, select just the corresponding rows in selector matrices\n        if likelihood in self._observables_per_likelihood_no_theory_uncertainty:\n            n = list(self._observables_per_likelihood_no_theory_uncertainty).index(likelihood)\n            selector_matrix_no_th_unc_univariate = self.selector_matrix_no_th_unc_univariate[[n], :]\n            selector_matrix_no_th_unc_multivariate = self.selector_matrix_no_th_unc_multivariate[[n], :]\n        else:\n            selector_matrix_no_th_unc_univariate = None\n            selector_matrix_no_th_unc_multivariate = None\n        if likelihood in self._observables_per_likelihood_correlated:\n            n = list(self._observables_per_likelihood_correlated).index(likelihood)\n            selector_matrix_correlated = [selector_matrix[[n], :] for selector_matrix in self.selector_matrix_correlated]\n        else:\n            selector_matrix_correlated = [None for _ in self.selector_matrix_correlated]\n\n    log_likelihood_data = [\n        self.prediction_data_no_theory_uncertainty,\n        self.prediction_data_correlated,\n        self.constraints_no_theory_uncertainty,\n        self.constraints_correlated_par_indep_cov,\n        self.constraints_correlated_par_dep_cov,\n        selector_matrix_no_th_unc_univariate,\n        selector_matrix_no_th_unc_multivariate,\n        selector_matrix_correlated,\n    ]\n\n    n_parameters = len(self.parameter_basis_split_re_im)\n    par_indices = jnp.array([self.parameter_basis_split_re_im[par] for par in par_list])\n\n    def negative_log_likelihood(\n        par_array: jnp.array,\n        scale: Union[float, int, jnp.array],\n        log_likelihood_data: List,\n    ) -&gt; float:\n\n        (\n            prediction_data_no_theory_uncertainty,\n            prediction_data_correlated,\n            constraints_no_theory_uncertainty,\n            constraints_correlated_par_indep_cov,\n            constraints_correlated_par_dep_cov,\n            selector_matrix_no_th_unc_univariate,\n            selector_matrix_no_th_unc_multivariate,\n            selector_matrix_correlated,\n        ) = log_likelihood_data\n\n        par_array_full = jnp.zeros(n_parameters)\n        par_array_full = par_array_full.at[par_indices].set(par_array)\n\n        # no theory uncertainty likelihoods\n        log_likelihood_no_th_unc_summed = 0.0\n        if selector_matrix_no_th_unc_univariate is not None:\n            prediction_no_theory_uncertainty = self.prediction_function_no_theory_uncertainty(\n                par_array_full, scale, prediction_data_no_theory_uncertainty\n            )\n            for distribution_type in constraints_no_theory_uncertainty.keys():\n                if distribution_type == 'MultivariateNormalDistribution':\n                    selector_matrix = selector_matrix_no_th_unc_multivariate\n                else:\n                    selector_matrix = selector_matrix_no_th_unc_univariate\n                log_likelihood_no_th_unc_summed += jnp.sum(\n                    logL_functions_summed[distribution_type](\n                        prediction_no_theory_uncertainty,\n                        selector_matrix,\n                        *constraints_no_theory_uncertainty[distribution_type]\n                    )\n                )\n\n        # correlated likelihoods\n        prediction_correlated = [\n            prediction_function(\n                par_array_full, scale, prediction_data_correlated[i]\n            ) for i, prediction_function in enumerate(self.prediction_function_correlated)  # includes predictions and par_monomials\n        ]\n        n_correlated_sectors = len(selector_matrix_correlated)\n        log_likelihood_correlated_summed = 0.0\n        if par_dep_cov:\n            (cov_coeff_th_scaled,\n             std_sm_exp,\n             observable_indices,\n             exp_central_scaled,\n             cov_exp_scaled,\n            ) = constraints_correlated_par_dep_cov\n            for i in range(n_correlated_sectors):\n                selector_matrix = selector_matrix_correlated[i]\n                if selector_matrix is not None:\n                    predictions, par_monomials = prediction_correlated[i]\n                    cov_obs_th_scaled = cov_coeff_to_cov_obs(par_monomials, cov_coeff_th_scaled[i])\n                    log_likelihood_correlated_summed += jnp.sum(\n                        logL_correlated_sectors_summed(\n                            predictions/std_sm_exp[i],\n                            selector_matrix,\n                            observable_indices[i],\n                            exp_central_scaled[i],\n                            cov_obs_th_scaled,\n                            cov_exp_scaled[i]\n                        )\n                    )\n        else:\n            (\n             observable_indices,\n             mean,\n             standard_deviation,\n             inverse_correlation,\n            ) = constraints_correlated_par_indep_cov\n            logL_function = logL_functions_summed['MultivariateNormalDistribution']\n            for i in range(n_correlated_sectors):\n                selector_matrix = selector_matrix_correlated[i]\n                if selector_matrix is not None:\n                    predictions, _ = prediction_correlated[i]\n                    log_likelihood_correlated_summed += jnp.sum(\n                        logL_function(\n                            predictions,\n                            selector_matrix,\n                            observable_indices[i],\n                            mean[i],\n                            standard_deviation[i],\n                            inverse_correlation[i],\n                        )\n                    )\n        return - (log_likelihood_no_th_unc_summed + log_likelihood_correlated_summed)\n\n    return negative_log_likelihood, log_likelihood_data\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Initialize <code>ObservableSector</code>, <code>Measurement</code>, <code>TheoryCorrelations</code>, and <code>ExperimentalCorrelations</code> classes by loading data from the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the directory containing the data files.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Load all observable sectors, measurements, and correlations from the specified path:</p> <pre><code>&gt;&gt;&gt; GlobalLikelihood.load('path/to/data')\n</code></pre> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>@classmethod\ndef load(cls, path):\n    '''\n    Initialize `ObservableSector`, `Measurement`, `TheoryCorrelations`, and `ExperimentalCorrelations` classes by loading data from the specified path.\n\n    Parameters\n    ----------\n    path : str\n        The path to the directory containing the data files.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n\n    Load all observable sectors, measurements, and correlations from the specified path:\n\n    &gt;&gt;&gt; GlobalLikelihood.load('path/to/data')\n    '''\n    # load all observable sectors\n    ObservableSector.load(path)\n    # load all measurements\n    Measurement.load(path)\n    # load all theory correlations\n    TheoryCorrelations.load(path)\n    # load all experimental correlations\n    ExperimentalCorrelations.load()\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood.parameter_point","title":"<code>parameter_point(*args, par_dep_cov=False)</code>","text":"<p>Create a <code>GlobalLikelihoodPoint</code> instance.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>tuple</code> <p>Positional arguments. The method dispatches based on the number and types of these arguments. Accepted input signatures:</p> <ol> <li> <p><code>parameter_point(par_dict: dict, scale: Union[float, int], *, par_dep_cov: bool = False)</code></p> <ul> <li>Create a <code>GlobalLikelihoodPoint</code> from a dictionary of parameters and a scale.</li> </ul> </li> <li> <p><code>parameter_point(w: wilson.Wilson, *, par_dep_cov: bool = False)</code></p> <ul> <li>Create a <code>GlobalLikelihoodPoint</code> from a <code>wilson.Wilson</code> object.</li> </ul> </li> <li> <p><code>parameter_point(wc: wilson.wcxf.WC, *, par_dep_cov: bool = False)</code></p> <ul> <li>Create a <code>GlobalLikelihoodPoint</code> from a <code>wilson.wcxf.WC</code> object.</li> </ul> </li> <li> <p><code>parameter_point(filename: str, *, par_dep_cov: bool = False)</code></p> <ul> <li>Create a <code>GlobalLikelihoodPoint</code> from the path to a WCxf file.</li> </ul> </li> </ol> <code>()</code> <code>par_dep_cov</code> <code>bool</code> <p>If <code>True</code>, use the parameter dependent covariance matrix for the likelihood point. Default is <code>False</code>.</p> <code>False</code> <p>Returns:</p> Type Description <code>GlobalLikelihoodPoint</code> <p>An instance of GlobalLikelihoodPoint with the specified parameters.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def parameter_point(self, *args, par_dep_cov: bool = False):\n    \"\"\"\n    Create a `GlobalLikelihoodPoint` instance.\n\n    Parameters\n    ----------\n    *args : tuple\n        Positional arguments. The method dispatches\n        based on the number and types of these arguments. Accepted input signatures:\n\n          1. `parameter_point(par_dict: dict, scale: Union[float, int], *, par_dep_cov: bool = False)`\n            - Create a `GlobalLikelihoodPoint` from a dictionary of parameters and a scale.\n\n          2. `parameter_point(w: wilson.Wilson, *, par_dep_cov: bool = False)`\n            - Create a `GlobalLikelihoodPoint` from a `wilson.Wilson` object.\n\n          3. `parameter_point(wc: wilson.wcxf.WC, *, par_dep_cov: bool = False)`\n            - Create a `GlobalLikelihoodPoint` from a `wilson.wcxf.WC` object.\n\n          4. `parameter_point(filename: str, *, par_dep_cov: bool = False)`\n            - Create a `GlobalLikelihoodPoint` from the path to a WCxf file.\n\n    par_dep_cov : bool, optional\n        If `True`, use the parameter dependent covariance matrix for the likelihood point.\n        Default is `False`.\n\n    Returns\n    -------\n    GlobalLikelihoodPoint\n        An instance of GlobalLikelihoodPoint with the specified parameters.\n    \"\"\"\n\n    if len(args) == 2:\n        par_dict, scale = args\n        if not isinstance(par_dict, dict) or not isinstance(scale, (float, int)):\n            raise ValueError(\n                \"Invalid types of the two positional arguments. Expected a dictionary and scale.\"\n            )\n    elif len(args) == 1:\n        arg = args[0]\n        if isinstance(arg, Wilson):\n            par_dict = arg.wc.dict\n            scale = arg.wc.scale\n        elif isinstance(arg, wcxf.WC):\n            par_dict = arg.dict\n            scale = arg.scale\n        elif isinstance(arg, str):\n            with open(arg, 'r') as f:\n                wc = wcxf.WC.load(f)\n            par_dict = wc.dict\n            scale = wc.scale\n        else:\n            raise ValueError(\n                \"Invalid type of the positional argument. Expected a Wilson or wcxf.WC object, or a filename.\"\n            )\n    else:\n        raise ValueError(\"Invalid number of positional arguments. Expected either two (a dictionary and scale) or one (a Wilson or wcxf.WC object, or a filename).\")\n    return GlobalLikelihoodPoint(self, self._get_par_array(par_dict), scale, par_dep_cov=par_dep_cov)\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood.GlobalLikelihood.plot_data_2d","title":"<code>plot_data_2d(par_fct, scale, x_min, x_max, y_min, y_max, x_log=False, y_log=False, steps=20, par_dep_cov=False)</code>","text":"<p>Computes a grid of chi-squared values over a 2D parameter space for plotting. Returns a dictionary containing the parameter grid and the corresponding chi-squared values.</p> <p>Parameters:</p> Name Type Description Default <code>par_fct</code> <code>Callable</code> <p>A function that takes two arguments (x, y) and returns a dictionary of parameters.</p> required <code>scale</code> <code>Union[float, int, Callable]</code> <p>The scale at which to evaluate the parameters. This can be a fixed float or int, or a callable that takes (x, y) and returns a scale.</p> required <code>x_min</code> <code>float</code> <p>The minimum value of the x-axis parameter (in log10 if x_log is <code>True</code>).</p> required <code>x_max</code> <code>float</code> <p>The maximum value of the x-axis parameter (in log10 if x_log is <code>True</code>).</p> required <code>y_min</code> <code>float</code> <p>The minimum value of the y-axis parameter (in log10 if y_log is <code>True</code>).</p> required <code>y_max</code> <code>float</code> <p>The maximum value of the y-axis parameter (in log10 if y_log is <code>True</code>).</p> required <code>x_log</code> <code>bool</code> <p>Whether to use a logarithmic scale for the x-axis. Default is <code>False</code>.</p> <code>False</code> <code>y_log</code> <code>bool</code> <p>Whether to use a logarithmic scale for the y-axis. Default is <code>False</code>.</p> <code>False</code> <code>steps</code> <code>int</code> <p>The number of steps in each dimension for the grid. Default is <code>20</code>.</p> <code>20</code> <code>par_dep_cov</code> <code>bool</code> <p>Whether to use the parameter-dependent covariance matrix for correlated likelihoods. Default is <code>False</code>.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>plotdata</code> <code>Dict</code> <p>A dictionary containing the parameter grid and the corresponding chi-squared values for each likelihood. The keys are the names of the likelihoods, and the values are dictionaries with keys <code>x</code>, <code>y</code>, and <code>z</code>, where <code>x</code> and <code>y</code> are the parameter grids and <code>z</code> is the chi-squared grid.</p> <p>Examples:</p> <p>Define a function that maps (x, y) to a dictionary of parameters:</p> <pre><code>&gt;&gt;&gt; def par_func(x, y):\n...     return {'lq1_1111': x, 'lq3_1111': y}\n</code></pre> <p>Obtain the 2D chi-squared grid for two parameters over specified ranges:</p> <pre><code>&gt;&gt;&gt; plot_data = gl.plot_data_2d(par_func, scale=1000.0, x_min=-1e-8, x_max=1e-8, y_min=-1e-8, y_max=1e-8, steps=50)\n</code></pre> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def plot_data_2d(self, par_fct, scale, x_min, x_max, y_min, y_max, x_log=False, y_log=False, steps=20, par_dep_cov=False):\n    '''\n    Computes a grid of chi-squared values over a 2D parameter space for plotting. Returns a dictionary containing the parameter grid and the corresponding chi-squared values.\n\n    Parameters\n    ----------\n    par_fct : Callable\n        A function that takes two arguments (x, y) and returns a dictionary of parameters.\n    scale : Union[float, int, Callable]\n        The scale at which to evaluate the parameters. This can be a fixed float or int, or a callable that takes (x, y) and returns a scale.\n    x_min : float\n        The minimum value of the x-axis parameter (in log10 if x_log is `True`).\n    x_max : float\n        The maximum value of the x-axis parameter (in log10 if x_log is `True`).\n    y_min : float\n        The minimum value of the y-axis parameter (in log10 if y_log is `True`).\n    y_max : float\n        The maximum value of the y-axis parameter (in log10 if y_log is `True`).\n    x_log : bool, optional\n        Whether to use a logarithmic scale for the x-axis. Default is `False`.\n    y_log : bool, optional\n        Whether to use a logarithmic scale for the y-axis. Default is `False`.\n    steps : int, optional\n        The number of steps in each dimension for the grid. Default is `20`.\n    par_dep_cov : bool, optional\n        Whether to use the parameter-dependent covariance matrix for correlated likelihoods. Default is `False`.\n\n    Returns\n    -------\n    plotdata : Dict\n        A dictionary containing the parameter grid and the corresponding chi-squared values for each likelihood. The keys are the names of the likelihoods, and the values are dictionaries with keys `x`, `y`, and `z`, where `x` and `y` are the parameter grids and `z` is the chi-squared grid.\n\n    Examples\n    --------\n    Define a function that maps (x, y) to a dictionary of parameters:\n    &gt;&gt;&gt; def par_func(x, y):\n    ...     return {'lq1_1111': x, 'lq3_1111': y}\n\n    Obtain the 2D chi-squared grid for two parameters over specified ranges:\n\n    &gt;&gt;&gt; plot_data = gl.plot_data_2d(par_func, scale=1000.0, x_min=-1e-8, x_max=1e-8, y_min=-1e-8, y_max=1e-8, steps=50)\n    '''\n    if x_log:\n        _x = jnp.logspace(x_min, x_max, steps)\n    else:\n        _x = jnp.linspace(x_min, x_max, steps)\n    if y_log:\n        _y = jnp.logspace(y_min, y_max, steps)\n    else:\n        _y = jnp.linspace(y_min, y_max, steps)\n    x, y = jnp.meshgrid(_x, _y)\n    xy = jnp.array([x, y]).reshape(2, steps**2).T\n    xy_enumerated = list(enumerate(xy))\n    if isinstance(scale, Number):\n        scale_fct = partial(_scale_fct_fixed, scale=scale)\n    else:\n        scale_fct = scale\n    ll = partial(_log_likelihood_2d, gl=self, par_fct=par_fct, scale_fct=scale_fct, par_dep_cov=par_dep_cov)\n    ll_dict_list_enumerated = map(ll, xy_enumerated)  # no multiprocessing for now\n    ll_dict_list = [\n        ll_dict[1] for ll_dict in\n        sorted(ll_dict_list_enumerated, key=itemgetter(0))\n    ]\n    plotdata = {}\n    keys = ll_dict_list[0].keys()  # look at first dict to fix keys\n    for k in keys:\n        z = -2 * np.array([ll_dict[k] for ll_dict in ll_dict_list]).reshape((steps, steps))\n        plotdata[k] = {'x': x, 'y': y, 'z': z}\n    return plotdata\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood._log_likelihood_2d","title":"<code>_log_likelihood_2d(xy_enumerated, gl, par_fct, scale_fct, par_dep_cov=False)</code>","text":"<p>Compute the likelihood on a 2D grid of 2 Wilson coefficients.</p> <p>This function is necessary because multiprocessing requires a picklable (i.e. top-level) object for parallel computation.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _log_likelihood_2d(xy_enumerated, gl, par_fct, scale_fct, par_dep_cov=False):\n    \"\"\"Compute the likelihood on a 2D grid of 2 Wilson coefficients.\n\n    This function is necessary because multiprocessing requires a picklable\n    (i.e. top-level) object for parallel computation.\n    \"\"\"\n    number, (x, y) = xy_enumerated\n    pp = gl.parameter_point(par_fct(x, y), scale_fct(x, y), par_dep_cov=par_dep_cov)\n    ll_dict = pp.log_likelihood_dict()\n    return (number, ll_dict)\n</code></pre>"},{"location":"jelli/core/global_likelihood/#jelli.core.global_likelihood._scale_fct_fixed","title":"<code>_scale_fct_fixed(*args, scale=0)</code>","text":"<p>This is a helper function that is necessary because multiprocessing requires a picklable (i.e. top-level) object for parallel computation.</p> Source code in <code>jelli/core/global_likelihood.py</code> <pre><code>def _scale_fct_fixed(*args, scale=0):\n    \"\"\"\n    This is a helper function that is necessary because multiprocessing requires\n    a picklable (i.e. top-level) object for parallel computation.\n    \"\"\"\n    return scale\n</code></pre>"},{"location":"jelli/core/global_likelihood_point/","title":"jelli.core.global_likelihood_point","text":""},{"location":"jelli/core/global_likelihood_point/#jelli.core.global_likelihood_point.GlobalLikelihoodPoint","title":"<code>GlobalLikelihoodPoint</code>","text":"<p>A class to represent a point in the parameter space of the global likelihood.</p> <p>Parameters:</p> Name Type Description Default <code>global_likelihood_instance</code> <code>GlobalLikelihood</code> <p>An instance of the <code>GlobalLikelihood</code> class.</p> required <code>par_array</code> <code>ndarray</code> <p>An array of parameter values.</p> required <code>scale</code> <code>float</code> <p>The scale at which the parameters are evaluated.</p> required <code>par_dep_cov</code> <code>bool</code> <p>If <code>True</code>, use parameter-dependent covariance matrices. Default is <code>False</code>.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>global_likelihood_instance</code> <code>GlobalLikelihood</code> <p>The instance of the <code>GlobalLikelihood</code> class.</p> <code>par_array</code> <code>ndarray</code> <p>The array of parameter values.</p> <code>scale</code> <code>float</code> <p>The scale at which the parameters are evaluated.</p> <code>par_dep_cov</code> <code>bool</code> <p>If <code>True</code>, use parameter-dependent covariance matrices.</p> <code>prediction_no_theory_uncertainty</code> <code>ndarray</code> <p>The predictions for observables without theory uncertainty.</p> <code>prediction_correlated</code> <code>List[ndarray]</code> <p>The predictions for observables with correlated uncertainties.</p> <code>log_likelihood_no_th_unc_univariate</code> <code>ndarray</code> <p>The log-likelihood contributions from observables without theory uncertainty with univariate constraints.</p> <code>log_likelihood_no_th_unc_multivariate</code> <code>List[ndarray]</code> <p>The log-likelihood contributions from observables without theory uncertainty with multivariate constraints.</p> <code>log_likelihood_correlated</code> <code>List[ndarray]</code> <p>The log-likelihood contributions from observables with correlated theoretical uncertainties.</p> <code>log_likelihood_summed</code> <code>ndarray</code> <p>The total log-likelihood summed over all observables.</p> <code>std_sm_exp_correlated_scaled</code> <code>List[ndarray]</code> <p>The scaled total standard deviations for correlated observables.</p> <code>_log_likelihood_dict</code> <code>dict</code> <p>A dictionary mapping likelihood names to their log-likelihood values.</p> <code>_chi2_dict</code> <code>dict</code> <p>A dictionary mapping likelihood names to their chi-squared values.</p> <code>_obstable_tree_cache</code> <code>defaultdict</code> <p>A cached tree structure for the observable table.</p> <p>Methods:</p> Name Description <code>log_likelihood_dict</code> <p>Returns a dictionary mapping likelihood names to their log-likelihood values.</p> <code>log_likelihood_global</code> <p>Returns the global log-likelihood value.</p> <code>chi2_dict</code> <p>Returns a dictionary mapping likelihood names to their chi-squared values.</p> <code>obstable</code> <p>Returns a pandas DataFrame representing the observable table with various filtering and sorting options.</p> <code>_obstable_tree</code> <p>Constructs and returns a tree structure for the observable table.</p> <code>_obstable_filter_sort</code> <p>Filters and sorts the observable table based on specified criteria.</p> <p>Examples:</p> <p>Initialize a <code>GlobalLikelihoodPoint</code> instance:</p> <pre><code>&gt;&gt;&gt; gl = GlobalLikelihood(...)\n&gt;&gt;&gt; glp = GlobalLikelihoodPoint(gl, par_array=np.array([0.1, 0.2]), scale=1000)\n</code></pre> <p>Access the log-likelihood dictionary:</p> <pre><code>&gt;&gt;&gt; log_likelihood_dict = glp.log_likelihood_dict()\n</code></pre> <p>Access the global log-likelihood value:</p> <pre><code>&gt;&gt;&gt; log_likelihood_global = glp.log_likelihood_global()\n</code></pre> <p>Access the chi-squared dictionary:</p> <pre><code>&gt;&gt;&gt; chi2_dict = glp.chi2_dict()\n</code></pre> <p>Get the observable table:</p> <pre><code>&gt;&gt;&gt; obstable_df = glp.obstable()\n</code></pre> Source code in <code>jelli/core/global_likelihood_point.py</code> <pre><code>class GlobalLikelihoodPoint:\n    '''\n    A class to represent a point in the parameter space of the global likelihood.\n\n    Parameters\n    ----------\n    global_likelihood_instance : GlobalLikelihood\n        An instance of the `GlobalLikelihood` class.\n    par_array : np.ndarray\n        An array of parameter values.\n    scale : float\n        The scale at which the parameters are evaluated.\n    par_dep_cov : bool, optional\n        If `True`, use parameter-dependent covariance matrices. Default is `False`.\n\n    Attributes\n    ----------\n    global_likelihood_instance : GlobalLikelihood\n        The instance of the `GlobalLikelihood` class.\n    par_array : np.ndarray\n        The array of parameter values.\n    scale : float\n        The scale at which the parameters are evaluated.\n    par_dep_cov : bool\n        If `True`, use parameter-dependent covariance matrices.\n    prediction_no_theory_uncertainty : np.ndarray\n        The predictions for observables without theory uncertainty.\n    prediction_correlated : List[np.ndarray]\n        The predictions for observables with correlated uncertainties.\n    log_likelihood_no_th_unc_univariate : np.ndarray\n        The log-likelihood contributions from observables without theory uncertainty with univariate constraints.\n    log_likelihood_no_th_unc_multivariate : List[np.ndarray]\n        The log-likelihood contributions from observables without theory uncertainty with multivariate constraints.\n    log_likelihood_correlated : List[np.ndarray]\n        The log-likelihood contributions from observables with correlated theoretical uncertainties.\n    log_likelihood_summed : np.ndarray\n        The total log-likelihood summed over all observables.\n    std_sm_exp_correlated_scaled : List[np.ndarray]\n        The scaled total standard deviations for correlated observables.\n    _log_likelihood_dict : dict\n        A dictionary mapping likelihood names to their log-likelihood values.\n    _chi2_dict : dict\n        A dictionary mapping likelihood names to their chi-squared values.\n    _obstable_tree_cache : defaultdict\n        A cached tree structure for the observable table.\n\n    Methods\n    -------\n    log_likelihood_dict()\n        Returns a dictionary mapping likelihood names to their log-likelihood values.\n    log_likelihood_global()\n        Returns the global log-likelihood value.\n    chi2_dict()\n        Returns a dictionary mapping likelihood names to their chi-squared values.\n    obstable(min_pull_exp=0, sort_by='pull exp.', ascending=None, min_val=None, max_val=None)\n        Returns a pandas DataFrame representing the observable table with various filtering and sorting options.\n    _obstable_tree()\n        Constructs and returns a tree structure for the observable table.\n    _obstable_filter_sort(info, sortkey='name', ascending=True, min_val=None, max_val=None, subset=None, max_rows=None)\n        Filters and sorts the observable table based on specified criteria.\n\n    Examples\n    --------\n    Initialize a `GlobalLikelihoodPoint` instance:\n\n    &gt;&gt;&gt; gl = GlobalLikelihood(...)\n    &gt;&gt;&gt; glp = GlobalLikelihoodPoint(gl, par_array=np.array([0.1, 0.2]), scale=1000)\n\n    Access the log-likelihood dictionary:\n\n    &gt;&gt;&gt; log_likelihood_dict = glp.log_likelihood_dict()\n\n    Access the global log-likelihood value:\n\n    &gt;&gt;&gt; log_likelihood_global = glp.log_likelihood_global()\n\n    Access the chi-squared dictionary:\n\n    &gt;&gt;&gt; chi2_dict = glp.chi2_dict()\n\n    Get the observable table:\n\n    &gt;&gt;&gt; obstable_df = glp.obstable()\n    '''\n\n    def __init__(self, global_likelihood_instance, par_array, scale, par_dep_cov=False):\n        '''\n        Initialize the `GlobalLikelihoodPoint` class.\n\n        Parameters\n        ----------\n        global_likelihood_instance : GlobalLikelihood\n            An instance of the `GlobalLikelihood` class.\n        par_array : np.ndarray\n            An array of parameter values.\n        scale : float\n            The scale at which the parameters are evaluated.\n        par_dep_cov : bool, optional\n            If `True`, use parameter-dependent covariance matrices. Default is `False`.\n\n        Attributes\n        ----------\n        global_likelihood_instance : GlobalLikelihood\n            The instance of the `GlobalLikelihood` class.\n        par_array : np.ndarray\n            The array of parameter values.\n        scale : float\n            The scale at which the parameters are evaluated.\n        par_dep_cov : bool\n            If `True`, use parameter-dependent covariance matrices.\n        prediction_no_theory_uncertainty : np.ndarray\n            The predictions for observables without theory uncertainty.\n        prediction_correlated : List[np.ndarray]\n            The predictions for observables with correlated uncertainties.\n        log_likelihood_no_th_unc_univariate : np.ndarray\n            The log-likelihood contributions from observables without theory uncertainty with univariate constraints.\n        log_likelihood_no_th_unc_multivariate : List[np.ndarray]\n            The log-likelihood contributions from observables without theory uncertainty with multivariate constraints.\n        log_likelihood_correlated : List[np.ndarray]\n            The log-likelihood contributions from observables with correlated theoretical uncertainties.\n        log_likelihood_summed : np.ndarray\n            The total log-likelihood summed over all observables.\n        std_sm_exp_correlated_scaled : List[np.ndarray]\n            The scaled total standard deviations for correlated observables.\n        _log_likelihood_dict : dict\n            A dictionary mapping likelihood names to their log-likelihood contributions.\n        _chi2_dict : dict\n            A dictionary mapping likelihood names to their chi-squared contributions.\n        _obstable_tree_cache : defaultdict\n            A cached tree structure for the observable table.\n\n        Examples\n        --------\n        Initialize a `GlobalLikelihoodPoint` instance:\n\n        &gt;&gt;&gt; gl = GlobalLikelihood(...)\n        &gt;&gt;&gt; glp = GlobalLikelihoodPoint(gl, par_array=np.array([0.1, 0.2]), scale=1000)\n        '''\n        self.global_likelihood_instance = global_likelihood_instance\n        self.par_array = par_array\n        self.scale = scale\n        self.par_dep_cov = par_dep_cov\n\n        (\n            self.prediction_no_theory_uncertainty,\n            self.prediction_correlated,\n            self.log_likelihood_no_th_unc_univariate,\n            self.log_likelihood_no_th_unc_multivariate,\n            self.log_likelihood_correlated,\n            self.log_likelihood_summed,\n            self.std_sm_exp_correlated_scaled,\n        ) = self.global_likelihood_instance._log_likelihood_point(\n            self.par_array,\n            self.scale,\n            par_dep_cov=self.par_dep_cov\n        )\n        self._log_likelihood_dict = None\n        self._chi2_dict = None\n        self._obstable_tree_cache = None\n\n    def log_likelihood_dict(self):\n        '''\n        Returns a dictionary mapping likelihood names to their log-likelihood values.\n\n        Returns\n        -------\n        dict\n            A dictionary where keys are likelihood names and values are their log-likelihood values.\n\n        Examples\n        --------\n        Access the log-likelihood dictionary:\n\n        &gt;&gt;&gt; log_likelihood_dict = glp.log_likelihood_dict()\n        '''\n        if self._log_likelihood_dict is None:\n            delta_log_likelihood = self.log_likelihood_summed - self.global_likelihood_instance.sm_log_likelihood_summed\n            self._log_likelihood_dict = dict(\n                zip(\n                    self.global_likelihood_instance.likelihoods,\n                    delta_log_likelihood\n                )\n            )\n        return self._log_likelihood_dict\n\n    def log_likelihood_global(self):\n        '''\n        Returns the global log-likelihood value.\n\n        Returns\n        -------\n        float\n            The global log-likelihood value.\n\n        Examples\n        --------\n        Access the global log-likelihood value:\n\n        &gt;&gt;&gt; log_likelihood_global = glp.log_likelihood_global()\n        '''\n        return self.log_likelihood_dict['global']\n\n    def chi2_dict(self):\n        '''\n        Returns a dictionary mapping likelihood names to their chi-squared values.\n\n        Returns\n        -------\n        dict\n            A dictionary where keys are likelihood names and values are their chi-squared values.\n\n        Examples\n        --------\n        Access the chi-squared dictionary:\n\n        &gt;&gt;&gt; chi2_dict = glp.chi2_dict()\n        '''\n        if self._chi2_dict is None:\n            chi2 = -2*self.log_likelihood_summed\n            self._chi2_dict = dict(\n                zip(\n                    self.global_likelihood_instance.likelihoods,\n                    chi2\n                )\n            )\n        return self._chi2_dict\n\n    def _obstable_tree(self):\n        '''\n        Constructs and returns a tree structure for the observable table.\n\n        Returns\n        -------\n        defaultdict\n            A tree structure containing observable names, constraints, predictions, uncertainties, and pulls.\n        '''\n        if self._obstable_tree_cache is None:\n            obstable_tree = tree()\n\n            (\n                log_likelihood_no_th_unc_multivariate,\n                log_likelihood_no_th_unc_multivariate_no_corr,\n                log_likelihood_correlated,\n                log_likelihood_correlated_no_corr,\n                exp_central_correlated,\n                std_th_exp_correlated,\n            ) = self.global_likelihood_instance._obstable(\n                self.prediction_no_theory_uncertainty,\n                self.prediction_correlated,\n                self.log_likelihood_no_th_unc_multivariate,\n                self.log_likelihood_correlated,\n                self.std_sm_exp_correlated_scaled,\n            )\n\n            pull_sm_no_theory_uncertainty_no_corr, pull_exp_no_theory_uncertainty_no_corr = compute_pulls(\n                self.log_likelihood_no_th_unc_univariate + log_likelihood_no_th_unc_multivariate_no_corr,\n                self.global_likelihood_instance.sm_log_likelihood_no_theory_uncertainty_no_corr\n            )\n\n            pull_sm_no_theory_uncertainty, pull_exp_no_theory_uncertainty = compute_pulls(\n                self.log_likelihood_no_th_unc_univariate + log_likelihood_no_th_unc_multivariate,\n                self.global_likelihood_instance.sm_log_likelihood_no_theory_uncertainty\n            )\n\n            # add no theory uncertainty observables\n            experimental_values_no_theory_uncertainty = self.global_likelihood_instance.experimental_values_no_theory_uncertainty\n            for i, obs_name in enumerate(self.global_likelihood_instance.observables_no_theory_uncertainty):\n                obstable_tree[obs_name] = {\n                    \"name\": obs_name,\n                    \"experiment\": experimental_values_no_theory_uncertainty[obs_name][0],\n                    \"exp. unc.\": experimental_values_no_theory_uncertainty[obs_name][1],\n                    \"theory\": self.prediction_no_theory_uncertainty[i],\n                    \"th. unc.\": 0.0,\n                    \"pull exp.\": pull_exp_no_theory_uncertainty_no_corr[i],\n                    \"pull SM\": pull_sm_no_theory_uncertainty_no_corr[i],\n                    \"pull exp. corr\": pull_exp_no_theory_uncertainty[i],\n                    \"pull SM corr\": pull_sm_no_theory_uncertainty[i],\n                }\n\n            # add correlated observables\n            for n_obs_sector, obs_names in enumerate(self.global_likelihood_instance.observables_correlated):\n                prediction_correlated = self.prediction_correlated[n_obs_sector][0]\n                pull_sm_correlated_no_corr, pull_exp_correlated_no_corr = compute_pulls(\n                    log_likelihood_correlated_no_corr[n_obs_sector],\n                    self.global_likelihood_instance.sm_log_likelihood_correlated_no_corr[n_obs_sector]\n                )\n                pull_sm_correlated, pull_exp_correlated = compute_pulls(\n                    log_likelihood_correlated[n_obs_sector],\n                    self.global_likelihood_instance.sm_log_likelihood_correlated[n_obs_sector]\n                )\n                experiment = exp_central_correlated[n_obs_sector]\n                std_th_exp = std_th_exp_correlated[n_obs_sector]\n                std_exp = self.global_likelihood_instance.std_exp[n_obs_sector]\n                std_th = std_th_exp*np.sqrt(1 - (std_exp/std_th_exp)**2)\n                for i, obs_name in enumerate(obs_names):\n                    obstable_tree[obs_name] = {\n                        \"name\": obs_name,\n                        \"experiment\": experiment[i],\n                        \"exp. unc.\": std_exp[i],\n                        \"theory\": prediction_correlated[i],\n                        \"th. unc.\": std_th[i],\n                        \"pull exp.\": pull_exp_correlated_no_corr[i],\n                        \"pull SM\": pull_sm_correlated_no_corr[i],\n                        \"pull exp. corr\": pull_exp_correlated[i],\n                        \"pull SM corr\": pull_sm_correlated[i],\n                    }\n\n            self._obstable_tree_cache = obstable_tree\n        return self._obstable_tree_cache\n\n    # TODO: this is mostly copy paste from smelli, we could think if something should be changed\n    def obstable(self, min_pull_exp=0, sort_by='pull exp.', ascending=None, min_val=None, max_val=None):\n        '''\n        Returns a pandas DataFrame representing the observable table with various filtering and sorting options. The table includes observable names, experimental values, uncertainties, theoretical predictions, and pulls.\n\n        Parameters\n        ----------\n        min_pull_exp : float, optional\n            Minimum absolute value of the experimental pull to include an observable. Default is `0`.\n        sort_by : str, optional\n            The column by which to sort the DataFrame. Options are `'name'`, `'exp. unc.'`, `'experiment'`, `'pull SM'`, `'pull exp.'`, `'th. unc.'`, `'theory'`, `'pull exp. corr'`, and `'pull SM corr'`. Default is `'pull exp.'`.\n        ascending : bool, optional\n            If `True`, sort in ascending order. If `False`, sort in descending order. If `None`, the default sorting order is used based on the `sort_by` parameter. Default is `None`.\n        min_val : float, optional\n            Minimum value for the `sort_by` column to include an observable. Default is `None`.\n        max_val : float, optional\n            Maximum value for the `sort_by` column to include an observable. Default is `None`.\n\n        Returns\n        -------\n        pd.DataFrame\n            A pandas DataFrame representing the observable table with the specified filtering and sorting applied.\n\n        Examples\n        --------\n        Get the observable table sorted by experimental pull in descending order:\n\n        &gt;&gt;&gt; obstable_df = glp.obstable(sort_by='pull exp.', ascending=False)\n        '''\n        sort_keys = ['name', 'exp. unc.', 'experiment', 'pull SM', 'pull exp.', 'th. unc.', 'theory', 'pull exp. corr', 'pull SM corr']\n        if sort_by not in sort_keys:\n            raise ValueError(\n                \"'{}' is not an allowed value for sort_by. Allowed values are \"\n                \"'{}', and '{}'.\".format(sort_by, \"', '\".join(sort_keys[:-1]),\n                                        sort_keys[-1])\n            )\n        subset = None\n        if sort_by == 'pull exp.':\n            # if sorted by pull exp., use descending order as default\n            if ascending is None:\n                ascending = False\n            if min_val is not None:\n                min_val = max(min_pull_exp, min_val)\n            else:\n                min_val = min_pull_exp\n        elif min_pull_exp != 0:\n            subset = lambda row: row['pull exp.'] &gt;= min_pull_exp\n        # if sorted not by pull exp., use ascending order as default\n        if ascending is None:\n            ascending = True\n        obstable_tree = self._obstable_filter_sort(\n            self._obstable_tree(),\n            sortkey=sort_by,\n            ascending=ascending,\n            min_val=min_val,\n            max_val=max_val,\n            subset=subset\n        )\n        df = pd.DataFrame(obstable_tree).T\n        if len(df) &gt;0:\n            del(df['name'])\n        return df\n\n    @staticmethod\n    def _obstable_filter_sort(info, sortkey='name', ascending=True, min_val=None, max_val=None, subset=None, max_rows=None):\n        '''\n        Filters and sorts the observable table based on specified criteria.\n\n        Parameters\n        ----------\n        info : dict\n            A dictionary containing observable information.\n        sortkey : str, optional\n            The key by which to sort the observables. Default is `'name'`.\n        ascending : bool, optional\n            If `True`, sort in ascending order. If `False`, sort in descending order. Default is `True`.\n        min_val : float, optional\n            Minimum value for the `sortkey` to include an observable. Default is `None`.\n        max_val : float, optional\n            Maximum value for the `sortkey` to include an observable. Default is `None`.\n        subset : callable, optional\n            A function that takes a row and returns `True` if the row should be included. Default is `None`.\n        max_rows : int, optional\n            Maximum number of rows to include in the output. If the number of rows exceeds this value, the output is split into multiple tables. Default is `None`.\n\n        Returns\n        -------\n        dict or list[dict]\n            A filtered and sorted dictionary of observables, or a list of such dictionaries if the number of rows exceeds `max_rows`.\n        '''\n        # impose min_val and max_val\n        if min_val is not None:\n            info = {obs:row for obs,row in info.items()\n                    if row[sortkey] &gt;= min_val}\n        if max_val is not None:\n            info = {obs:row for obs,row in info.items()\n                    if row[sortkey] &lt;= max_val}\n        # get only subset:\n        if subset is not None:\n            info = {obs:row for obs,row in info.items() if subset(row)}\n        # sort\n        info = OrderedDict(sorted(info.items(), key=lambda x: x[1][sortkey],\n                                reverse=(not ascending)))\n        # restrict number of rows per tabular to max_rows\n        if max_rows is None or len(info)&lt;=max_rows:\n            return info\n        else:\n            info_list = []\n            for n in range(ceil(len(info)/max_rows)):\n                info_n = OrderedDict((obs,row)\n                                     for i,(obs,row) in enumerate(info.items())\n                                     if i&gt;=n*max_rows and i&lt;(n+1)*max_rows)\n                info_list.append(info_n)\n            return info_list\n</code></pre>"},{"location":"jelli/core/global_likelihood_point/#jelli.core.global_likelihood_point.GlobalLikelihoodPoint.__init__","title":"<code>__init__(global_likelihood_instance, par_array, scale, par_dep_cov=False)</code>","text":"<p>Initialize the <code>GlobalLikelihoodPoint</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>global_likelihood_instance</code> <code>GlobalLikelihood</code> <p>An instance of the <code>GlobalLikelihood</code> class.</p> required <code>par_array</code> <code>ndarray</code> <p>An array of parameter values.</p> required <code>scale</code> <code>float</code> <p>The scale at which the parameters are evaluated.</p> required <code>par_dep_cov</code> <code>bool</code> <p>If <code>True</code>, use parameter-dependent covariance matrices. Default is <code>False</code>.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>global_likelihood_instance</code> <code>GlobalLikelihood</code> <p>The instance of the <code>GlobalLikelihood</code> class.</p> <code>par_array</code> <code>ndarray</code> <p>The array of parameter values.</p> <code>scale</code> <code>float</code> <p>The scale at which the parameters are evaluated.</p> <code>par_dep_cov</code> <code>bool</code> <p>If <code>True</code>, use parameter-dependent covariance matrices.</p> <code>prediction_no_theory_uncertainty</code> <code>ndarray</code> <p>The predictions for observables without theory uncertainty.</p> <code>prediction_correlated</code> <code>List[ndarray]</code> <p>The predictions for observables with correlated uncertainties.</p> <code>log_likelihood_no_th_unc_univariate</code> <code>ndarray</code> <p>The log-likelihood contributions from observables without theory uncertainty with univariate constraints.</p> <code>log_likelihood_no_th_unc_multivariate</code> <code>List[ndarray]</code> <p>The log-likelihood contributions from observables without theory uncertainty with multivariate constraints.</p> <code>log_likelihood_correlated</code> <code>List[ndarray]</code> <p>The log-likelihood contributions from observables with correlated theoretical uncertainties.</p> <code>log_likelihood_summed</code> <code>ndarray</code> <p>The total log-likelihood summed over all observables.</p> <code>std_sm_exp_correlated_scaled</code> <code>List[ndarray]</code> <p>The scaled total standard deviations for correlated observables.</p> <code>_log_likelihood_dict</code> <code>dict</code> <p>A dictionary mapping likelihood names to their log-likelihood contributions.</p> <code>_chi2_dict</code> <code>dict</code> <p>A dictionary mapping likelihood names to their chi-squared contributions.</p> <code>_obstable_tree_cache</code> <code>defaultdict</code> <p>A cached tree structure for the observable table.</p> <p>Examples:</p> <p>Initialize a <code>GlobalLikelihoodPoint</code> instance:</p> <pre><code>&gt;&gt;&gt; gl = GlobalLikelihood(...)\n&gt;&gt;&gt; glp = GlobalLikelihoodPoint(gl, par_array=np.array([0.1, 0.2]), scale=1000)\n</code></pre> Source code in <code>jelli/core/global_likelihood_point.py</code> <pre><code>def __init__(self, global_likelihood_instance, par_array, scale, par_dep_cov=False):\n    '''\n    Initialize the `GlobalLikelihoodPoint` class.\n\n    Parameters\n    ----------\n    global_likelihood_instance : GlobalLikelihood\n        An instance of the `GlobalLikelihood` class.\n    par_array : np.ndarray\n        An array of parameter values.\n    scale : float\n        The scale at which the parameters are evaluated.\n    par_dep_cov : bool, optional\n        If `True`, use parameter-dependent covariance matrices. Default is `False`.\n\n    Attributes\n    ----------\n    global_likelihood_instance : GlobalLikelihood\n        The instance of the `GlobalLikelihood` class.\n    par_array : np.ndarray\n        The array of parameter values.\n    scale : float\n        The scale at which the parameters are evaluated.\n    par_dep_cov : bool\n        If `True`, use parameter-dependent covariance matrices.\n    prediction_no_theory_uncertainty : np.ndarray\n        The predictions for observables without theory uncertainty.\n    prediction_correlated : List[np.ndarray]\n        The predictions for observables with correlated uncertainties.\n    log_likelihood_no_th_unc_univariate : np.ndarray\n        The log-likelihood contributions from observables without theory uncertainty with univariate constraints.\n    log_likelihood_no_th_unc_multivariate : List[np.ndarray]\n        The log-likelihood contributions from observables without theory uncertainty with multivariate constraints.\n    log_likelihood_correlated : List[np.ndarray]\n        The log-likelihood contributions from observables with correlated theoretical uncertainties.\n    log_likelihood_summed : np.ndarray\n        The total log-likelihood summed over all observables.\n    std_sm_exp_correlated_scaled : List[np.ndarray]\n        The scaled total standard deviations for correlated observables.\n    _log_likelihood_dict : dict\n        A dictionary mapping likelihood names to their log-likelihood contributions.\n    _chi2_dict : dict\n        A dictionary mapping likelihood names to their chi-squared contributions.\n    _obstable_tree_cache : defaultdict\n        A cached tree structure for the observable table.\n\n    Examples\n    --------\n    Initialize a `GlobalLikelihoodPoint` instance:\n\n    &gt;&gt;&gt; gl = GlobalLikelihood(...)\n    &gt;&gt;&gt; glp = GlobalLikelihoodPoint(gl, par_array=np.array([0.1, 0.2]), scale=1000)\n    '''\n    self.global_likelihood_instance = global_likelihood_instance\n    self.par_array = par_array\n    self.scale = scale\n    self.par_dep_cov = par_dep_cov\n\n    (\n        self.prediction_no_theory_uncertainty,\n        self.prediction_correlated,\n        self.log_likelihood_no_th_unc_univariate,\n        self.log_likelihood_no_th_unc_multivariate,\n        self.log_likelihood_correlated,\n        self.log_likelihood_summed,\n        self.std_sm_exp_correlated_scaled,\n    ) = self.global_likelihood_instance._log_likelihood_point(\n        self.par_array,\n        self.scale,\n        par_dep_cov=self.par_dep_cov\n    )\n    self._log_likelihood_dict = None\n    self._chi2_dict = None\n    self._obstable_tree_cache = None\n</code></pre>"},{"location":"jelli/core/global_likelihood_point/#jelli.core.global_likelihood_point.GlobalLikelihoodPoint._obstable_filter_sort","title":"<code>_obstable_filter_sort(info, sortkey='name', ascending=True, min_val=None, max_val=None, subset=None, max_rows=None)</code>  <code>staticmethod</code>","text":"<p>Filters and sorts the observable table based on specified criteria.</p> <p>Parameters:</p> Name Type Description Default <code>info</code> <code>dict</code> <p>A dictionary containing observable information.</p> required <code>sortkey</code> <code>str</code> <p>The key by which to sort the observables. Default is <code>'name'</code>.</p> <code>'name'</code> <code>ascending</code> <code>bool</code> <p>If <code>True</code>, sort in ascending order. If <code>False</code>, sort in descending order. Default is <code>True</code>.</p> <code>True</code> <code>min_val</code> <code>float</code> <p>Minimum value for the <code>sortkey</code> to include an observable. Default is <code>None</code>.</p> <code>None</code> <code>max_val</code> <code>float</code> <p>Maximum value for the <code>sortkey</code> to include an observable. Default is <code>None</code>.</p> <code>None</code> <code>subset</code> <code>callable</code> <p>A function that takes a row and returns <code>True</code> if the row should be included. Default is <code>None</code>.</p> <code>None</code> <code>max_rows</code> <code>int</code> <p>Maximum number of rows to include in the output. If the number of rows exceeds this value, the output is split into multiple tables. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict or list[dict]</code> <p>A filtered and sorted dictionary of observables, or a list of such dictionaries if the number of rows exceeds <code>max_rows</code>.</p> Source code in <code>jelli/core/global_likelihood_point.py</code> <pre><code>@staticmethod\ndef _obstable_filter_sort(info, sortkey='name', ascending=True, min_val=None, max_val=None, subset=None, max_rows=None):\n    '''\n    Filters and sorts the observable table based on specified criteria.\n\n    Parameters\n    ----------\n    info : dict\n        A dictionary containing observable information.\n    sortkey : str, optional\n        The key by which to sort the observables. Default is `'name'`.\n    ascending : bool, optional\n        If `True`, sort in ascending order. If `False`, sort in descending order. Default is `True`.\n    min_val : float, optional\n        Minimum value for the `sortkey` to include an observable. Default is `None`.\n    max_val : float, optional\n        Maximum value for the `sortkey` to include an observable. Default is `None`.\n    subset : callable, optional\n        A function that takes a row and returns `True` if the row should be included. Default is `None`.\n    max_rows : int, optional\n        Maximum number of rows to include in the output. If the number of rows exceeds this value, the output is split into multiple tables. Default is `None`.\n\n    Returns\n    -------\n    dict or list[dict]\n        A filtered and sorted dictionary of observables, or a list of such dictionaries if the number of rows exceeds `max_rows`.\n    '''\n    # impose min_val and max_val\n    if min_val is not None:\n        info = {obs:row for obs,row in info.items()\n                if row[sortkey] &gt;= min_val}\n    if max_val is not None:\n        info = {obs:row for obs,row in info.items()\n                if row[sortkey] &lt;= max_val}\n    # get only subset:\n    if subset is not None:\n        info = {obs:row for obs,row in info.items() if subset(row)}\n    # sort\n    info = OrderedDict(sorted(info.items(), key=lambda x: x[1][sortkey],\n                            reverse=(not ascending)))\n    # restrict number of rows per tabular to max_rows\n    if max_rows is None or len(info)&lt;=max_rows:\n        return info\n    else:\n        info_list = []\n        for n in range(ceil(len(info)/max_rows)):\n            info_n = OrderedDict((obs,row)\n                                 for i,(obs,row) in enumerate(info.items())\n                                 if i&gt;=n*max_rows and i&lt;(n+1)*max_rows)\n            info_list.append(info_n)\n        return info_list\n</code></pre>"},{"location":"jelli/core/global_likelihood_point/#jelli.core.global_likelihood_point.GlobalLikelihoodPoint._obstable_tree","title":"<code>_obstable_tree()</code>","text":"<p>Constructs and returns a tree structure for the observable table.</p> <p>Returns:</p> Type Description <code>defaultdict</code> <p>A tree structure containing observable names, constraints, predictions, uncertainties, and pulls.</p> Source code in <code>jelli/core/global_likelihood_point.py</code> <pre><code>def _obstable_tree(self):\n    '''\n    Constructs and returns a tree structure for the observable table.\n\n    Returns\n    -------\n    defaultdict\n        A tree structure containing observable names, constraints, predictions, uncertainties, and pulls.\n    '''\n    if self._obstable_tree_cache is None:\n        obstable_tree = tree()\n\n        (\n            log_likelihood_no_th_unc_multivariate,\n            log_likelihood_no_th_unc_multivariate_no_corr,\n            log_likelihood_correlated,\n            log_likelihood_correlated_no_corr,\n            exp_central_correlated,\n            std_th_exp_correlated,\n        ) = self.global_likelihood_instance._obstable(\n            self.prediction_no_theory_uncertainty,\n            self.prediction_correlated,\n            self.log_likelihood_no_th_unc_multivariate,\n            self.log_likelihood_correlated,\n            self.std_sm_exp_correlated_scaled,\n        )\n\n        pull_sm_no_theory_uncertainty_no_corr, pull_exp_no_theory_uncertainty_no_corr = compute_pulls(\n            self.log_likelihood_no_th_unc_univariate + log_likelihood_no_th_unc_multivariate_no_corr,\n            self.global_likelihood_instance.sm_log_likelihood_no_theory_uncertainty_no_corr\n        )\n\n        pull_sm_no_theory_uncertainty, pull_exp_no_theory_uncertainty = compute_pulls(\n            self.log_likelihood_no_th_unc_univariate + log_likelihood_no_th_unc_multivariate,\n            self.global_likelihood_instance.sm_log_likelihood_no_theory_uncertainty\n        )\n\n        # add no theory uncertainty observables\n        experimental_values_no_theory_uncertainty = self.global_likelihood_instance.experimental_values_no_theory_uncertainty\n        for i, obs_name in enumerate(self.global_likelihood_instance.observables_no_theory_uncertainty):\n            obstable_tree[obs_name] = {\n                \"name\": obs_name,\n                \"experiment\": experimental_values_no_theory_uncertainty[obs_name][0],\n                \"exp. unc.\": experimental_values_no_theory_uncertainty[obs_name][1],\n                \"theory\": self.prediction_no_theory_uncertainty[i],\n                \"th. unc.\": 0.0,\n                \"pull exp.\": pull_exp_no_theory_uncertainty_no_corr[i],\n                \"pull SM\": pull_sm_no_theory_uncertainty_no_corr[i],\n                \"pull exp. corr\": pull_exp_no_theory_uncertainty[i],\n                \"pull SM corr\": pull_sm_no_theory_uncertainty[i],\n            }\n\n        # add correlated observables\n        for n_obs_sector, obs_names in enumerate(self.global_likelihood_instance.observables_correlated):\n            prediction_correlated = self.prediction_correlated[n_obs_sector][0]\n            pull_sm_correlated_no_corr, pull_exp_correlated_no_corr = compute_pulls(\n                log_likelihood_correlated_no_corr[n_obs_sector],\n                self.global_likelihood_instance.sm_log_likelihood_correlated_no_corr[n_obs_sector]\n            )\n            pull_sm_correlated, pull_exp_correlated = compute_pulls(\n                log_likelihood_correlated[n_obs_sector],\n                self.global_likelihood_instance.sm_log_likelihood_correlated[n_obs_sector]\n            )\n            experiment = exp_central_correlated[n_obs_sector]\n            std_th_exp = std_th_exp_correlated[n_obs_sector]\n            std_exp = self.global_likelihood_instance.std_exp[n_obs_sector]\n            std_th = std_th_exp*np.sqrt(1 - (std_exp/std_th_exp)**2)\n            for i, obs_name in enumerate(obs_names):\n                obstable_tree[obs_name] = {\n                    \"name\": obs_name,\n                    \"experiment\": experiment[i],\n                    \"exp. unc.\": std_exp[i],\n                    \"theory\": prediction_correlated[i],\n                    \"th. unc.\": std_th[i],\n                    \"pull exp.\": pull_exp_correlated_no_corr[i],\n                    \"pull SM\": pull_sm_correlated_no_corr[i],\n                    \"pull exp. corr\": pull_exp_correlated[i],\n                    \"pull SM corr\": pull_sm_correlated[i],\n                }\n\n        self._obstable_tree_cache = obstable_tree\n    return self._obstable_tree_cache\n</code></pre>"},{"location":"jelli/core/global_likelihood_point/#jelli.core.global_likelihood_point.GlobalLikelihoodPoint.chi2_dict","title":"<code>chi2_dict()</code>","text":"<p>Returns a dictionary mapping likelihood names to their chi-squared values.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where keys are likelihood names and values are their chi-squared values.</p> <p>Examples:</p> <p>Access the chi-squared dictionary:</p> <pre><code>&gt;&gt;&gt; chi2_dict = glp.chi2_dict()\n</code></pre> Source code in <code>jelli/core/global_likelihood_point.py</code> <pre><code>def chi2_dict(self):\n    '''\n    Returns a dictionary mapping likelihood names to their chi-squared values.\n\n    Returns\n    -------\n    dict\n        A dictionary where keys are likelihood names and values are their chi-squared values.\n\n    Examples\n    --------\n    Access the chi-squared dictionary:\n\n    &gt;&gt;&gt; chi2_dict = glp.chi2_dict()\n    '''\n    if self._chi2_dict is None:\n        chi2 = -2*self.log_likelihood_summed\n        self._chi2_dict = dict(\n            zip(\n                self.global_likelihood_instance.likelihoods,\n                chi2\n            )\n        )\n    return self._chi2_dict\n</code></pre>"},{"location":"jelli/core/global_likelihood_point/#jelli.core.global_likelihood_point.GlobalLikelihoodPoint.log_likelihood_dict","title":"<code>log_likelihood_dict()</code>","text":"<p>Returns a dictionary mapping likelihood names to their log-likelihood values.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary where keys are likelihood names and values are their log-likelihood values.</p> <p>Examples:</p> <p>Access the log-likelihood dictionary:</p> <pre><code>&gt;&gt;&gt; log_likelihood_dict = glp.log_likelihood_dict()\n</code></pre> Source code in <code>jelli/core/global_likelihood_point.py</code> <pre><code>def log_likelihood_dict(self):\n    '''\n    Returns a dictionary mapping likelihood names to their log-likelihood values.\n\n    Returns\n    -------\n    dict\n        A dictionary where keys are likelihood names and values are their log-likelihood values.\n\n    Examples\n    --------\n    Access the log-likelihood dictionary:\n\n    &gt;&gt;&gt; log_likelihood_dict = glp.log_likelihood_dict()\n    '''\n    if self._log_likelihood_dict is None:\n        delta_log_likelihood = self.log_likelihood_summed - self.global_likelihood_instance.sm_log_likelihood_summed\n        self._log_likelihood_dict = dict(\n            zip(\n                self.global_likelihood_instance.likelihoods,\n                delta_log_likelihood\n            )\n        )\n    return self._log_likelihood_dict\n</code></pre>"},{"location":"jelli/core/global_likelihood_point/#jelli.core.global_likelihood_point.GlobalLikelihoodPoint.log_likelihood_global","title":"<code>log_likelihood_global()</code>","text":"<p>Returns the global log-likelihood value.</p> <p>Returns:</p> Type Description <code>float</code> <p>The global log-likelihood value.</p> <p>Examples:</p> <p>Access the global log-likelihood value:</p> <pre><code>&gt;&gt;&gt; log_likelihood_global = glp.log_likelihood_global()\n</code></pre> Source code in <code>jelli/core/global_likelihood_point.py</code> <pre><code>def log_likelihood_global(self):\n    '''\n    Returns the global log-likelihood value.\n\n    Returns\n    -------\n    float\n        The global log-likelihood value.\n\n    Examples\n    --------\n    Access the global log-likelihood value:\n\n    &gt;&gt;&gt; log_likelihood_global = glp.log_likelihood_global()\n    '''\n    return self.log_likelihood_dict['global']\n</code></pre>"},{"location":"jelli/core/global_likelihood_point/#jelli.core.global_likelihood_point.GlobalLikelihoodPoint.obstable","title":"<code>obstable(min_pull_exp=0, sort_by='pull exp.', ascending=None, min_val=None, max_val=None)</code>","text":"<p>Returns a pandas DataFrame representing the observable table with various filtering and sorting options. The table includes observable names, experimental values, uncertainties, theoretical predictions, and pulls.</p> <p>Parameters:</p> Name Type Description Default <code>min_pull_exp</code> <code>float</code> <p>Minimum absolute value of the experimental pull to include an observable. Default is <code>0</code>.</p> <code>0</code> <code>sort_by</code> <code>str</code> <p>The column by which to sort the DataFrame. Options are <code>'name'</code>, <code>'exp. unc.'</code>, <code>'experiment'</code>, <code>'pull SM'</code>, <code>'pull exp.'</code>, <code>'th. unc.'</code>, <code>'theory'</code>, <code>'pull exp. corr'</code>, and <code>'pull SM corr'</code>. Default is <code>'pull exp.'</code>.</p> <code>'pull exp.'</code> <code>ascending</code> <code>bool</code> <p>If <code>True</code>, sort in ascending order. If <code>False</code>, sort in descending order. If <code>None</code>, the default sorting order is used based on the <code>sort_by</code> parameter. Default is <code>None</code>.</p> <code>None</code> <code>min_val</code> <code>float</code> <p>Minimum value for the <code>sort_by</code> column to include an observable. Default is <code>None</code>.</p> <code>None</code> <code>max_val</code> <code>float</code> <p>Maximum value for the <code>sort_by</code> column to include an observable. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>A pandas DataFrame representing the observable table with the specified filtering and sorting applied.</p> <p>Examples:</p> <p>Get the observable table sorted by experimental pull in descending order:</p> <pre><code>&gt;&gt;&gt; obstable_df = glp.obstable(sort_by='pull exp.', ascending=False)\n</code></pre> Source code in <code>jelli/core/global_likelihood_point.py</code> <pre><code>def obstable(self, min_pull_exp=0, sort_by='pull exp.', ascending=None, min_val=None, max_val=None):\n    '''\n    Returns a pandas DataFrame representing the observable table with various filtering and sorting options. The table includes observable names, experimental values, uncertainties, theoretical predictions, and pulls.\n\n    Parameters\n    ----------\n    min_pull_exp : float, optional\n        Minimum absolute value of the experimental pull to include an observable. Default is `0`.\n    sort_by : str, optional\n        The column by which to sort the DataFrame. Options are `'name'`, `'exp. unc.'`, `'experiment'`, `'pull SM'`, `'pull exp.'`, `'th. unc.'`, `'theory'`, `'pull exp. corr'`, and `'pull SM corr'`. Default is `'pull exp.'`.\n    ascending : bool, optional\n        If `True`, sort in ascending order. If `False`, sort in descending order. If `None`, the default sorting order is used based on the `sort_by` parameter. Default is `None`.\n    min_val : float, optional\n        Minimum value for the `sort_by` column to include an observable. Default is `None`.\n    max_val : float, optional\n        Maximum value for the `sort_by` column to include an observable. Default is `None`.\n\n    Returns\n    -------\n    pd.DataFrame\n        A pandas DataFrame representing the observable table with the specified filtering and sorting applied.\n\n    Examples\n    --------\n    Get the observable table sorted by experimental pull in descending order:\n\n    &gt;&gt;&gt; obstable_df = glp.obstable(sort_by='pull exp.', ascending=False)\n    '''\n    sort_keys = ['name', 'exp. unc.', 'experiment', 'pull SM', 'pull exp.', 'th. unc.', 'theory', 'pull exp. corr', 'pull SM corr']\n    if sort_by not in sort_keys:\n        raise ValueError(\n            \"'{}' is not an allowed value for sort_by. Allowed values are \"\n            \"'{}', and '{}'.\".format(sort_by, \"', '\".join(sort_keys[:-1]),\n                                    sort_keys[-1])\n        )\n    subset = None\n    if sort_by == 'pull exp.':\n        # if sorted by pull exp., use descending order as default\n        if ascending is None:\n            ascending = False\n        if min_val is not None:\n            min_val = max(min_pull_exp, min_val)\n        else:\n            min_val = min_pull_exp\n    elif min_pull_exp != 0:\n        subset = lambda row: row['pull exp.'] &gt;= min_pull_exp\n    # if sorted not by pull exp., use ascending order as default\n    if ascending is None:\n        ascending = True\n    obstable_tree = self._obstable_filter_sort(\n        self._obstable_tree(),\n        sortkey=sort_by,\n        ascending=ascending,\n        min_val=min_val,\n        max_val=max_val,\n        subset=subset\n    )\n    df = pd.DataFrame(obstable_tree).T\n    if len(df) &gt;0:\n        del(df['name'])\n    return df\n</code></pre>"},{"location":"jelli/core/global_likelihood_point/#jelli.core.global_likelihood_point.compute_pulls","title":"<code>compute_pulls(log_likelihood, log_likelihood_sm)</code>","text":"<p>Computes the pulls with respect to the Standard Model and the central experimental likelihoods.</p> <p>Parameters:</p> Name Type Description Default <code>log_likelihood</code> <code>ndarray</code> <p>The log-likelihood values for the observables.</p> required <code>log_likelihood_sm</code> <code>ndarray</code> <p>The Standard Model log-likelihood values for the observables.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing two np.ndarrays: the pulls with respect to the Standard Model and the pulls with respect to the central experimental values.</p> <p>Examples:</p> <p>Compute the pulls for given log-likelihood values:</p> <pre><code>&gt;&gt;&gt; pulls_sm, pulls_exp = compute_pulls(log_likelihood, log_likelihood_sm)\n</code></pre> Source code in <code>jelli/core/global_likelihood_point.py</code> <pre><code>def compute_pulls(log_likelihood, log_likelihood_sm):\n    '''\n    Computes the pulls with respect to the Standard Model and the central experimental likelihoods.\n\n    Parameters\n    ----------\n    log_likelihood : np.ndarray\n        The log-likelihood values for the observables.\n    log_likelihood_sm : np.ndarray\n        The Standard Model log-likelihood values for the observables.\n\n    Returns\n    -------\n    tuple\n        A tuple containing two np.ndarrays: the pulls with respect to the Standard Model and the pulls with respect to the central experimental values.\n\n    Examples\n    --------\n    Compute the pulls for given log-likelihood values:\n\n    &gt;&gt;&gt; pulls_sm, pulls_exp = compute_pulls(log_likelihood, log_likelihood_sm)\n    '''\n    s = np.where(log_likelihood &gt; log_likelihood_sm, -1, 1)\n    pull_sm = s * np.sqrt(np.abs(-2 * (log_likelihood - log_likelihood_sm)))\n    pull_exp = np.sqrt(np.abs(-2 * log_likelihood))\n    return pull_sm, pull_exp\n</code></pre>"},{"location":"jelli/core/global_likelihood_point/#jelli.core.global_likelihood_point.tree","title":"<code>tree()</code>","text":"<p>Creates a new tree structure.</p> <p>Returns:</p> Type Description <code>defaultdict</code> <p>A tree structure implemented as a nested defaultdict.</p> <p>Examples:</p> <p>Create a new tree structure:</p> <pre><code>&gt;&gt;&gt; my_tree = tree()\n</code></pre> Source code in <code>jelli/core/global_likelihood_point.py</code> <pre><code>def tree():\n    '''\n    Creates a new tree structure.\n\n    Returns\n    -------\n    defaultdict\n        A tree structure implemented as a nested defaultdict.\n\n    Examples\n    --------\n    Create a new tree structure:\n\n    &gt;&gt;&gt; my_tree = tree()\n    '''\n    return defaultdict(tree)\n</code></pre>"},{"location":"jelli/core/measurement/","title":"jelli.core.measurement","text":""},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement","title":"<code>Measurement</code>","text":"<p>Class to store measurements and constraints on observables.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the measurement.</p> required <code>constraints</code> <code>list[dict]</code> <p>List of constraints on observables. Each constraint is a dictionary with the following keys:</p> <ul> <li> <p><code>type</code> (<code>str</code>):     Type of the distribution. Can be <code>NormalDistribution</code>, <code>HalfNormalDistribution</code>, <code>GammaDistributionPositive</code>, <code>NumericalDistribution</code>, <code>MultivariateNormalDistribution</code>.</p> </li> <li> <p><code>observables</code> (<code>list[str]</code>):     List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a \\(q^2\\) value or \\(q^2\\) range).</p> </li> <li> <p><code>parameters</code> (<code>dict</code>):     Parameters of the distribution. The keys depend on the type of the distribution as follows:</p> <ul> <li><code>NormalDistribution</code>: <code>central_value</code> (<code>float</code>), <code>standard_deviation</code> (<code>float</code>)</li> <li><code>HalfNormalDistribution</code>: <code>central_value</code> (<code>float</code>), <code>standard_deviation</code> (<code>float</code>)</li> <li><code>GammaDistributionPositive</code>: <code>a</code> (<code>float</code>), <code>loc</code> (<code>float</code>), <code>scale</code> (<code>float</code>)</li> <li><code>NumericalDistribution</code>: <code>x</code> (<code>list[float]</code>), <code>y</code> (<code>list[float]</code>)</li> <li><code>MultivariateNormalDistribution</code>: <code>central_value</code> (<code>list[float]</code>), <code>covariance</code> (<code>list[list[float]]</code>)</li> </ul> </li> </ul> required <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Name of the measurement.</p> <code>constraints</code> <code>list[dict]</code> <p>List of constraints on observables. Each constraint is a dictionary with the following keys:</p> <ul> <li> <p><code>observables</code> (<code>list[str]</code>):     List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a \\(q^2\\) value or \\(q^2\\) range).</p> </li> <li> <p><code>distribution_type</code> (<code>str</code>):     Type of the distribution. Can be <code>NormalDistribution</code>, <code>HalfNormalDistribution</code>, <code>GammaDistributionPositive</code>, <code>NumericalDistribution</code>, <code>MultivariateNormalDistribution</code>.</p> </li> <li> <p><code>parameters</code> (<code>dict</code>)     Parameters of the distribution. The keys depend on the type of the distribution as follows:</p> <ul> <li><code>NormalDistribution</code>: <code>central_value</code> (<code>float</code>), <code>standard_deviation</code> (<code>float</code>)</li> <li><code>HalfNormalDistribution</code>: <code>central_value</code> (<code>float</code>), <code>standard_deviation</code> (<code>float</code>)</li> <li><code>GammaDistributionPositive</code>: <code>a</code> (<code>float</code>), <code>loc</code> (<code>float</code>), <code>scale</code> (<code>float</code>)</li> <li><code>NumericalDistribution</code>: <code>x</code> (<code>list[float]</code>), <code>y</code> (<code>list[float]</code>)</li> <li><code>MultivariateNormalDistribution</code>: <code>central_value</code> (<code>list[float]</code>), <code>covariance</code> (<code>list[list[float]]</code>)</li> </ul> </li> </ul> <code>constrained_observables</code> <code>set</code> <p>Set of observables that the measurement constrains</p> <p>Methods:</p> Name Description <code>get_all_measurements</code> <p>Return all measurements.</p> <code>get_all_observables</code> <p>Return all observables.</p> <code>get_measurements</code> <p>Return measurements that constrain the specified observables.</p> <code>get_constraints</code> <p>Return constraints on the specified observables.</p> <code>get_combined_constraints</code> <p>Return combined constraints on the specified observables.</p> <code>load</code> <p>Load measurements from a json file or a directory containing json files</p> <code>unload</code> <p>Unload measurements.</p> <code>clear</code> <p>Clear all measurements.</p> <p>Examples:</p> <p>Load measurements from a json file:</p> <pre><code>&gt;&gt;&gt; Measurement.load('measurements.json')\n</code></pre> <p>Get all measurements:</p> <pre><code>&gt;&gt;&gt; Measurement.get_all_measurements()\n{'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n</code></pre> <p>Get all observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_all_observables()\n{'observable1', 'observable2', ...}\n</code></pre> <p>Get measurements that contain the specified observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_measurements({'observable1', 'observable2'})\n{'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n</code></pre> <p>Get constraints on the specified observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_constraints({'observable1', 'observable2'})\n{'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n</code></pre> <p>Get combined constraints on the specified observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_combined_constraints({'observable1', 'observable2'})\n{'NormalDistribution': {'measurement_name': ['measurement1'], 'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n</code></pre> <p>Unload measurements:</p> <pre><code>&gt;&gt;&gt; Measurement.unload(['measurement1', 'measurement2'])\n</code></pre> <p>Clear all measurements:</p> <pre><code>&gt;&gt;&gt; Measurement.clear()\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>class Measurement:\n    '''\n    Class to store measurements and constraints on observables.\n\n    Parameters\n    ----------\n    name : str\n        Name of the measurement.\n    constraints : list[dict]\n        List of constraints on observables. Each constraint is a dictionary with the following keys:\n\n          - `type` (`str`):\n            Type of the distribution. Can be `NormalDistribution`, `HalfNormalDistribution`, `GammaDistributionPositive`, `NumericalDistribution`, `MultivariateNormalDistribution`.\n\n          - `observables` (`list[str]`):\n            List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a $q^2$ value or $q^2$ range).\n\n          - `parameters` (`dict`):\n            Parameters of the distribution. The keys depend on the type of the distribution as follows:\n              - `NormalDistribution`: `central_value` (`float`), `standard_deviation` (`float`)\n              - `HalfNormalDistribution`: `central_value` (`float`), `standard_deviation` (`float`)\n              - `GammaDistributionPositive`: `a` (`float`), `loc` (`float`), `scale` (`float`)\n              - `NumericalDistribution`: `x` (`list[float]`), `y` (`list[float]`)\n              - `MultivariateNormalDistribution`: `central_value` (`list[float]`), `covariance` (`list[list[float]]`)\n\n    Attributes\n    ----------\n    name : str\n        Name of the measurement.\n    constraints : list[dict]\n        List of constraints on observables. Each constraint is a dictionary with the following keys:\n\n          - `observables` (`list[str]`):\n            List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a $q^2$ value or $q^2$ range).\n\n          - `distribution_type` (`str`):\n            Type of the distribution. Can be `NormalDistribution`, `HalfNormalDistribution`, `GammaDistributionPositive`, `NumericalDistribution`, `MultivariateNormalDistribution`.\n\n          - `parameters` (`dict`)\n            Parameters of the distribution. The keys depend on the type of the distribution as follows:\n              - `NormalDistribution`: `central_value` (`float`), `standard_deviation` (`float`)\n              - `HalfNormalDistribution`: `central_value` (`float`), `standard_deviation` (`float`)\n              - `GammaDistributionPositive`: `a` (`float`), `loc` (`float`), `scale` (`float`)\n              - `NumericalDistribution`: `x` (`list[float]`), `y` (`list[float]`)\n              - `MultivariateNormalDistribution`: `central_value` (`list[float]`), `covariance` (`list[list[float]]`)\n    constrained_observables : set\n        Set of observables that the measurement constrains\n\n    Methods\n    -------\n    get_all_measurements()\n        Return all measurements.\n    get_all_observables()\n        Return all observables.\n    get_measurements(observables)\n        Return measurements that constrain the specified observables.\n    get_constraints(observables)\n        Return constraints on the specified observables.\n    get_combined_constraints(observables)\n        Return combined constraints on the specified observables.\n    load(path)\n        Load measurements from a json file or a directory containing json files\n    unload(measurement_names)\n        Unload measurements.\n    clear()\n        Clear all measurements.\n\n    Examples\n    --------\n    Load measurements from a json file:\n\n    &gt;&gt;&gt; Measurement.load('measurements.json')\n\n    Get all measurements:\n\n    &gt;&gt;&gt; Measurement.get_all_measurements()\n    {'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n\n    Get all observables:\n\n    &gt;&gt;&gt; Measurement.get_all_observables()\n    {'observable1', 'observable2', ...}\n\n    Get measurements that contain the specified observables:\n\n    &gt;&gt;&gt; Measurement.get_measurements({'observable1', 'observable2'})\n    {'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n\n    Get constraints on the specified observables:\n\n    &gt;&gt;&gt; Measurement.get_constraints({'observable1', 'observable2'})\n    {'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n\n    Get combined constraints on the specified observables:\n\n    &gt;&gt;&gt; Measurement.get_combined_constraints({'observable1', 'observable2'})\n    {'NormalDistribution': {'measurement_name': ['measurement1'], 'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n\n    Unload measurements:\n\n    &gt;&gt;&gt; Measurement.unload(['measurement1', 'measurement2'])\n\n    Clear all measurements:\n\n    &gt;&gt;&gt; Measurement.clear()\n    '''\n\n    _measurements: Dict[str, 'Measurement'] = {}  # Class attribute to store all measurements\n    _observable_to_measurements: DefaultDict[str, Set[str]] = defaultdict(set)  # Class attribute to map observables to measurements\n    _pdfxf_versions = ['1.0'] # List of supported versions of the pdfxf JSON schema\n\n    def __init__(self, name: str, constraints: List[dict]):\n        '''\n        Initialize a Measurement object.\n\n        Parameters\n        ----------\n        name : str\n            Name of the measurement.\n        constraints : list[dict]\n            List of constraints on observables. Each constraint is a dictionary with the following keys:\n\n              - `type` (`str`)\n                Type of the distribution. Can be `NormalDistribution`, `HalfNormalDistribution`, `GammaDistributionPositive`, `NumericalDistribution`, `MultivariateNormalDistribution`.\n\n              - `observables` (`list[str]`)\n                List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a $q^2$ value or $q^2$ range).\n\n              - `parameters` (`dict`)\n                Parameters of the distribution. The keys depend on the type of the distribution as follows:\n                  - `NormalDistribution`: `central_value` (`float`), `standard_deviation` (`float`)\n                  - `HalfNormalDistribution`: `central_value` (`float`), `standard_deviation` (`float`)\n                  - `GammaDistributionPositive`: `a` (`float`), `loc` (`float`), `scale` (`float`)\n                  - `NumericalDistribution`: `x` (`list[float]`), `y` (`list[float]`)\n                  - `MultivariateNormalDistribution`: `central_value` (`list[float]`), `covariance` (`list[list[float]]`)\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Initialize a Measurement object:\n\n        &gt;&gt;&gt; Measurement('measurement1', [{'type': 'NormalDistribution', 'observables': ['observable1'], 'parameters': {'central_value': 0.0, 'standard_deviation': 1.0}}])\n        '''\n        self.name: str = name\n        self.constraints: list[dict] = []\n        for constraint in constraints:\n\n            # Convert list of observable names to numpy array containing strings\n            constraint['observables'] = np.array(constraint['observables'], dtype=object)\n\n            # Add measurement name to `_observable_to_measurements` class attribute\n            for observable in constraint['observables']:\n                self._observable_to_measurements[observable].add(name)\n\n            # Add constraint to `constraints` attribute of the Measurement object\n            self.constraints.append(self._define_constraint(**constraint))\n\n        # Add set of observables that the measurement constrains to `constrained_observables` attribute of the Measurement object\n        self.constrained_observables = set(chain.from_iterable(\n            constraint['observables'] for constraint in self.constraints\n        ))\n\n        # Add measurement to `_measurements` class attribute\n        self._measurements[name] = self\n\n    @staticmethod\n    def _define_constraint(observables: np.ndarray, distribution_type: str, **parameters: dict) -&gt; dict:\n\n        # Convert GeneralGammaDistributionPositive to NumericalDistribution\n        if distribution_type == 'GeneralGammaDistributionPositive':\n            distribution_type, parameters = convert_GeneralGammaDistributionPositive(**parameters)\n\n        # Convert lists to numpy arrays for numerical distributions,\n        # normalize PDF to 1, and add log PDF\n        elif distribution_type == 'NumericalDistribution':\n            x = np.array(parameters['x'])\n            y = np.array(parameters['y'])\n            y = np.maximum(0, y)  # make sure PDF is positive\n            y = y /  np.trapz(y, x=x)  # normalize PDF to 1\n            # ignore warning from log(0)=-np.inf\n            with np.errstate(divide='ignore', invalid='ignore'):\n                log_y = np.log(y)\n            # replace -np.inf with a large negative number\n            log_y[np.isneginf(log_y)] = LOG_ZERO\n            parameters['x'] = x\n            parameters['y'] = y\n            parameters['log_y'] = log_y\n\n        # Convert lists to numpy arrays for multivariate normal distribution\n        elif distribution_type == 'MultivariateNormalDistribution':\n            parameters['standard_deviation'] = np.array(parameters['standard_deviation'])\n            parameters['correlation'] = np.array(parameters['correlation'])\n\n        return {'observables': observables, 'distribution_type': distribution_type, 'parameters': parameters}\n\n    def __repr__(self):\n        return f'&lt;Measurement {self.name} constraining {self.constrained_observables}&gt;'\n\n    def __str__(self):\n        return f'Measurement {self.name} constraining {self.constrained_observables}'\n\n    @classmethod\n    def get_all_measurements(cls):\n        '''\n        Return all measurements.\n\n        Returns\n        -------\n        dict\n            Dictionary containing all measurements.\n\n        Examples\n        --------\n        Get all measurements:\n\n        &gt;&gt;&gt; Measurement.get_all_measurements()\n        {'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n        '''\n        return cls._measurements\n\n    @classmethod\n    def get_all_observables(cls):\n        '''\n        Return all observables.\n\n        Returns\n        -------\n        set\n            Set containing all observables.\n\n        Examples\n        --------\n        Get all observables:\n\n        &gt;&gt;&gt; Measurement.get_all_observables()\n        {'observable1', 'observable2', ...}\n        '''\n        return set(cls._observable_to_measurements.keys())\n\n    @classmethod\n    def get_measurements(\n        cls,\n        observables: Union[List[str], np.ndarray],\n        include_measurements: Optional[List[str]] = None,\n        exclude_measurements: Optional[List[str]] = None,\n    ) -&gt; Dict[str, 'Measurement']:\n        '''\n        Return measurements that constrain the specified observables.\n\n        Parameters\n        ----------\n        observables : list or array[str]\n            Observables to constrain.\n        include_measurements : list[str], optional\n            A list of measurements to include. If `None`, include all measurements.\n        exclude_measurements : list[str], optional\n            A list of measurements to exclude. If `None`, exclude no measurements.\n\n        Returnsxw\n        -------\n        dict\n            Dictionary containing measurements that constrain the specified observables.\n\n        Examples\n        --------\n        Get measurements that constrain the specified observables:\n\n        &gt;&gt;&gt; Measurement.get_measurements(['observable1', 'observable2'])\n        {'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n        '''\n        if include_measurements is not None and exclude_measurements is not None:\n            raise ValueError(\"Please provide either `include_measurements` or `exclude_measurements`, not both.\")\n        measurement_names = set(chain.from_iterable(\n            cls._observable_to_measurements.get(observable, set())\n            for observable in observables\n        ))\n        all_measurements = set(cls.get_all_measurements())\n        if include_measurements is not None:\n            if set(include_measurements) - all_measurements:\n                raise ValueError(f\"Measurements {set(include_measurements) - all_measurements} provided in `include_measurements` not found in loaded measurements.\")\n            measurement_names = set(include_measurements) &amp; measurement_names\n        elif exclude_measurements is not None:\n            if set(exclude_measurements) - all_measurements:\n                raise ValueError(f\"Measurements {set(exclude_measurements) - all_measurements} provided in `exclude_measurements` not found in loaded measurements.\")\n            measurement_names = measurement_names - set(exclude_measurements)\n        return {name: cls._measurements[name] for name in measurement_names}\n\n    @classmethod\n    def get_constraints(\n        cls,\n        observables: Union[List[str], np.ndarray],\n        observables_for_indices: Union[List[str], np.ndarray] = None,\n        distribution_types: Optional[List[str]] = None,\n        include_measurements: Optional[List[str]] = None,\n        exclude_measurements: Optional[List[str]] = None,\n    ) -&gt; Dict[str, Dict[str, np.ndarray]]:\n        '''\n        Return constraints on the specified observables.\n\n        Parameters\n        ----------\n        observables : list or array[str]\n            Observables to constrain.\n        observables_for_indices : list or array[str], optional\n            Observables to create indices for. If `None`, use the same observables as `observables`.\n        distribution_types : list[str], optional\n            Types of distributions to include. If `None`, include all distributions.\n        include_measurements : list[str], optional\n            A list of measurements to include. If `None`, include all measurements.\n        exclude_measurements : list[str], optional\n            A list of measurements to exclude. If `None`, exclude no measurements.\n\n        Returns\n        -------\n        dict\n            Dictionary containing constraints on the specified observables.\n\n        Examples\n        --------\n        Get constraints on the specified observables:\n\n        &gt;&gt;&gt; Measurement.get_constraints(['observable1', 'observable2'])\n        {'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n\n        Get constraints on the specified observables with specific distribution types:\n\n        &gt;&gt;&gt; Measurement.get_constraints(['observable1', 'observable2'], ['NormalDistribution', 'MultivariateNormalDistribution'])\n        {'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, 'MultivariateNormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'covariance': [[1.0, 0.0], [0.0, 1.0]], 'inverse_covariance': [[1.0, 0.0], [0.0, 1.0]]}}\n        '''\n        if not isinstance(observables, (list, tuple, np.ndarray)):\n            raise ValueError('observables must be a list, tuple, or array')\n        if isinstance(observables, np.ndarray):\n            observables = observables.tolist()\n        if observables_for_indices is None:\n            observables_for_indices = observables\n        else:\n            if not isinstance(observables_for_indices, (list, tuple, np.ndarray)):\n                raise ValueError('observables_for_indices must be a list, tuple, or array')\n            if isinstance(observables_for_indices, np.ndarray):\n                observables_for_indices = observables_for_indices.tolist()\n        measurements = cls.get_measurements(observables, include_measurements, exclude_measurements)\n        observables_set = set(observables)\n        constraints = defaultdict(lambda: defaultdict(list))\n        for measurement_name, measurement in measurements.items():\n            for constraint in measurement.constraints:\n                selected_observables = set(constraint['observables']) &amp; observables_set\n                if selected_observables:\n                    distribution_type = constraint['distribution_type']\n                    if distribution_types is not None and distribution_type not in distribution_types:\n                        continue\n                    if distribution_type == 'MultivariateNormalDistribution':\n\n                        # Boolean mask for order-preserving selection\n                        selected_observables_array = np.array(list(selected_observables), dtype=object)\n                        mask = np.isin(constraint['observables'], selected_observables_array)\n\n                        # Skip if no matches\n                        if not np.any(mask):\n                            continue\n\n                        # Select entries using the boolean mask\n                        constraint_observables = constraint['observables'][mask]\n                        constraint_central_value = np.array(constraint['parameters']['central_value'])[mask]\n                        constraint_standard_deviation = np.array(constraint['parameters']['standard_deviation'])[mask]\n                        constraint_correlation = np.array(constraint['parameters']['correlation'])[mask][:, mask]\n                        observable_indices = np.array([observables_for_indices.index(obs) for obs in constraint_observables])\n\n                        if np.sum(mask) == 1: # Univariate normal distribution\n                            constraints['NormalDistribution']['measurement_name'].append(measurement_name)\n                            constraints['NormalDistribution']['observables'].extend(constraint_observables)\n                            constraints['NormalDistribution']['observable_indices'].extend(observable_indices)\n                            constraints['NormalDistribution']['central_value'].extend(constraint_central_value)\n                            constraints['NormalDistribution']['standard_deviation'].extend(constraint_standard_deviation)\n                        else: # Multivariate normal distribution\n                            constraints[distribution_type]['measurement_name'].append(measurement_name)\n                            constraints[distribution_type]['observables'].append(\n                                np.asarray(constraint_observables, dtype=object)\n                            )\n                            constraints[distribution_type]['observable_indices'].append(\n                                np.asarray(observable_indices, dtype=int)\n                            )\n                            constraints[distribution_type]['central_value'].append(\n                                np.asarray(constraint_central_value)\n                            )\n                            constraints[distribution_type]['standard_deviation'].append(\n                                np.asarray(constraint_standard_deviation)\n                            )\n                            constraints[distribution_type]['inverse_correlation'].append(\n                                np.linalg.inv(constraint_correlation)\n                            )\n                            n = len(constraint_observables)\n                            logdet_corr = np.linalg.slogdet(constraint_correlation)[1]\n                            logprod_std2 = 2 * np.sum(np.log(constraint_standard_deviation))\n                            constraints[distribution_type]['logpdf_normalization_per_observable'].append(\n                                -0.5 * ( (logdet_corr + logprod_std2) / n + np.log(2 * np.pi) )\n                            )\n                    else:\n                        constraints[distribution_type]['measurement_name'].append(measurement_name)\n                        observable_indices = [observables_for_indices.index(obs) for obs in constraint['observables']]\n                        constraints[distribution_type]['observables'].extend(constraint['observables'])\n                        constraints[distribution_type]['observable_indices'].extend(observable_indices)\n                        for key in constraint['parameters']:\n                            constraints[distribution_type][key].append(constraint['parameters'][key])\n        for distribution_type in constraints:\n\n            # Pad arrays to the same length for numerical distributions\n            if distribution_type == 'NumericalDistribution':\n                constraints[distribution_type]['x'] = pad_arrays(constraints[distribution_type]['x'])\n                constraints[distribution_type]['y'] = pad_arrays(constraints[distribution_type]['y'])\n                constraints[distribution_type]['log_y'] = pad_arrays(constraints[distribution_type]['log_y'])\n\n            # Convert lists to numpy arrays\n            if distribution_type == 'MultivariateNormalDistribution':\n                for key in constraints[distribution_type]:\n                    nparray = np.empty(len(constraints[distribution_type][key]), dtype=object)\n                    nparray[:] = constraints[distribution_type][key]\n                    constraints[distribution_type][key] = nparray\n            else:\n                for key in constraints[distribution_type]:\n                    if key == 'observable_indices':\n                        dtype = int\n                    elif key == 'observables':\n                        dtype = object\n                    else:\n                        dtype = None\n                    constraints[distribution_type][key] = np.asarray(\n                        constraints[distribution_type][key],\n                        dtype=dtype\n                    )\n        return constraints\n\n    def combine_constraints(\n            constraints_list: List[Dict[str, Dict[str, np.ndarray]]],\n    ) -&gt; Dict[str, Dict[str, np.ndarray]]:\n        '''\n        Combine the constraints provided in the list of constraints, where each element of the list is a dictionary of constraints on a single observable.\n\n        Normal distributions are combined analytically, while other distributions are combined numerically.\n\n        Parameters\n        ----------\n        constraints_list : list[dict]\n            List of constraints to combine, one constraints dictionary per observable.\n\n        Returns\n        -------\n        dict\n            Dictionary containing combined constraints.\n\n        Examples\n        --------\n        Combine two constraints on two observables:\n        &gt;&gt;&gt; Measurement.combine_constraints([\n        ...     {'NormalDistribution': {'measurement_name': ['measurement1', 'measurement2'], 'observables': ['observable1', 'observable1'], 'observable_indices': np.array([0, 0]), 'central_value': np.array([1.0, 1.2]), 'standard_deviation': np.array([0.2, 0.3])}},\n        ...     {'NormalDistribution': {'measurement_name': ['measurement3', 'measurement4'], 'observables': ['observable2', 'observable2'], 'observable_indices': np.array([1, 1]), 'central_value': np.array([2.0, 2.5]), 'standard_deviation': np.array([0.5, 0.7])}},\n        ... ])\n        {'NormalDistribution':\n            {\n                'measurement_name': array(['measurement1, measurement2', 'measurement3, measurement4']),\n                'observables': array(['observable1', 'observable2']),\n                'observable_indices': array([0, 1]),\n                'central_value': array([1.06153846, 2.16891892]),\n                'standard_deviation': array([0.16641006, 0.40686674])\n            },\n        }\n        '''\n\n        combined_constraints = defaultdict(lambda: defaultdict(list))\n        for constraints in constraints_list:\n            # handle normal distributions\n            if 'NormalDistribution' in constraints:\n                constraints['NormalDistribution'] = combine_normal_distributions(**constraints['NormalDistribution'])\n\n            if len(constraints) &gt; 1 or len(next(iter(constraints.values()))['measurement_name']) &gt; 1:\n                numerical_distribution = combine_distributions_numerically(constraints)\n                for key, value in numerical_distribution.items():\n                    combined_constraints['NumericalDistribution'][key].append(value)\n            else:\n                dist_type, dist_info = next(iter(constraints.items()))\n                for key, value in dist_info.items():\n                    if dist_type == 'NumericalDistribution' and key in ['x', 'y', 'log_y']:\n                        value = np.squeeze(value)\n                    combined_constraints[dist_type][key].append(value)\n\n        for dist_type, dist_info in combined_constraints.items():\n            for key, value_list in dist_info.items():\n                if dist_type == 'NumericalDistribution' and key in ['x', 'y', 'log_y']:\n                    combined_constraints[dist_type][key] = pad_arrays(value_list)\n                else:\n                    combined_constraints[dist_type][key] = np.concatenate(value_list, axis=0)\n        return combined_constraints\n\n    @classmethod\n    def get_combined_constraints(\n        cls,\n        observables: Union[List[str], np.ndarray],\n        ) -&gt; Dict[str, Dict[str, np.ndarray]]:\n        '''\n        Return combined constraints on the specified observables.\n\n        Parameters\n        ----------\n        observables : list or array[str]\n            Observables to combine constraints for.\n\n        Returns\n        -------\n        dict\n            Dictionary containing combined constraints on the specified observables.\n\n        Examples\n        --------\n        Get combined constraints on the specified observables:\n\n        &gt;&gt;&gt; Measurement.get_combined_constraints(['observable1', 'observable2'])\n        {'NormalDistribution': {'measurement_name': ['measurement1'], 'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n        '''\n\n        return cls.combine_constraints(\n            [cls.get_constraints([observable], observables) for observable in observables]\n        )\n\n    @classmethod\n    def _load_file(cls, path: str) -&gt; None:\n        with open(path, 'r') as f:\n            json_data = json.load(f)\n        schema_name, schema_version = get_json_schema(json_data)\n        if schema_name == 'pdfxf' and schema_version in cls._pdfxf_versions:\n            del json_data['$schema']\n            for name, constraints in json_data.items():\n                cls(name, constraints)\n\n    @classmethod\n    def load(cls, path: str) -&gt; None:\n        '''\n        Load measurements from a json file or a directory containing json files.\n\n        Parameters\n        ----------\n        path : str\n            Path to a json file or a directory containing json files.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Load measurements from a json file:\n\n        &gt;&gt;&gt; Measurement.load('./measurements.json')\n\n        Load measurements from a directory containing json files:\n\n        &gt;&gt;&gt; Measurement.load('./measurements/')\n        '''\n        # load all json files in the directory\n        if os.path.isdir(path):\n            for file in os.listdir(path):\n                if file.endswith('.json'):\n                    cls._load_file(os.path.join(path, file))\n        # load single json file\n        else:\n            cls._load_file(path)\n\n    @classmethod\n    def unload(cls, measurement_names: List[str]) -&gt; None:\n        '''\n        Unload measurements.\n\n        Parameters\n        ----------\n        measurement_names : list[str]\n            Names of the measurements to unload.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Unload measurements:\n\n        &gt;&gt;&gt; Measurement.unload(['measurement1', 'measurement2'])\n        '''\n        for name in measurement_names:\n            measurement = cls._measurements.pop(name, None)\n            if measurement is not None:\n                for constraint in measurement.constraints:\n                    for observable in constraint['observables']:\n                        cls._observable_to_measurements[observable].remove(name)\n                        if not cls._observable_to_measurements[observable]:\n                            del cls._observable_to_measurements[observable]\n\n    @classmethod\n    def clear(cls) -&gt; None:\n        '''\n        Clear all measurements.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Clear all measurements:\n\n        &gt;&gt;&gt; Measurement.clear()\n        '''\n        cls._measurements.clear()\n        cls._observable_to_measurements.clear()\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.__init__","title":"<code>__init__(name, constraints)</code>","text":"<p>Initialize a Measurement object.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the measurement.</p> required <code>constraints</code> <code>list[dict]</code> <p>List of constraints on observables. Each constraint is a dictionary with the following keys:</p> <ul> <li> <p><code>type</code> (<code>str</code>)     Type of the distribution. Can be <code>NormalDistribution</code>, <code>HalfNormalDistribution</code>, <code>GammaDistributionPositive</code>, <code>NumericalDistribution</code>, <code>MultivariateNormalDistribution</code>.</p> </li> <li> <p><code>observables</code> (<code>list[str]</code>)     List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a \\(q^2\\) value or \\(q^2\\) range).</p> </li> <li> <p><code>parameters</code> (<code>dict</code>)     Parameters of the distribution. The keys depend on the type of the distribution as follows:</p> <ul> <li><code>NormalDistribution</code>: <code>central_value</code> (<code>float</code>), <code>standard_deviation</code> (<code>float</code>)</li> <li><code>HalfNormalDistribution</code>: <code>central_value</code> (<code>float</code>), <code>standard_deviation</code> (<code>float</code>)</li> <li><code>GammaDistributionPositive</code>: <code>a</code> (<code>float</code>), <code>loc</code> (<code>float</code>), <code>scale</code> (<code>float</code>)</li> <li><code>NumericalDistribution</code>: <code>x</code> (<code>list[float]</code>), <code>y</code> (<code>list[float]</code>)</li> <li><code>MultivariateNormalDistribution</code>: <code>central_value</code> (<code>list[float]</code>), <code>covariance</code> (<code>list[list[float]]</code>)</li> </ul> </li> </ul> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Initialize a Measurement object:</p> <pre><code>&gt;&gt;&gt; Measurement('measurement1', [{'type': 'NormalDistribution', 'observables': ['observable1'], 'parameters': {'central_value': 0.0, 'standard_deviation': 1.0}}])\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>def __init__(self, name: str, constraints: List[dict]):\n    '''\n    Initialize a Measurement object.\n\n    Parameters\n    ----------\n    name : str\n        Name of the measurement.\n    constraints : list[dict]\n        List of constraints on observables. Each constraint is a dictionary with the following keys:\n\n          - `type` (`str`)\n            Type of the distribution. Can be `NormalDistribution`, `HalfNormalDistribution`, `GammaDistributionPositive`, `NumericalDistribution`, `MultivariateNormalDistribution`.\n\n          - `observables` (`list[str]`)\n            List of observables that the constraint applies to. The observables are either strings or tuples (in case of additional arguments like a $q^2$ value or $q^2$ range).\n\n          - `parameters` (`dict`)\n            Parameters of the distribution. The keys depend on the type of the distribution as follows:\n              - `NormalDistribution`: `central_value` (`float`), `standard_deviation` (`float`)\n              - `HalfNormalDistribution`: `central_value` (`float`), `standard_deviation` (`float`)\n              - `GammaDistributionPositive`: `a` (`float`), `loc` (`float`), `scale` (`float`)\n              - `NumericalDistribution`: `x` (`list[float]`), `y` (`list[float]`)\n              - `MultivariateNormalDistribution`: `central_value` (`list[float]`), `covariance` (`list[list[float]]`)\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Initialize a Measurement object:\n\n    &gt;&gt;&gt; Measurement('measurement1', [{'type': 'NormalDistribution', 'observables': ['observable1'], 'parameters': {'central_value': 0.0, 'standard_deviation': 1.0}}])\n    '''\n    self.name: str = name\n    self.constraints: list[dict] = []\n    for constraint in constraints:\n\n        # Convert list of observable names to numpy array containing strings\n        constraint['observables'] = np.array(constraint['observables'], dtype=object)\n\n        # Add measurement name to `_observable_to_measurements` class attribute\n        for observable in constraint['observables']:\n            self._observable_to_measurements[observable].add(name)\n\n        # Add constraint to `constraints` attribute of the Measurement object\n        self.constraints.append(self._define_constraint(**constraint))\n\n    # Add set of observables that the measurement constrains to `constrained_observables` attribute of the Measurement object\n    self.constrained_observables = set(chain.from_iterable(\n        constraint['observables'] for constraint in self.constraints\n    ))\n\n    # Add measurement to `_measurements` class attribute\n    self._measurements[name] = self\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.clear","title":"<code>clear()</code>  <code>classmethod</code>","text":"<p>Clear all measurements.</p> <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Clear all measurements:</p> <pre><code>&gt;&gt;&gt; Measurement.clear()\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef clear(cls) -&gt; None:\n    '''\n    Clear all measurements.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Clear all measurements:\n\n    &gt;&gt;&gt; Measurement.clear()\n    '''\n    cls._measurements.clear()\n    cls._observable_to_measurements.clear()\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.combine_constraints","title":"<code>combine_constraints(constraints_list)</code>","text":"<p>Combine the constraints provided in the list of constraints, where each element of the list is a dictionary of constraints on a single observable.</p> <p>Normal distributions are combined analytically, while other distributions are combined numerically.</p> <p>Parameters:</p> Name Type Description Default <code>constraints_list</code> <code>list[dict]</code> <p>List of constraints to combine, one constraints dictionary per observable.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing combined constraints.</p> <p>Examples:</p> <p>Combine two constraints on two observables:</p> <pre><code>&gt;&gt;&gt; Measurement.combine_constraints([\n...     {'NormalDistribution': {'measurement_name': ['measurement1', 'measurement2'], 'observables': ['observable1', 'observable1'], 'observable_indices': np.array([0, 0]), 'central_value': np.array([1.0, 1.2]), 'standard_deviation': np.array([0.2, 0.3])}},\n...     {'NormalDistribution': {'measurement_name': ['measurement3', 'measurement4'], 'observables': ['observable2', 'observable2'], 'observable_indices': np.array([1, 1]), 'central_value': np.array([2.0, 2.5]), 'standard_deviation': np.array([0.5, 0.7])}},\n... ])\n{'NormalDistribution':\n    {\n        'measurement_name': array(['measurement1, measurement2', 'measurement3, measurement4']),\n        'observables': array(['observable1', 'observable2']),\n        'observable_indices': array([0, 1]),\n        'central_value': array([1.06153846, 2.16891892]),\n        'standard_deviation': array([0.16641006, 0.40686674])\n    },\n}\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>def combine_constraints(\n        constraints_list: List[Dict[str, Dict[str, np.ndarray]]],\n) -&gt; Dict[str, Dict[str, np.ndarray]]:\n    '''\n    Combine the constraints provided in the list of constraints, where each element of the list is a dictionary of constraints on a single observable.\n\n    Normal distributions are combined analytically, while other distributions are combined numerically.\n\n    Parameters\n    ----------\n    constraints_list : list[dict]\n        List of constraints to combine, one constraints dictionary per observable.\n\n    Returns\n    -------\n    dict\n        Dictionary containing combined constraints.\n\n    Examples\n    --------\n    Combine two constraints on two observables:\n    &gt;&gt;&gt; Measurement.combine_constraints([\n    ...     {'NormalDistribution': {'measurement_name': ['measurement1', 'measurement2'], 'observables': ['observable1', 'observable1'], 'observable_indices': np.array([0, 0]), 'central_value': np.array([1.0, 1.2]), 'standard_deviation': np.array([0.2, 0.3])}},\n    ...     {'NormalDistribution': {'measurement_name': ['measurement3', 'measurement4'], 'observables': ['observable2', 'observable2'], 'observable_indices': np.array([1, 1]), 'central_value': np.array([2.0, 2.5]), 'standard_deviation': np.array([0.5, 0.7])}},\n    ... ])\n    {'NormalDistribution':\n        {\n            'measurement_name': array(['measurement1, measurement2', 'measurement3, measurement4']),\n            'observables': array(['observable1', 'observable2']),\n            'observable_indices': array([0, 1]),\n            'central_value': array([1.06153846, 2.16891892]),\n            'standard_deviation': array([0.16641006, 0.40686674])\n        },\n    }\n    '''\n\n    combined_constraints = defaultdict(lambda: defaultdict(list))\n    for constraints in constraints_list:\n        # handle normal distributions\n        if 'NormalDistribution' in constraints:\n            constraints['NormalDistribution'] = combine_normal_distributions(**constraints['NormalDistribution'])\n\n        if len(constraints) &gt; 1 or len(next(iter(constraints.values()))['measurement_name']) &gt; 1:\n            numerical_distribution = combine_distributions_numerically(constraints)\n            for key, value in numerical_distribution.items():\n                combined_constraints['NumericalDistribution'][key].append(value)\n        else:\n            dist_type, dist_info = next(iter(constraints.items()))\n            for key, value in dist_info.items():\n                if dist_type == 'NumericalDistribution' and key in ['x', 'y', 'log_y']:\n                    value = np.squeeze(value)\n                combined_constraints[dist_type][key].append(value)\n\n    for dist_type, dist_info in combined_constraints.items():\n        for key, value_list in dist_info.items():\n            if dist_type == 'NumericalDistribution' and key in ['x', 'y', 'log_y']:\n                combined_constraints[dist_type][key] = pad_arrays(value_list)\n            else:\n                combined_constraints[dist_type][key] = np.concatenate(value_list, axis=0)\n    return combined_constraints\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.get_all_measurements","title":"<code>get_all_measurements()</code>  <code>classmethod</code>","text":"<p>Return all measurements.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing all measurements.</p> <p>Examples:</p> <p>Get all measurements:</p> <pre><code>&gt;&gt;&gt; Measurement.get_all_measurements()\n{'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef get_all_measurements(cls):\n    '''\n    Return all measurements.\n\n    Returns\n    -------\n    dict\n        Dictionary containing all measurements.\n\n    Examples\n    --------\n    Get all measurements:\n\n    &gt;&gt;&gt; Measurement.get_all_measurements()\n    {'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n    '''\n    return cls._measurements\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.get_all_observables","title":"<code>get_all_observables()</code>  <code>classmethod</code>","text":"<p>Return all observables.</p> <p>Returns:</p> Type Description <code>set</code> <p>Set containing all observables.</p> <p>Examples:</p> <p>Get all observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_all_observables()\n{'observable1', 'observable2', ...}\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef get_all_observables(cls):\n    '''\n    Return all observables.\n\n    Returns\n    -------\n    set\n        Set containing all observables.\n\n    Examples\n    --------\n    Get all observables:\n\n    &gt;&gt;&gt; Measurement.get_all_observables()\n    {'observable1', 'observable2', ...}\n    '''\n    return set(cls._observable_to_measurements.keys())\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.get_combined_constraints","title":"<code>get_combined_constraints(observables)</code>  <code>classmethod</code>","text":"<p>Return combined constraints on the specified observables.</p> <p>Parameters:</p> Name Type Description Default <code>observables</code> <code>list or array[str]</code> <p>Observables to combine constraints for.</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing combined constraints on the specified observables.</p> <p>Examples:</p> <p>Get combined constraints on the specified observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_combined_constraints(['observable1', 'observable2'])\n{'NormalDistribution': {'measurement_name': ['measurement1'], 'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef get_combined_constraints(\n    cls,\n    observables: Union[List[str], np.ndarray],\n    ) -&gt; Dict[str, Dict[str, np.ndarray]]:\n    '''\n    Return combined constraints on the specified observables.\n\n    Parameters\n    ----------\n    observables : list or array[str]\n        Observables to combine constraints for.\n\n    Returns\n    -------\n    dict\n        Dictionary containing combined constraints on the specified observables.\n\n    Examples\n    --------\n    Get combined constraints on the specified observables:\n\n    &gt;&gt;&gt; Measurement.get_combined_constraints(['observable1', 'observable2'])\n    {'NormalDistribution': {'measurement_name': ['measurement1'], 'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n    '''\n\n    return cls.combine_constraints(\n        [cls.get_constraints([observable], observables) for observable in observables]\n    )\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.get_constraints","title":"<code>get_constraints(observables, observables_for_indices=None, distribution_types=None, include_measurements=None, exclude_measurements=None)</code>  <code>classmethod</code>","text":"<p>Return constraints on the specified observables.</p> <p>Parameters:</p> Name Type Description Default <code>observables</code> <code>list or array[str]</code> <p>Observables to constrain.</p> required <code>observables_for_indices</code> <code>list or array[str]</code> <p>Observables to create indices for. If <code>None</code>, use the same observables as <code>observables</code>.</p> <code>None</code> <code>distribution_types</code> <code>list[str]</code> <p>Types of distributions to include. If <code>None</code>, include all distributions.</p> <code>None</code> <code>include_measurements</code> <code>list[str]</code> <p>A list of measurements to include. If <code>None</code>, include all measurements.</p> <code>None</code> <code>exclude_measurements</code> <code>list[str]</code> <p>A list of measurements to exclude. If <code>None</code>, exclude no measurements.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing constraints on the specified observables.</p> <p>Examples:</p> <p>Get constraints on the specified observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_constraints(['observable1', 'observable2'])\n{'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n</code></pre> <p>Get constraints on the specified observables with specific distribution types:</p> <pre><code>&gt;&gt;&gt; Measurement.get_constraints(['observable1', 'observable2'], ['NormalDistribution', 'MultivariateNormalDistribution'])\n{'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, 'MultivariateNormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'covariance': [[1.0, 0.0], [0.0, 1.0]], 'inverse_covariance': [[1.0, 0.0], [0.0, 1.0]]}}\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef get_constraints(\n    cls,\n    observables: Union[List[str], np.ndarray],\n    observables_for_indices: Union[List[str], np.ndarray] = None,\n    distribution_types: Optional[List[str]] = None,\n    include_measurements: Optional[List[str]] = None,\n    exclude_measurements: Optional[List[str]] = None,\n) -&gt; Dict[str, Dict[str, np.ndarray]]:\n    '''\n    Return constraints on the specified observables.\n\n    Parameters\n    ----------\n    observables : list or array[str]\n        Observables to constrain.\n    observables_for_indices : list or array[str], optional\n        Observables to create indices for. If `None`, use the same observables as `observables`.\n    distribution_types : list[str], optional\n        Types of distributions to include. If `None`, include all distributions.\n    include_measurements : list[str], optional\n        A list of measurements to include. If `None`, include all measurements.\n    exclude_measurements : list[str], optional\n        A list of measurements to exclude. If `None`, exclude no measurements.\n\n    Returns\n    -------\n    dict\n        Dictionary containing constraints on the specified observables.\n\n    Examples\n    --------\n    Get constraints on the specified observables:\n\n    &gt;&gt;&gt; Measurement.get_constraints(['observable1', 'observable2'])\n    {'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, ...}\n\n    Get constraints on the specified observables with specific distribution types:\n\n    &gt;&gt;&gt; Measurement.get_constraints(['observable1', 'observable2'], ['NormalDistribution', 'MultivariateNormalDistribution'])\n    {'NormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'standard_deviation': [1.0, 1.0]}, 'MultivariateNormalDistribution': {'observables': ['observable1', 'observable2'], 'observable_indices': [0, 1], 'central_value': [0.0, 0.0], 'covariance': [[1.0, 0.0], [0.0, 1.0]], 'inverse_covariance': [[1.0, 0.0], [0.0, 1.0]]}}\n    '''\n    if not isinstance(observables, (list, tuple, np.ndarray)):\n        raise ValueError('observables must be a list, tuple, or array')\n    if isinstance(observables, np.ndarray):\n        observables = observables.tolist()\n    if observables_for_indices is None:\n        observables_for_indices = observables\n    else:\n        if not isinstance(observables_for_indices, (list, tuple, np.ndarray)):\n            raise ValueError('observables_for_indices must be a list, tuple, or array')\n        if isinstance(observables_for_indices, np.ndarray):\n            observables_for_indices = observables_for_indices.tolist()\n    measurements = cls.get_measurements(observables, include_measurements, exclude_measurements)\n    observables_set = set(observables)\n    constraints = defaultdict(lambda: defaultdict(list))\n    for measurement_name, measurement in measurements.items():\n        for constraint in measurement.constraints:\n            selected_observables = set(constraint['observables']) &amp; observables_set\n            if selected_observables:\n                distribution_type = constraint['distribution_type']\n                if distribution_types is not None and distribution_type not in distribution_types:\n                    continue\n                if distribution_type == 'MultivariateNormalDistribution':\n\n                    # Boolean mask for order-preserving selection\n                    selected_observables_array = np.array(list(selected_observables), dtype=object)\n                    mask = np.isin(constraint['observables'], selected_observables_array)\n\n                    # Skip if no matches\n                    if not np.any(mask):\n                        continue\n\n                    # Select entries using the boolean mask\n                    constraint_observables = constraint['observables'][mask]\n                    constraint_central_value = np.array(constraint['parameters']['central_value'])[mask]\n                    constraint_standard_deviation = np.array(constraint['parameters']['standard_deviation'])[mask]\n                    constraint_correlation = np.array(constraint['parameters']['correlation'])[mask][:, mask]\n                    observable_indices = np.array([observables_for_indices.index(obs) for obs in constraint_observables])\n\n                    if np.sum(mask) == 1: # Univariate normal distribution\n                        constraints['NormalDistribution']['measurement_name'].append(measurement_name)\n                        constraints['NormalDistribution']['observables'].extend(constraint_observables)\n                        constraints['NormalDistribution']['observable_indices'].extend(observable_indices)\n                        constraints['NormalDistribution']['central_value'].extend(constraint_central_value)\n                        constraints['NormalDistribution']['standard_deviation'].extend(constraint_standard_deviation)\n                    else: # Multivariate normal distribution\n                        constraints[distribution_type]['measurement_name'].append(measurement_name)\n                        constraints[distribution_type]['observables'].append(\n                            np.asarray(constraint_observables, dtype=object)\n                        )\n                        constraints[distribution_type]['observable_indices'].append(\n                            np.asarray(observable_indices, dtype=int)\n                        )\n                        constraints[distribution_type]['central_value'].append(\n                            np.asarray(constraint_central_value)\n                        )\n                        constraints[distribution_type]['standard_deviation'].append(\n                            np.asarray(constraint_standard_deviation)\n                        )\n                        constraints[distribution_type]['inverse_correlation'].append(\n                            np.linalg.inv(constraint_correlation)\n                        )\n                        n = len(constraint_observables)\n                        logdet_corr = np.linalg.slogdet(constraint_correlation)[1]\n                        logprod_std2 = 2 * np.sum(np.log(constraint_standard_deviation))\n                        constraints[distribution_type]['logpdf_normalization_per_observable'].append(\n                            -0.5 * ( (logdet_corr + logprod_std2) / n + np.log(2 * np.pi) )\n                        )\n                else:\n                    constraints[distribution_type]['measurement_name'].append(measurement_name)\n                    observable_indices = [observables_for_indices.index(obs) for obs in constraint['observables']]\n                    constraints[distribution_type]['observables'].extend(constraint['observables'])\n                    constraints[distribution_type]['observable_indices'].extend(observable_indices)\n                    for key in constraint['parameters']:\n                        constraints[distribution_type][key].append(constraint['parameters'][key])\n    for distribution_type in constraints:\n\n        # Pad arrays to the same length for numerical distributions\n        if distribution_type == 'NumericalDistribution':\n            constraints[distribution_type]['x'] = pad_arrays(constraints[distribution_type]['x'])\n            constraints[distribution_type]['y'] = pad_arrays(constraints[distribution_type]['y'])\n            constraints[distribution_type]['log_y'] = pad_arrays(constraints[distribution_type]['log_y'])\n\n        # Convert lists to numpy arrays\n        if distribution_type == 'MultivariateNormalDistribution':\n            for key in constraints[distribution_type]:\n                nparray = np.empty(len(constraints[distribution_type][key]), dtype=object)\n                nparray[:] = constraints[distribution_type][key]\n                constraints[distribution_type][key] = nparray\n        else:\n            for key in constraints[distribution_type]:\n                if key == 'observable_indices':\n                    dtype = int\n                elif key == 'observables':\n                    dtype = object\n                else:\n                    dtype = None\n                constraints[distribution_type][key] = np.asarray(\n                    constraints[distribution_type][key],\n                    dtype=dtype\n                )\n    return constraints\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.get_measurements","title":"<code>get_measurements(observables, include_measurements=None, exclude_measurements=None)</code>  <code>classmethod</code>","text":"<p>Return measurements that constrain the specified observables.</p> <p>Parameters:</p> Name Type Description Default <code>observables</code> <code>list or array[str]</code> <p>Observables to constrain.</p> required <code>include_measurements</code> <code>list[str]</code> <p>A list of measurements to include. If <code>None</code>, include all measurements.</p> <code>None</code> <code>exclude_measurements</code> <code>list[str]</code> <p>A list of measurements to exclude. If <code>None</code>, exclude no measurements.</p> <code>None</code> Returnsxw <p>dict     Dictionary containing measurements that constrain the specified observables.</p> <p>Examples:</p> <p>Get measurements that constrain the specified observables:</p> <pre><code>&gt;&gt;&gt; Measurement.get_measurements(['observable1', 'observable2'])\n{'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef get_measurements(\n    cls,\n    observables: Union[List[str], np.ndarray],\n    include_measurements: Optional[List[str]] = None,\n    exclude_measurements: Optional[List[str]] = None,\n) -&gt; Dict[str, 'Measurement']:\n    '''\n    Return measurements that constrain the specified observables.\n\n    Parameters\n    ----------\n    observables : list or array[str]\n        Observables to constrain.\n    include_measurements : list[str], optional\n        A list of measurements to include. If `None`, include all measurements.\n    exclude_measurements : list[str], optional\n        A list of measurements to exclude. If `None`, exclude no measurements.\n\n    Returnsxw\n    -------\n    dict\n        Dictionary containing measurements that constrain the specified observables.\n\n    Examples\n    --------\n    Get measurements that constrain the specified observables:\n\n    &gt;&gt;&gt; Measurement.get_measurements(['observable1', 'observable2'])\n    {'measurement1': &lt;Measurement object&gt;, 'measurement2': &lt;Measurement object&gt;, ...}\n    '''\n    if include_measurements is not None and exclude_measurements is not None:\n        raise ValueError(\"Please provide either `include_measurements` or `exclude_measurements`, not both.\")\n    measurement_names = set(chain.from_iterable(\n        cls._observable_to_measurements.get(observable, set())\n        for observable in observables\n    ))\n    all_measurements = set(cls.get_all_measurements())\n    if include_measurements is not None:\n        if set(include_measurements) - all_measurements:\n            raise ValueError(f\"Measurements {set(include_measurements) - all_measurements} provided in `include_measurements` not found in loaded measurements.\")\n        measurement_names = set(include_measurements) &amp; measurement_names\n    elif exclude_measurements is not None:\n        if set(exclude_measurements) - all_measurements:\n            raise ValueError(f\"Measurements {set(exclude_measurements) - all_measurements} provided in `exclude_measurements` not found in loaded measurements.\")\n        measurement_names = measurement_names - set(exclude_measurements)\n    return {name: cls._measurements[name] for name in measurement_names}\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load measurements from a json file or a directory containing json files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a json file or a directory containing json files.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Load measurements from a json file:</p> <pre><code>&gt;&gt;&gt; Measurement.load('./measurements.json')\n</code></pre> <p>Load measurements from a directory containing json files:</p> <pre><code>&gt;&gt;&gt; Measurement.load('./measurements/')\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; None:\n    '''\n    Load measurements from a json file or a directory containing json files.\n\n    Parameters\n    ----------\n    path : str\n        Path to a json file or a directory containing json files.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Load measurements from a json file:\n\n    &gt;&gt;&gt; Measurement.load('./measurements.json')\n\n    Load measurements from a directory containing json files:\n\n    &gt;&gt;&gt; Measurement.load('./measurements/')\n    '''\n    # load all json files in the directory\n    if os.path.isdir(path):\n        for file in os.listdir(path):\n            if file.endswith('.json'):\n                cls._load_file(os.path.join(path, file))\n    # load single json file\n    else:\n        cls._load_file(path)\n</code></pre>"},{"location":"jelli/core/measurement/#jelli.core.measurement.Measurement.unload","title":"<code>unload(measurement_names)</code>  <code>classmethod</code>","text":"<p>Unload measurements.</p> <p>Parameters:</p> Name Type Description Default <code>measurement_names</code> <code>list[str]</code> <p>Names of the measurements to unload.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Unload measurements:</p> <pre><code>&gt;&gt;&gt; Measurement.unload(['measurement1', 'measurement2'])\n</code></pre> Source code in <code>jelli/core/measurement.py</code> <pre><code>@classmethod\ndef unload(cls, measurement_names: List[str]) -&gt; None:\n    '''\n    Unload measurements.\n\n    Parameters\n    ----------\n    measurement_names : list[str]\n        Names of the measurements to unload.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Unload measurements:\n\n    &gt;&gt;&gt; Measurement.unload(['measurement1', 'measurement2'])\n    '''\n    for name in measurement_names:\n        measurement = cls._measurements.pop(name, None)\n        if measurement is not None:\n            for constraint in measurement.constraints:\n                for observable in constraint['observables']:\n                    cls._observable_to_measurements[observable].remove(name)\n                    if not cls._observable_to_measurements[observable]:\n                        del cls._observable_to_measurements[observable]\n</code></pre>"},{"location":"jelli/core/observable_sector/","title":"jelli.core.observable_sector","text":""},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector","title":"<code>ObservableSector</code>","text":"<p>A class to represent an observable sector.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the observable sector.</p> required <code>json_data</code> <code>dict</code> <p>The JSON data containing the metadata and data of the observable sector.</p> required <p>Attributes:</p> Name Type Description <code>metadata</code> <code>dict</code> <p>The metadata of the observable sector.</p> <code>data</code> <code>dict</code> <p>The data of the observable sector.</p> <code>observable_names</code> <code>list</code> <p>The names of the observables.</p> <code>polynomial_names</code> <code>list</code> <p>The names of the polynomial coefficients.</p> <code>observable_expressions</code> <code>list</code> <p>The expressions of the observables in terms of the polynomial coefficients.</p> <code>observable_central</code> <code>array</code> <p>The central values of the polynomial coefficients of the observables (possibly an expansion to second order in the parameters).</p> <code>observable_uncertainties</code> <code>array</code> <p>The uncertainties of the polynomial coefficients of the observables (possibly an expansion to second order in the parameters).</p> <code>observable_uncertainties_SM</code> <code>array</code> <p>The uncertainties of the polynomial coefficients of the observables for the SM entry.</p> <code>polynomial_central</code> <code>array</code> <p>The central values of the polynomial coefficients.</p> <code>eft</code> <code>str</code> <p>The EFT of the observable sector.</p> <code>basis</code> <code>str</code> <p>The EFT basis of the observable sector.</p> <code>scale</code> <code>(float, int)</code> <p>The renormalization scale of the observable sector.</p> <code>sectors</code> <code>list</code> <p>The EFT sectors of the observable sector.</p> <code>parameters</code> <code>list</code> <p>The parameters of the observable sector.</p> <code>keys_pars_by_sectors</code> <code>list</code> <p>The keys of the parameters by sector.</p> <code>keys_pars</code> <code>list</code> <p>The keys of the parameters.</p> <code>sector_indices</code> <code>dict</code> <p>The indices of parameters from EFT sectors in the full parameter basis.</p> <code>evolution_matrices</code> <code>dict</code> <p>The Renormalization Group evolution matrices.</p> <code>construct_par_monomials_observable</code> <code>function</code> <p>The function that constructs the parameter monomials from the parameter array, for the polynomial coefficients of the observables (possibly corresponding to an expansion to second order in the parameters).</p> <code>construct_par_monomials_polynomial</code> <code>function</code> <p>The function that constructs the parameter monomials from the parameter array, for the polynomial coefficients.</p> <code>observable_expression_functions</code> <code>list</code> <p>The functions that evaluate the observable expressions in terms of the polynomial predictions.</p> <code>prediction</code> <code>function</code> <p>The function that makes a prediction for the observable sector.</p> <p>Methods:</p> Name Description <code>get_prediction_data</code> <p>Get the data needed to make a prediction for a given EFT and basis.</p> <code>_get_evolution_matrices</code> <p>Get the Renormalization Group evolution matrices for a given EFT and basis.</p> <code>_get_prediction_function</code> <p>Get the function that makes a prediction for the observable sector.</p> <code>_get_construct_par_monomials</code> <p>Get the function that constructs the parameter monomials from the parameter array.</p> <code>_get_observable_expression_function</code> <p>Get the function that evaluates a given observable expression in terms of the polynomial predictions.</p> <code>get_class_prediction_data</code> <p>Get the data needed to make a prediction for a list of observable sectors.</p> <code>get_class_prediction_function</code> <p>Get the function that makes a prediction for a list of observable sectors.</p> <code>get_all_names</code> <p>Get the names of all observable sectors.</p> <code>get</code> <p>Get an observable sector by name.</p> <code>get_all</code> <p>Get all observable sectors.</p> <p>Examples:</p> <p>Initialize an observable sector:</p> <pre><code>&gt;&gt;&gt; ObservableSector(json_data)\n</code></pre> <p>Load an observable sector from a json file:</p> <pre><code>&gt;&gt;&gt; ObservableSector.load('./observable_sector.json')\n</code></pre> <p>Load all observable sectors from a directory containing json files:</p> <pre><code>&gt;&gt;&gt; ObservableSector.load('./observable_sectors/')\n</code></pre> <p>Get the prediction data for the observable sector:</p> <pre><code>&gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n</code></pre> <p>Make a prediction for the observable sector:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> <p>Get an observable sector by name:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get('observable_sector_1')\n</code></pre> <p>Get all observable sectors:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all()\n</code></pre> <p>Get the names of all observable sectors:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names()\n</code></pre> <p>Get the names of all observable sectors that can provide predictions in the <code>SMEFT</code> basis <code>Warsaw</code>:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names('SMEFT', 'Warsaw')\n</code></pre> <p>Get the names of all observable sectors that can provide predictions in the <code>WET</code> basis <code>flavio</code>:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names('WET', 'flavio')\n</code></pre> <p>Get the names of all observable sectors that can provide predictions in the custom basis <code>custom_basis</code>:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names(custom_basis='custom_basis')\n</code></pre> <p>Get the prediction data for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n</code></pre> <p>Get the prediction function for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n</code></pre> <p>Make a prediction for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n&gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>class ObservableSector:\n    '''\n    A class to represent an observable sector.\n\n    Parameters\n    ----------\n    name : str\n        The name of the observable sector.\n    json_data : dict\n        The JSON data containing the metadata and data of the observable sector.\n\n    Attributes\n    ----------\n    metadata : dict\n        The metadata of the observable sector.\n    data : dict\n        The data of the observable sector.\n    observable_names : list\n        The names of the observables.\n    polynomial_names : list\n        The names of the polynomial coefficients.\n    observable_expressions : list\n        The expressions of the observables in terms of the polynomial coefficients.\n    observable_central : jnp.array\n        The central values of the polynomial coefficients of the observables (possibly an expansion to second order in the parameters).\n    observable_uncertainties : jnp.array\n        The uncertainties of the polynomial coefficients of the observables (possibly an expansion to second order in the parameters).\n    observable_uncertainties_SM : jnp.array\n        The uncertainties of the polynomial coefficients of the observables for the SM entry.\n    polynomial_central : jnp.array\n        The central values of the polynomial coefficients.\n    eft : str\n        The EFT of the observable sector.\n    basis : str\n        The EFT basis of the observable sector.\n    scale : float, int\n        The renormalization scale of the observable sector.\n    sectors : list\n        The EFT sectors of the observable sector.\n    parameters : list\n        The parameters of the observable sector.\n    keys_pars_by_sectors : list\n        The keys of the parameters by sector.\n    keys_pars : list\n        The keys of the parameters.\n    sector_indices : dict\n        The indices of parameters from EFT sectors in the full parameter basis.\n    evolution_matrices : dict\n        The Renormalization Group evolution matrices.\n    construct_par_monomials_observable : function\n        The function that constructs the parameter monomials from the parameter array, for the polynomial coefficients of the observables (possibly corresponding to an expansion to second order in the parameters).\n    construct_par_monomials_polynomial : function\n        The function that constructs the parameter monomials from the parameter array, for the polynomial coefficients.\n    observable_expression_functions : list\n        The functions that evaluate the observable expressions in terms of the polynomial predictions.\n    prediction : function\n        The function that makes a prediction for the observable sector.\n\n    Methods\n    -------\n    get_prediction_data(eft, basis)\n        Get the data needed to make a prediction for a given EFT and basis.\n    _get_evolution_matrices(eft, basis)\n        Get the Renormalization Group evolution matrices for a given EFT and basis.\n    _get_prediction_function()\n        Get the function that makes a prediction for the observable sector.\n    _get_construct_par_monomials(keys_coeff)\n        Get the function that constructs the parameter monomials from the parameter array.\n    _get_observable_expression_function(i)\n        Get the function that evaluates a given observable expression in terms of the polynomial predictions.\n    get_class_prediction_data(eft, basis, observable_sector_names)\n        Get the data needed to make a prediction for a list of observable sectors.\n    get_class_prediction_function(observable_sector_names)\n        Get the function that makes a prediction for a list of observable sectors.\n    get_all_names(eft)\n        Get the names of all observable sectors.\n    get(name)\n        Get an observable sector by name.\n    get_all()\n        Get all observable sectors.\n\n    Examples\n    --------\n    Initialize an observable sector:\n\n    &gt;&gt;&gt; ObservableSector(json_data)\n\n    Load an observable sector from a json file:\n\n    &gt;&gt;&gt; ObservableSector.load('./observable_sector.json')\n\n    Load all observable sectors from a directory containing json files:\n\n    &gt;&gt;&gt; ObservableSector.load('./observable_sectors/')\n\n    Get the prediction data for the observable sector:\n\n    &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n\n    Make a prediction for the observable sector:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n    Get an observable sector by name:\n\n    &gt;&gt;&gt; ObservableSector.get('observable_sector_1')\n\n    Get all observable sectors:\n\n    &gt;&gt;&gt; ObservableSector.get_all()\n\n    Get the names of all observable sectors:\n    &gt;&gt;&gt; ObservableSector.get_all_names()\n\n    Get the names of all observable sectors that can provide predictions in the `SMEFT` basis `Warsaw`:\n    &gt;&gt;&gt; ObservableSector.get_all_names('SMEFT', 'Warsaw')\n\n    Get the names of all observable sectors that can provide predictions in the `WET` basis `flavio`:\n    &gt;&gt;&gt; ObservableSector.get_all_names('WET', 'flavio')\n\n    Get the names of all observable sectors that can provide predictions in the custom basis `custom_basis`:\n    &gt;&gt;&gt; ObservableSector.get_all_names(custom_basis='custom_basis')\n\n    Get the prediction data for a list of observable sectors:\n\n    &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n\n    Get the prediction function for a list of observable sectors:\n\n    &gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n\n    Make a prediction for a list of observable sectors:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n    &gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n    '''\n\n    _observable_sectors: Dict[str, 'ObservableSector'] = {}  # Class attribute to store all observable sectors\n    _popxf_versions = ['1.0'] # List of supported versions of the popxf JSON schema\n\n    def __init__(self, name: str,  json_data: Dict[str, Any])-&gt; None:\n        '''\n        Initialize an observable sector.\n\n        Parameters\n        ----------\n        name : str\n            The name of the observable sector.\n        json_data : dict\n            The JSON data containing the metadata and data of the observable sector.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Initialize an observable sector:\n\n        &gt;&gt;&gt; ObservableSector(json_data)\n        '''\n        self.name = name\n        self.metadata = json_data['metadata']\n        self.data = {\n            k: {ast.literal_eval(kk): vv for kk, vv in v.items()}\n            for k, v in json_data['data'].items()\n        }\n\n        self.observable_names = self.metadata['observable_names']\n        self.polynomial_names = self.metadata.get('polynomial_names', None)\n        self.observable_expressions = self.metadata.get('observable_expressions', None)\n\n        observable_central = self.data.get('observable_central')\n        self.keys_coeff_observable = sorted(observable_central.keys())\n        self.observable_central = jnp.array(np.array([observable_central[k] for k in self.keys_coeff_observable]))\n\n        observable_uncertainties = self.data.get('observable_uncertainties', None)\n        self.observable_uncertainties = np.array([observable_uncertainties[k] for k in self.keys_coeff_observable]) if observable_uncertainties else None\n        self.observable_uncertainties_SM = self.observable_uncertainties[0].copy() if observable_uncertainties else None\n\n        polynomial_central = self.data.get('polynomial_central', None)\n        self.keys_coeff_polynomial = sorted(polynomial_central.keys()) if polynomial_central else None\n        self.polynomial_central = jnp.array(np.array([polynomial_central[k] for k in self.keys_coeff_polynomial])) if self.keys_coeff_polynomial else None\n\n        self.parameters = self.metadata['parameters']\n        self.scale = self.metadata['scale']\n        if isinstance(self.scale, list):\n            raise NotImplementedError(\n                f\"The current version of ObservableSector does not support lists of scales.\\n\"\n                f\"Please use a single scale for {self.name}.\\n\"\n            )\n        wcxf = self.metadata['basis'].get('wcxf')\n        if wcxf:\n            self.eft = wcxf['eft']\n            self.basis = wcxf['basis']\n            self.custom_basis = None\n            self.sectors = sorted(chain.from_iterable(\n                supersectors.get(sector, [sector])\n                for sector in wcxf['sectors']\n                ))\n            if self.basis in bases_installed.get(self.eft, []):\n                self.basis_mode = 'rgevolve'\n                _parameter_basis = {sector: get_wc_basis(eft=self.eft, basis=self.basis, sector=sector)\n                                    for sector in self.sectors}\n            else:\n                self.basis_mode = 'wcxf'\n                _parameter_basis = {sector: get_wc_basis_from_wcxf(eft=self.eft, basis=self.basis, sector=sector)\n                                    for sector in self.sectors}\n                if self.basis in bases_available.get(self.eft, []):\n                    warnings.warn(\n                        f\"\\nRG evolution matrices for the {self.basis} basis in {self.eft} are not installed. \"\n                        f\"Falling back to fixed-scale mode: predictions will only be available at the fixed scale of {self.scale} GeV. \"\n                        f\"You can enable RG evolution by installing the corresponding module:\\n\"\n                        f\"    pip install rgevolve.{normalize(self.eft)}.{normalize(self.basis)}\",\n                        UserWarning,\n                        stacklevel=2\n                        )\n                else:\n                    warnings.warn(\n                        f\"\\nRG evolution matrices for the {self.basis} basis in {self.eft} are not available. \"\n                        f\"Falling back to fixed-scale mode: predictions will only be available at the fixed scale of {self.scale} GeV.\",\n                        UserWarning,\n                        stacklevel=2\n                        )\n            self.keys_pars_by_sectors = [\n                tuple(\n                    par_name\n                    for par_name in _parameter_basis[sector]\n                    if par_name[0] in self.parameters\n                ) for sector in self.sectors\n                ]\n\n            if self.basis_mode == 'rgevolve':\n                self.sector_indices = {\n                    eft: {\n                        basis: get_sector_indices(\n                            eft, basis,\n                            sectors = (\n                                sorted({matching_sectors[sector] for sector in self.sectors})\n                                if eft == 'SMEFT' and self.eft != 'SMEFT' else self.sectors\n                            )\n                        )\n                        for basis in bases_installed.get(eft, [])\n                    } for eft in efts_available.get(self.eft, [])\n                }\n                self.evolution_matrices = {\n                    eft: {\n                        basis: self._get_evolution_matrices(eft, basis)\n                        for basis in bases_installed.get(eft, [])\n                    } for eft in efts_available.get(self.eft, [])\n                }\n            else:\n                self.sector_indices = {\n                    self.eft: {\n                        self.basis: get_sector_indices_from_wcxf(\n                            self.eft, self.basis, self.sectors\n                        )\n                    }\n                }\n                shapes_in = [len(get_wc_basis_from_wcxf(self.eft, self.basis, sector)) for sector in self.sectors]\n                shapes_out = [len(keys_pars) for keys_pars in self.keys_pars_by_sectors]\n                self.evolution_matrices = {\n                    self.eft: {\n                        self.basis: self._get_unit_evolution_matrices(\n                            shapes_in, shapes_out, 1\n                        )\n                    }\n                }\n        else:\n            name = self.metadata['basis']['custom']['name']\n            custom_basis = CustomBasis.get(name)\n            if custom_basis:\n                self.eft = None\n                self.basis = None\n                self.custom_basis = name\n                self.sectors = [None]\n                _parameter_basis = custom_basis.get_parameter_basis()\n                self.basis_mode = 'custom'\n                self.keys_pars_by_sectors = [\n                    tuple(\n                        parameter_name\n                        for parameter_name in _parameter_basis\n                        if parameter_name[0] in self.parameters\n                    )\n                    ]\n                self.sector_indices = {\n                    None: {\n                        None: np.arange(len(_parameter_basis))\n                    }\n                }\n                shapes_in = [len(_parameter_basis)]\n                shapes_out = [len(self.keys_pars_by_sectors[0])]\n                self.evolution_matrices = {\n                    None: {\n                        None: self._get_unit_evolution_matrices(\n                            shapes_in, shapes_out, 1\n                        )\n                    }\n                }\n                warnings.warn(\n                    f\"\\nRG evolution matrices for the custom basis {self.custom_basis} are not available. \"\n                    f\"Falling back to fixed-scale mode: predictions will only be available at the fixed scale of {self.scale} GeV.\",\n                    UserWarning,\n                    stacklevel=2\n                )\n            else:\n                raise ValueError(\n                    f\"Basis {name} not found in CustomBasis. Please define it using `CustomBasis({name}, parameters)`.\"\n                )\n\n        self.keys_pars = [('', 'R')] + list(chain.from_iterable(self.keys_pars_by_sectors))\n\n        self.construct_par_monomials_observable = self._get_construct_par_monomials(self.keys_coeff_observable)\n        self.construct_par_monomials_polynomial = self._get_construct_par_monomials(self.keys_coeff_polynomial)\n\n        self.observable_expression_functions = [\n            self._get_observable_expression_function(i)\n            for i in range(len(self.observable_names))\n        ] if self.observable_expressions else None\n\n        self.prediction = self._get_prediction_function()\n\n        # Add observable sector to `_observable_sectors` class attribute\n        self._observable_sectors[self.name] = self\n\n    @classmethod\n    def load(cls, path: str) -&gt; None:\n        '''\n        Load an observable sector from a json file or several observable sectors from a directory containing json files.\n\n        Parameters\n        ----------\n        path : str\n            Path to a json file or a directory containing json files.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Load an observable sector from a json file:\n\n        &gt;&gt;&gt; ObservableSector.load('./observable_sector.json')\n\n        Load all observable sectors from a directory containing json files:\n\n        &gt;&gt;&gt; ObservableSector.load('./observable_sectors/')\n        '''\n        # load all json files in the directory\n        if os.path.isdir(path):\n            for file in os.listdir(path):\n                if file.endswith('.json'):\n                    cls._load_file(os.path.join(path, file))\n        # load single json file\n        else:\n            cls._load_file(path)\n\n    @classmethod\n    def _load_file(cls, path: str) -&gt; None:\n        '''\n        Load an observable sector from a json file.\n\n        Parameters\n        ----------\n        path : str\n            Path to a json file.\n\n        Returns\n        -------\n        None\n        '''\n        path = Path(path)\n        with path.open('r') as f:\n            json_data = json.load(f)\n        schema_name, schema_version = get_json_schema(json_data)\n        if schema_name == 'popxf' and schema_version in cls._popxf_versions:\n            cls(path.stem, json_data)\n\n\n    def get_prediction_data(self, eft: str, basis: str) -&gt; List[jnp.array]:\n        '''\n        Get the data needed to make a prediction for a given EFT and basis.\n\n        Parameters\n        ----------\n        eft : str\n            The EFT to make the prediction in.\n        basis : str\n            The basis to make the prediction in.\n\n        Returns\n        -------\n        list\n            A list containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients.\n\n        Examples\n        --------\n        Get the data needed to make a prediction in the Warsaw basis of the SMEFT:\n\n        &gt;&gt;&gt; observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n\n        Get the data needed to make a prediction in the flavio basis of the WET:\n\n        &gt;&gt;&gt; observable_sector.get_prediction_data('WET', 'flavio')\n        '''\n        return [\n            jnp.array(self.sector_indices[eft][basis]),\n            jnp.array(self.evolution_matrices[eft][basis], dtype=jnp.float32),\n            jnp.concatenate([\n                jnp.array(self.get_scales(eft, basis), dtype=jnp.float32),\n                jnp.array([jnp.nan], dtype=jnp.float32) # Add NaN to handle out-of-range cases\n            ]),\n            jnp.array(\n                self.observable_central if self.observable_expressions is None\n                else self.polynomial_central,\n                dtype=jnp.float64\n            ),\n        ]\n\n    def get_scales(self, eft: str, basis: str) -&gt; np.ndarray:\n        '''\n        Get the scales at which the Renormalization Group evolution matrices are defined for a given EFT and basis.\n\n        Parameters\n        ----------\n        eft : str\n            The EFT to get the Renormalization Group scales for.\n        basis : str\n            The basis to get the Renormalization Group scales for.\n\n        Returns\n        -------\n        np.ndarray\n            The scales at which the Renormalization Group evolution matrices are defined.\n        '''\n        if self.basis_mode == 'rgevolve':\n            return get_scales(eft, basis)\n        else:\n            return np.array([self.scale])\n\n    def _get_evolution_matrices(self, eft: str, basis: str) -&gt; np.ndarray:\n        '''\n        Get the Renormalization Group evolution matrices for a given EFT and basis.\n\n        Parameters\n        ----------\n        eft : str\n            The EFT to get the Renormalization Group evolution matrices for.\n        basis : str\n            The basis to get the Renormalization Group evolution matrices for.\n\n        Returns\n        -------\n        np.ndarray\n            The Renormalization Group evolution matrices.\n        '''\n        pars_out = dict(zip(self.sectors, self.keys_pars_by_sectors))\n        scales_in = get_scales(eft, basis)\n        if eft == 'SMEFT' and self.eft != 'SMEFT':\n            sectors_in = sorted({matching_sectors[sector] for sector in self.sectors})\n            shapes_in = [len(get_wc_basis(eft, basis, sector)) for sector in sectors_in]\n            shapes_out = [len(keys_pars) for keys_pars in self.keys_pars_by_sectors]\n        matrices_scales = []\n        for scale_in in scales_in:\n            matrices_sectors = []\n            for sector_out in self.sectors:\n                matrix_sector = run_and_match(\n                    eft_in=eft, eft_out=self.eft,\n                    basis_in=basis, basis_out=self.basis,\n                    scale_in=scale_in, scale_out=self.scale,\n                    sector_out=sector_out,\n                )\n                par_mask = get_wc_mask(self.eft, self.basis, sector_out, pars_out[sector_out])\n                matrices_sectors.append(matrix_sector[par_mask])\n            if eft == 'SMEFT' and self.eft != 'SMEFT':\n                matrix_scale = np.block([\n                    [\n                        matrices_sectors[i]\n                        if matching_sectors.get(self.sectors[i]) == sectors_in[j]\n                        else np.zeros((shapes_out[i], shapes_in[j]))\n                        for j in range(len(sectors_in))\n                    ]\n                    for i in range(len(self.sectors))\n                ])\n            else:\n                matrix_scale = scipy.linalg.block_diag(*matrices_sectors)\n            matrices_scales.append(matrix_scale)\n        return np.array(matrices_scales)\n\n    def _get_unit_evolution_matrices(self, shapes_in: List[int], shapes_out: List[int], n: int) -&gt; np.ndarray:\n        '''\n        Constructs a list of block-diagonal matrices composed of identity matrices.\n        Each identity matrix has shape (shapes_out[i], shapes_in[i]).\n\n        Parameters\n        ----------\n        shapes_in : list\n            List of input dimensions for each block.\n        shapes_out : list\n            List of output dimensions for each block.\n        n : int\n            Number of matrices to create.\n\n        Returns\n        -------\n        np.ndarray\n            An array of shape (n, ..., ...) containing block-diagonal matrices.\n\n        '''\n        matrices = []\n        for _ in range(n):\n            blocks = [np.eye(out_dim, in_dim) for in_dim, out_dim in zip(shapes_in, shapes_out)]\n            matrices.append(scipy.linalg.block_diag(*blocks))\n        return np.array(matrices)\n\n    def _get_prediction_function(self) -&gt; Callable[\n        [jnp.ndarray, Union[float, int, jnp.ndarray], List[jnp.ndarray]],\n        Tuple[jnp.ndarray, jnp.ndarray],\n    ]:\n        '''\n        Get the function that makes a prediction for the observable sector.\n\n        Returns\n        -------\n        function\n            The function that makes a prediction for the observable sector.\n\n            Parameters\n\n            par_array : jnp.ndarray\n                The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.\n\n            scale : float, int, or jnp.ndarray\n                The scale at which to make the prediction. If `par_array` has batch dimensions, `scale` can be a scalar or an array with the same shape as the batch dimensions of `par_array`. If `par_array` has no batch dimensions, `scale` can be a scalar or an array.\n\n            prediction_data : list\n                A list containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients.\n\n            Returns\n\n            jnp.ndarray\n                The observable predictions.\n\n            jnp.ndarray\n                The parameter monomials corresponding to the `observable_central` polynomial coefficients. These are used to compute the parameter dependence of the theoretical uncertainties.\n\n        Examples\n        --------\n        Get the prediction function for the observable sector:\n\n        &gt;&gt;&gt; prediction = observable_sector.prediction\n\n        Make a prediction for the observable sector:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n        &gt;&gt;&gt; scale = 1000\n        &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n        Make a prediction for the observable sector with batch dimensions in `par_array` and a scalar `scale`:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n        &gt;&gt;&gt; scale = 1000\n        &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n        Make a prediction for the observable sector with no batch dimensions in `par_array` and an array `scale`:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n        &gt;&gt;&gt; scale = jnp.array([1000, 2000])\n        &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n        Make a prediction for the observable sector with both `par_array` and `scale` having batch dimensions:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n        &gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n        &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n        '''\n        if self.observable_expressions is None:\n\n            # Define the prediction function for the case where there are no observable expressions\n            # In this case, the prediction is just the polynomial prediction\n            def prediction(\n                par_array: jnp.array, scale: Union[float, int, jnp.array],\n                prediction_data: List[jnp.array]\n            ) -&gt; Tuple[jnp.array, jnp.array]:\n                sector_indices, evolution_matrices, evolution_scales, polynomial_coefficients = prediction_data\n                par_array_sector = jnp.take(par_array, sector_indices, axis=-1)\n                par_array_evolved = interpolate_rg_evolution(\n                    par_array_sector, scale, evolution_matrices, evolution_scales\n                )\n                par_monomials = self.construct_par_monomials_observable(par_array_evolved)\n                polynomial_predictions = jnp.dot(par_monomials, polynomial_coefficients)\n                return polynomial_predictions, par_monomials\n\n        else:\n\n            # Define the prediction function for the case where there are observable expressions\n            # In this case, the prediction is the observable prediction evaluated in terms of the polynomial prediction\n            def prediction(\n                par_array: jnp.array, scale: Union[float, int, jnp.array],\n                prediction_data: List[jnp.array]\n            ) -&gt; Tuple[jnp.array, jnp.array]:\n                sector_indices, evolution_matrices, evolution_scales, polynomial_coefficients = prediction_data\n                par_array_sector = jnp.take(par_array, sector_indices, axis=-1)\n                par_array_evolved = interpolate_rg_evolution(\n                    par_array_sector, scale, evolution_matrices, evolution_scales\n                )\n                par_monomials = self.construct_par_monomials_polynomial(par_array_evolved)\n                par_monomials_expansion = self.construct_par_monomials_observable(par_array_evolved)\n                polynomial_predictions = jnp.dot(par_monomials, polynomial_coefficients)\n                observable_predictions = jnp.asarray([\n                    observable_expression_function(polynomial_predictions)\n                    for observable_expression_function in self.observable_expression_functions\n                ])\n                return jnp.moveaxis(observable_predictions, 0, -1), par_monomials_expansion\n\n        return prediction\n\n    def _get_construct_par_monomials(self, keys_coeff: List[tuple]) -&gt; Callable[[jnp.ndarray], jnp.ndarray]:\n        '''\n        Get the function that constructs the parameter monomials from the parameter array.\n\n        Parameters\n        ----------\n        keys_coeff : list\n            The keys of the polynomial coefficients.\n\n        Returns\n        -------\n        function\n            The function that constructs the parameter monomials from the parameter array.\n\n            Parameters\n\n            par_array : jnp.ndarray\n                The parameter array.\n\n            Returns\n\n            jnp.ndarray\n                The parameter monomials.\n        '''\n        if not keys_coeff:\n            return None\n        par_monomial_indices = get_par_monomial_indices(self.keys_pars, keys_coeff)\n\n        def construct_par_monomials(par_array: jnp.ndarray) -&gt; jnp.ndarray:\n\n            # insert 1 (in a batch-friendly way) to account for SM term\n            ones_column = jnp.ones((*par_array.shape[:-1], 1))\n            par_array = jnp.concatenate([ones_column, par_array], axis=-1)\n\n            par_monomial = batched_outer_ravel(par_array)\n            return jnp.take(par_monomial, par_monomial_indices, axis=-1)\n        return construct_par_monomials\n\n    def _get_observable_expression_function(self, i: int) -&gt; Callable[[jnp.ndarray], Union[float, jnp.ndarray]]:\n        '''\n        Get the function that evaluates a given observable expression in terms of the polynomial predictions.\n\n        Parameters\n        ----------\n        i : int\n            The index of the observable expression.\n\n        Returns\n        -------\n        function\n            The function that evaluates the observable expression in terms of the polynomial predictions.\n\n            Parameters\n\n            polynomial_predictions : jnp.ndarray\n                The polynomial predictions.\n\n            Returns\n\n            float or jnp.ndarray\n                The value of the observable expression evaluated in terms of the polynomial predictions.\n        '''\n\n        # Create a function from the observable expression string\n        s = (\n            'from jax.numpy import sqrt\\n'\n            'def observable_expression(terms):\\n'\n            '    {}, = terms\\n'\n            '    return {}'\n        ).format(\n            ', '.join(self.observable_expressions[i]['terms'].keys()),\n            self.observable_expressions[i]['expression'],\n        )\n        namespace = OrderedDict()\n        exec(s, namespace)\n        observable_expression = namespace.popitem()[1]\n\n        # Create the observable expression function that takes the polynomial predictions as input\n        polynomial_indices = jnp.array([\n            self.polynomial_names.index(v)\n            for v in self.observable_expressions[i]['terms'].values()\n        ])\n        def observable_expression_function(polynomial_predictions: jnp.ndarray) -&gt; Union[float, jnp.ndarray]:\n            selected_polynomial_predictions = jnp.take(polynomial_predictions, polynomial_indices, axis=-1)\n\n            # Move the last dimension to the first dimension to allow for unpacking of the terms in batch mode\n            return observable_expression(jnp.moveaxis(selected_polynomial_predictions, -1, 0))\n\n        return observable_expression_function\n\n    @classmethod\n    def get_class_prediction_data(cls, eft: str, basis: str, observable_sector_names: List[str]) -&gt; List[jnp.array]:\n        '''\n        Get the data needed to make a prediction for a list of observable sectors.\n\n        Parameters\n        ----------\n        eft : str\n            The EFT to make the prediction in.\n        basis : str\n            The basis to make the prediction in.\n        observable_sector_names : list\n            The names of the observable sectors to make the prediction for.\n\n        Returns\n        -------\n        list\n            A list of lists containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients for each observable sector.\n\n        Examples\n        --------\n        Get the data needed to make a prediction in the Warsaw basis of the SMEFT for a list of observable sectors:\n\n        &gt;&gt;&gt; ObservableSector.get_all_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n\n        Get the data needed to make a prediction in the flavio basis of the WET for a list of observable sectors:\n\n        &gt;&gt;&gt; ObservableSector.get_all_prediction_data('WET', 'flavio', ['observable_sector_1', 'observable_sector_2'])\n        '''\n        return [\n            cls._observable_sectors[name].get_prediction_data(eft, basis)\n            for name in observable_sector_names\n        ]\n\n    @classmethod\n    def get_class_prediction_function(cls, observable_sector_names: List[str]) -&gt; Callable[\n        [jnp.ndarray, Union[float, int, jnp.ndarray], List[jnp.ndarray]],\n        jnp.ndarray\n    ]:\n        '''\n        Get the function that makes a prediction for a list of observable sectors.\n\n        Parameters\n        ----------\n        observable_sector_names : list\n            The names of the observable sectors to make the prediction for.\n\n        Returns\n        -------\n        function\n            The function that makes a prediction for a list of observable sectors.\n\n            Parameters\n\n              - `par_array : jnp.ndarray`\n                The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.\n\n              - `scale : float, int, or jnp.ndarray`\n                The scale at which to make the prediction. If `par_array` has batch dimensions, `scale` can be a scalar or an array with the same shape as the batch dimensions of `par_array`. If `par_array` has no batch dimensions, `scale` can be a scalar or an array.\n\n              - `prediction_data : list`\n                A list of lists containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients for each observable sector.\n\n            Returns\n\n            `jnp.ndarray`\n                The observable predictions.\n\n        Examples\n        --------\n        Get the prediction function for a list of observable sectors:\n\n        &gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n\n        Make a prediction for a list of observable sectors:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n        &gt;&gt;&gt; scale = 1000\n        &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n        Make a prediction for a list of observable sectors with batch dimensions in `par_array` and a scalar `scale`:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n        &gt;&gt;&gt; scale = 1000\n        &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n        Make a prediction for a list of observable sectors with no batch dimensions in `par_array` and an array `scale`:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n        &gt;&gt;&gt; scale = jnp.array([1000, 2000])\n        &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n        Make a prediction for a list of observable sectors with both `par_array` and `scale` having batch dimensions:\n\n        &gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n        &gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n        &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n        &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n        '''\n        prediction_functions = [\n            cls._observable_sectors[name].prediction\n            for name in observable_sector_names\n        ]\n\n        def prediction(\n            par_array: jnp.array, scale: Union[float, int, jnp.array],\n            prediction_data: List[List[jnp.array]]\n        ) -&gt; jnp.array:\n            return jnp.concatenate([\n                prediction_function(par_array, scale, data)[0]\n                for prediction_function, data in zip(prediction_functions, prediction_data)\n            ], axis=-1)\n\n        return prediction\n\n    @classmethod\n    def get_all_names(cls, eft: Optional[str] = None, basis: Optional[str] = None, custom_basis: Optional[str]=None) -&gt; List[str]:\n        '''\n        Get the names of all observable sectors.\n\n        Parameters\n        ----------\n        eft : str, optional\n            The EFT for which the observable sectors can provide predictions.\n        basis : str, optional\n            The basis for which the observable sectors can provide predictions\n        custom_basis : str, optional\n            The custom basis for which the observable sectors can provide predictions.\n\n        Notes\n        -----\n        If all parameters are None, all observable sectors are returned.\n\n        Returns\n        -------\n        list\n            The names of all observable sectors.\n\n        Examples\n        --------\n        Get the names of all observable sectors:\n        &gt;&gt;&gt; ObservableSector.get_all_names()\n\n        Get the names of all observable sectors that can provide predictions in the `SMEFT` basis `Warsaw`:\n        &gt;&gt;&gt; ObservableSector.get_all_names('SMEFT', 'Warsaw')\n\n        Get the names of all observable sectors that can provide predictions in the `WET` basis `flavio`:\n        &gt;&gt;&gt; ObservableSector.get_all_names('WET', 'flavio')\n\n        Get the names of all observable sectors that can provide predictions in the custom basis `custom_basis`:\n        &gt;&gt;&gt; ObservableSector.get_all_names(custom_basis='custom_basis')\n        '''\n\n        if custom_basis is not None:\n            if eft is not None or basis is not None:\n                raise ValueError(\n                    'The custom_basis parameter cannot be used together with the eft or basis parameters.'\n                )\n            return sorted(\n                name\n                for name, observable_sector in cls._observable_sectors.items()\n                if observable_sector.custom_basis == custom_basis\n            )\n        elif eft is not None:\n            if basis is not None:\n                observable_sectors_wcxf = sorted(name for name, observable_sector in cls._observable_sectors.items()\n                                                 if observable_sector.basis_mode == 'wcxf'\n                                                 and observable_sector.basis == basis\n                                                 and observable_sector.eft == eft)\n                if observable_sectors_wcxf:\n                    return observable_sectors_wcxf\n                else:\n                    if basis in bases_installed.get(eft, []):\n                        return sorted(\n                            name\n                            for name, observable_sector in cls._observable_sectors.items()\n                            if observable_sector.basis_mode == 'rgevolve'\n                            and eft in efts_available.get(observable_sector.eft, [])\n                            )\n                    else:\n                        return []\n            else:\n                raise ValueError(\n                    'The basis parameter is required when the eft parameter is provided.'\n                )\n        elif basis is not None:\n            raise ValueError(\n                'The basis parameter is only valid when the eft parameter is also provided.'\n            )\n        else:\n            return sorted(cls._observable_sectors.keys())\n\n    @classmethod\n    def get(cls, name: str) -&gt; 'ObservableSector':\n        '''\n        Get an observable sector by name.\n\n        Parameters\n        ----------\n        name : str\n            The name of the observable sector.\n\n        Returns\n        -------\n        ObservableSector\n            The observable sector.\n\n        Examples\n        --------\n        Get an observable sector by name:\n\n        &gt;&gt;&gt; ObservableSector.get('observable_sector_1')\n        '''\n        return cls._observable_sectors[name]\n\n    @classmethod\n    def get_all(cls) -&gt; List['ObservableSector']:\n        '''\n        Get all observable sectors.\n\n        Returns\n        -------\n        list\n            A list of all observable sectors.\n\n        Examples\n        --------\n        Get all observable sectors:\n\n        &gt;&gt;&gt; ObservableSector.get_all()\n        '''\n        return [cls._observable_sectors[name] for name in cls.get_all_names()]\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.__init__","title":"<code>__init__(name, json_data)</code>","text":"<p>Initialize an observable sector.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the observable sector.</p> required <code>json_data</code> <code>dict</code> <p>The JSON data containing the metadata and data of the observable sector.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Initialize an observable sector:</p> <pre><code>&gt;&gt;&gt; ObservableSector(json_data)\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def __init__(self, name: str,  json_data: Dict[str, Any])-&gt; None:\n    '''\n    Initialize an observable sector.\n\n    Parameters\n    ----------\n    name : str\n        The name of the observable sector.\n    json_data : dict\n        The JSON data containing the metadata and data of the observable sector.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Initialize an observable sector:\n\n    &gt;&gt;&gt; ObservableSector(json_data)\n    '''\n    self.name = name\n    self.metadata = json_data['metadata']\n    self.data = {\n        k: {ast.literal_eval(kk): vv for kk, vv in v.items()}\n        for k, v in json_data['data'].items()\n    }\n\n    self.observable_names = self.metadata['observable_names']\n    self.polynomial_names = self.metadata.get('polynomial_names', None)\n    self.observable_expressions = self.metadata.get('observable_expressions', None)\n\n    observable_central = self.data.get('observable_central')\n    self.keys_coeff_observable = sorted(observable_central.keys())\n    self.observable_central = jnp.array(np.array([observable_central[k] for k in self.keys_coeff_observable]))\n\n    observable_uncertainties = self.data.get('observable_uncertainties', None)\n    self.observable_uncertainties = np.array([observable_uncertainties[k] for k in self.keys_coeff_observable]) if observable_uncertainties else None\n    self.observable_uncertainties_SM = self.observable_uncertainties[0].copy() if observable_uncertainties else None\n\n    polynomial_central = self.data.get('polynomial_central', None)\n    self.keys_coeff_polynomial = sorted(polynomial_central.keys()) if polynomial_central else None\n    self.polynomial_central = jnp.array(np.array([polynomial_central[k] for k in self.keys_coeff_polynomial])) if self.keys_coeff_polynomial else None\n\n    self.parameters = self.metadata['parameters']\n    self.scale = self.metadata['scale']\n    if isinstance(self.scale, list):\n        raise NotImplementedError(\n            f\"The current version of ObservableSector does not support lists of scales.\\n\"\n            f\"Please use a single scale for {self.name}.\\n\"\n        )\n    wcxf = self.metadata['basis'].get('wcxf')\n    if wcxf:\n        self.eft = wcxf['eft']\n        self.basis = wcxf['basis']\n        self.custom_basis = None\n        self.sectors = sorted(chain.from_iterable(\n            supersectors.get(sector, [sector])\n            for sector in wcxf['sectors']\n            ))\n        if self.basis in bases_installed.get(self.eft, []):\n            self.basis_mode = 'rgevolve'\n            _parameter_basis = {sector: get_wc_basis(eft=self.eft, basis=self.basis, sector=sector)\n                                for sector in self.sectors}\n        else:\n            self.basis_mode = 'wcxf'\n            _parameter_basis = {sector: get_wc_basis_from_wcxf(eft=self.eft, basis=self.basis, sector=sector)\n                                for sector in self.sectors}\n            if self.basis in bases_available.get(self.eft, []):\n                warnings.warn(\n                    f\"\\nRG evolution matrices for the {self.basis} basis in {self.eft} are not installed. \"\n                    f\"Falling back to fixed-scale mode: predictions will only be available at the fixed scale of {self.scale} GeV. \"\n                    f\"You can enable RG evolution by installing the corresponding module:\\n\"\n                    f\"    pip install rgevolve.{normalize(self.eft)}.{normalize(self.basis)}\",\n                    UserWarning,\n                    stacklevel=2\n                    )\n            else:\n                warnings.warn(\n                    f\"\\nRG evolution matrices for the {self.basis} basis in {self.eft} are not available. \"\n                    f\"Falling back to fixed-scale mode: predictions will only be available at the fixed scale of {self.scale} GeV.\",\n                    UserWarning,\n                    stacklevel=2\n                    )\n        self.keys_pars_by_sectors = [\n            tuple(\n                par_name\n                for par_name in _parameter_basis[sector]\n                if par_name[0] in self.parameters\n            ) for sector in self.sectors\n            ]\n\n        if self.basis_mode == 'rgevolve':\n            self.sector_indices = {\n                eft: {\n                    basis: get_sector_indices(\n                        eft, basis,\n                        sectors = (\n                            sorted({matching_sectors[sector] for sector in self.sectors})\n                            if eft == 'SMEFT' and self.eft != 'SMEFT' else self.sectors\n                        )\n                    )\n                    for basis in bases_installed.get(eft, [])\n                } for eft in efts_available.get(self.eft, [])\n            }\n            self.evolution_matrices = {\n                eft: {\n                    basis: self._get_evolution_matrices(eft, basis)\n                    for basis in bases_installed.get(eft, [])\n                } for eft in efts_available.get(self.eft, [])\n            }\n        else:\n            self.sector_indices = {\n                self.eft: {\n                    self.basis: get_sector_indices_from_wcxf(\n                        self.eft, self.basis, self.sectors\n                    )\n                }\n            }\n            shapes_in = [len(get_wc_basis_from_wcxf(self.eft, self.basis, sector)) for sector in self.sectors]\n            shapes_out = [len(keys_pars) for keys_pars in self.keys_pars_by_sectors]\n            self.evolution_matrices = {\n                self.eft: {\n                    self.basis: self._get_unit_evolution_matrices(\n                        shapes_in, shapes_out, 1\n                    )\n                }\n            }\n    else:\n        name = self.metadata['basis']['custom']['name']\n        custom_basis = CustomBasis.get(name)\n        if custom_basis:\n            self.eft = None\n            self.basis = None\n            self.custom_basis = name\n            self.sectors = [None]\n            _parameter_basis = custom_basis.get_parameter_basis()\n            self.basis_mode = 'custom'\n            self.keys_pars_by_sectors = [\n                tuple(\n                    parameter_name\n                    for parameter_name in _parameter_basis\n                    if parameter_name[0] in self.parameters\n                )\n                ]\n            self.sector_indices = {\n                None: {\n                    None: np.arange(len(_parameter_basis))\n                }\n            }\n            shapes_in = [len(_parameter_basis)]\n            shapes_out = [len(self.keys_pars_by_sectors[0])]\n            self.evolution_matrices = {\n                None: {\n                    None: self._get_unit_evolution_matrices(\n                        shapes_in, shapes_out, 1\n                    )\n                }\n            }\n            warnings.warn(\n                f\"\\nRG evolution matrices for the custom basis {self.custom_basis} are not available. \"\n                f\"Falling back to fixed-scale mode: predictions will only be available at the fixed scale of {self.scale} GeV.\",\n                UserWarning,\n                stacklevel=2\n            )\n        else:\n            raise ValueError(\n                f\"Basis {name} not found in CustomBasis. Please define it using `CustomBasis({name}, parameters)`.\"\n            )\n\n    self.keys_pars = [('', 'R')] + list(chain.from_iterable(self.keys_pars_by_sectors))\n\n    self.construct_par_monomials_observable = self._get_construct_par_monomials(self.keys_coeff_observable)\n    self.construct_par_monomials_polynomial = self._get_construct_par_monomials(self.keys_coeff_polynomial)\n\n    self.observable_expression_functions = [\n        self._get_observable_expression_function(i)\n        for i in range(len(self.observable_names))\n    ] if self.observable_expressions else None\n\n    self.prediction = self._get_prediction_function()\n\n    # Add observable sector to `_observable_sectors` class attribute\n    self._observable_sectors[self.name] = self\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector._get_construct_par_monomials","title":"<code>_get_construct_par_monomials(keys_coeff)</code>","text":"<p>Get the function that constructs the parameter monomials from the parameter array.</p> <p>Parameters:</p> Name Type Description Default <code>keys_coeff</code> <code>list</code> <p>The keys of the polynomial coefficients.</p> required <p>Returns:</p> Type Description <code>function</code> <p>The function that constructs the parameter monomials from the parameter array.</p> <p>Parameters</p> <p>par_array : jnp.ndarray     The parameter array.</p> <p>Returns</p> <p>jnp.ndarray     The parameter monomials.</p> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def _get_construct_par_monomials(self, keys_coeff: List[tuple]) -&gt; Callable[[jnp.ndarray], jnp.ndarray]:\n    '''\n    Get the function that constructs the parameter monomials from the parameter array.\n\n    Parameters\n    ----------\n    keys_coeff : list\n        The keys of the polynomial coefficients.\n\n    Returns\n    -------\n    function\n        The function that constructs the parameter monomials from the parameter array.\n\n        Parameters\n\n        par_array : jnp.ndarray\n            The parameter array.\n\n        Returns\n\n        jnp.ndarray\n            The parameter monomials.\n    '''\n    if not keys_coeff:\n        return None\n    par_monomial_indices = get_par_monomial_indices(self.keys_pars, keys_coeff)\n\n    def construct_par_monomials(par_array: jnp.ndarray) -&gt; jnp.ndarray:\n\n        # insert 1 (in a batch-friendly way) to account for SM term\n        ones_column = jnp.ones((*par_array.shape[:-1], 1))\n        par_array = jnp.concatenate([ones_column, par_array], axis=-1)\n\n        par_monomial = batched_outer_ravel(par_array)\n        return jnp.take(par_monomial, par_monomial_indices, axis=-1)\n    return construct_par_monomials\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector._get_evolution_matrices","title":"<code>_get_evolution_matrices(eft, basis)</code>","text":"<p>Get the Renormalization Group evolution matrices for a given EFT and basis.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The EFT to get the Renormalization Group evolution matrices for.</p> required <code>basis</code> <code>str</code> <p>The basis to get the Renormalization Group evolution matrices for.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The Renormalization Group evolution matrices.</p> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def _get_evolution_matrices(self, eft: str, basis: str) -&gt; np.ndarray:\n    '''\n    Get the Renormalization Group evolution matrices for a given EFT and basis.\n\n    Parameters\n    ----------\n    eft : str\n        The EFT to get the Renormalization Group evolution matrices for.\n    basis : str\n        The basis to get the Renormalization Group evolution matrices for.\n\n    Returns\n    -------\n    np.ndarray\n        The Renormalization Group evolution matrices.\n    '''\n    pars_out = dict(zip(self.sectors, self.keys_pars_by_sectors))\n    scales_in = get_scales(eft, basis)\n    if eft == 'SMEFT' and self.eft != 'SMEFT':\n        sectors_in = sorted({matching_sectors[sector] for sector in self.sectors})\n        shapes_in = [len(get_wc_basis(eft, basis, sector)) for sector in sectors_in]\n        shapes_out = [len(keys_pars) for keys_pars in self.keys_pars_by_sectors]\n    matrices_scales = []\n    for scale_in in scales_in:\n        matrices_sectors = []\n        for sector_out in self.sectors:\n            matrix_sector = run_and_match(\n                eft_in=eft, eft_out=self.eft,\n                basis_in=basis, basis_out=self.basis,\n                scale_in=scale_in, scale_out=self.scale,\n                sector_out=sector_out,\n            )\n            par_mask = get_wc_mask(self.eft, self.basis, sector_out, pars_out[sector_out])\n            matrices_sectors.append(matrix_sector[par_mask])\n        if eft == 'SMEFT' and self.eft != 'SMEFT':\n            matrix_scale = np.block([\n                [\n                    matrices_sectors[i]\n                    if matching_sectors.get(self.sectors[i]) == sectors_in[j]\n                    else np.zeros((shapes_out[i], shapes_in[j]))\n                    for j in range(len(sectors_in))\n                ]\n                for i in range(len(self.sectors))\n            ])\n        else:\n            matrix_scale = scipy.linalg.block_diag(*matrices_sectors)\n        matrices_scales.append(matrix_scale)\n    return np.array(matrices_scales)\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector._get_observable_expression_function","title":"<code>_get_observable_expression_function(i)</code>","text":"<p>Get the function that evaluates a given observable expression in terms of the polynomial predictions.</p> <p>Parameters:</p> Name Type Description Default <code>i</code> <code>int</code> <p>The index of the observable expression.</p> required <p>Returns:</p> Type Description <code>function</code> <p>The function that evaluates the observable expression in terms of the polynomial predictions.</p> <p>Parameters</p> <p>polynomial_predictions : jnp.ndarray     The polynomial predictions.</p> <p>Returns</p> <p>float or jnp.ndarray     The value of the observable expression evaluated in terms of the polynomial predictions.</p> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def _get_observable_expression_function(self, i: int) -&gt; Callable[[jnp.ndarray], Union[float, jnp.ndarray]]:\n    '''\n    Get the function that evaluates a given observable expression in terms of the polynomial predictions.\n\n    Parameters\n    ----------\n    i : int\n        The index of the observable expression.\n\n    Returns\n    -------\n    function\n        The function that evaluates the observable expression in terms of the polynomial predictions.\n\n        Parameters\n\n        polynomial_predictions : jnp.ndarray\n            The polynomial predictions.\n\n        Returns\n\n        float or jnp.ndarray\n            The value of the observable expression evaluated in terms of the polynomial predictions.\n    '''\n\n    # Create a function from the observable expression string\n    s = (\n        'from jax.numpy import sqrt\\n'\n        'def observable_expression(terms):\\n'\n        '    {}, = terms\\n'\n        '    return {}'\n    ).format(\n        ', '.join(self.observable_expressions[i]['terms'].keys()),\n        self.observable_expressions[i]['expression'],\n    )\n    namespace = OrderedDict()\n    exec(s, namespace)\n    observable_expression = namespace.popitem()[1]\n\n    # Create the observable expression function that takes the polynomial predictions as input\n    polynomial_indices = jnp.array([\n        self.polynomial_names.index(v)\n        for v in self.observable_expressions[i]['terms'].values()\n    ])\n    def observable_expression_function(polynomial_predictions: jnp.ndarray) -&gt; Union[float, jnp.ndarray]:\n        selected_polynomial_predictions = jnp.take(polynomial_predictions, polynomial_indices, axis=-1)\n\n        # Move the last dimension to the first dimension to allow for unpacking of the terms in batch mode\n        return observable_expression(jnp.moveaxis(selected_polynomial_predictions, -1, 0))\n\n    return observable_expression_function\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector._get_prediction_function","title":"<code>_get_prediction_function()</code>","text":"<p>Get the function that makes a prediction for the observable sector.</p> <p>Returns:</p> Type Description <code>function</code> <p>The function that makes a prediction for the observable sector.</p> <p>Parameters</p> <p>par_array : jnp.ndarray     The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.</p> <p>scale : float, int, or jnp.ndarray     The scale at which to make the prediction. If <code>par_array</code> has batch dimensions, <code>scale</code> can be a scalar or an array with the same shape as the batch dimensions of <code>par_array</code>. If <code>par_array</code> has no batch dimensions, <code>scale</code> can be a scalar or an array.</p> <p>prediction_data : list     A list containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients.</p> <p>Returns</p> <p>jnp.ndarray     The observable predictions.</p> <p>jnp.ndarray     The parameter monomials corresponding to the <code>observable_central</code> polynomial coefficients. These are used to compute the parameter dependence of the theoretical uncertainties.</p> <p>Examples:</p> <p>Get the prediction function for the observable sector:</p> <pre><code>&gt;&gt;&gt; prediction = observable_sector.prediction\n</code></pre> <p>Make a prediction for the observable sector:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> <p>Make a prediction for the observable sector with batch dimensions in <code>par_array</code> and a scalar <code>scale</code>:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> <p>Make a prediction for the observable sector with no batch dimensions in <code>par_array</code> and an array <code>scale</code>:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = jnp.array([1000, 2000])\n&gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> <p>Make a prediction for the observable sector with both <code>par_array</code> and <code>scale</code> having batch dimensions:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n&gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n&gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def _get_prediction_function(self) -&gt; Callable[\n    [jnp.ndarray, Union[float, int, jnp.ndarray], List[jnp.ndarray]],\n    Tuple[jnp.ndarray, jnp.ndarray],\n]:\n    '''\n    Get the function that makes a prediction for the observable sector.\n\n    Returns\n    -------\n    function\n        The function that makes a prediction for the observable sector.\n\n        Parameters\n\n        par_array : jnp.ndarray\n            The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.\n\n        scale : float, int, or jnp.ndarray\n            The scale at which to make the prediction. If `par_array` has batch dimensions, `scale` can be a scalar or an array with the same shape as the batch dimensions of `par_array`. If `par_array` has no batch dimensions, `scale` can be a scalar or an array.\n\n        prediction_data : list\n            A list containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients.\n\n        Returns\n\n        jnp.ndarray\n            The observable predictions.\n\n        jnp.ndarray\n            The parameter monomials corresponding to the `observable_central` polynomial coefficients. These are used to compute the parameter dependence of the theoretical uncertainties.\n\n    Examples\n    --------\n    Get the prediction function for the observable sector:\n\n    &gt;&gt;&gt; prediction = observable_sector.prediction\n\n    Make a prediction for the observable sector:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n    Make a prediction for the observable sector with batch dimensions in `par_array` and a scalar `scale`:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n    Make a prediction for the observable sector with no batch dimensions in `par_array` and an array `scale`:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = jnp.array([1000, 2000])\n    &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n    Make a prediction for the observable sector with both `par_array` and `scale` having batch dimensions:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n    &gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n    &gt;&gt;&gt; prediction_data = observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n    '''\n    if self.observable_expressions is None:\n\n        # Define the prediction function for the case where there are no observable expressions\n        # In this case, the prediction is just the polynomial prediction\n        def prediction(\n            par_array: jnp.array, scale: Union[float, int, jnp.array],\n            prediction_data: List[jnp.array]\n        ) -&gt; Tuple[jnp.array, jnp.array]:\n            sector_indices, evolution_matrices, evolution_scales, polynomial_coefficients = prediction_data\n            par_array_sector = jnp.take(par_array, sector_indices, axis=-1)\n            par_array_evolved = interpolate_rg_evolution(\n                par_array_sector, scale, evolution_matrices, evolution_scales\n            )\n            par_monomials = self.construct_par_monomials_observable(par_array_evolved)\n            polynomial_predictions = jnp.dot(par_monomials, polynomial_coefficients)\n            return polynomial_predictions, par_monomials\n\n    else:\n\n        # Define the prediction function for the case where there are observable expressions\n        # In this case, the prediction is the observable prediction evaluated in terms of the polynomial prediction\n        def prediction(\n            par_array: jnp.array, scale: Union[float, int, jnp.array],\n            prediction_data: List[jnp.array]\n        ) -&gt; Tuple[jnp.array, jnp.array]:\n            sector_indices, evolution_matrices, evolution_scales, polynomial_coefficients = prediction_data\n            par_array_sector = jnp.take(par_array, sector_indices, axis=-1)\n            par_array_evolved = interpolate_rg_evolution(\n                par_array_sector, scale, evolution_matrices, evolution_scales\n            )\n            par_monomials = self.construct_par_monomials_polynomial(par_array_evolved)\n            par_monomials_expansion = self.construct_par_monomials_observable(par_array_evolved)\n            polynomial_predictions = jnp.dot(par_monomials, polynomial_coefficients)\n            observable_predictions = jnp.asarray([\n                observable_expression_function(polynomial_predictions)\n                for observable_expression_function in self.observable_expression_functions\n            ])\n            return jnp.moveaxis(observable_predictions, 0, -1), par_monomials_expansion\n\n    return prediction\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector._get_unit_evolution_matrices","title":"<code>_get_unit_evolution_matrices(shapes_in, shapes_out, n)</code>","text":"<p>Constructs a list of block-diagonal matrices composed of identity matrices. Each identity matrix has shape (shapes_out[i], shapes_in[i]).</p> <p>Parameters:</p> Name Type Description Default <code>shapes_in</code> <code>list</code> <p>List of input dimensions for each block.</p> required <code>shapes_out</code> <code>list</code> <p>List of output dimensions for each block.</p> required <code>n</code> <code>int</code> <p>Number of matrices to create.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of shape (n, ..., ...) containing block-diagonal matrices.</p> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def _get_unit_evolution_matrices(self, shapes_in: List[int], shapes_out: List[int], n: int) -&gt; np.ndarray:\n    '''\n    Constructs a list of block-diagonal matrices composed of identity matrices.\n    Each identity matrix has shape (shapes_out[i], shapes_in[i]).\n\n    Parameters\n    ----------\n    shapes_in : list\n        List of input dimensions for each block.\n    shapes_out : list\n        List of output dimensions for each block.\n    n : int\n        Number of matrices to create.\n\n    Returns\n    -------\n    np.ndarray\n        An array of shape (n, ..., ...) containing block-diagonal matrices.\n\n    '''\n    matrices = []\n    for _ in range(n):\n        blocks = [np.eye(out_dim, in_dim) for in_dim, out_dim in zip(shapes_in, shapes_out)]\n        matrices.append(scipy.linalg.block_diag(*blocks))\n    return np.array(matrices)\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector._load_file","title":"<code>_load_file(path)</code>  <code>classmethod</code>","text":"<p>Load an observable sector from a json file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a json file.</p> required <p>Returns:</p> Type Description <code>None</code> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef _load_file(cls, path: str) -&gt; None:\n    '''\n    Load an observable sector from a json file.\n\n    Parameters\n    ----------\n    path : str\n        Path to a json file.\n\n    Returns\n    -------\n    None\n    '''\n    path = Path(path)\n    with path.open('r') as f:\n        json_data = json.load(f)\n    schema_name, schema_version = get_json_schema(json_data)\n    if schema_name == 'popxf' and schema_version in cls._popxf_versions:\n        cls(path.stem, json_data)\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get","title":"<code>get(name)</code>  <code>classmethod</code>","text":"<p>Get an observable sector by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the observable sector.</p> required <p>Returns:</p> Type Description <code>ObservableSector</code> <p>The observable sector.</p> <p>Examples:</p> <p>Get an observable sector by name:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get('observable_sector_1')\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef get(cls, name: str) -&gt; 'ObservableSector':\n    '''\n    Get an observable sector by name.\n\n    Parameters\n    ----------\n    name : str\n        The name of the observable sector.\n\n    Returns\n    -------\n    ObservableSector\n        The observable sector.\n\n    Examples\n    --------\n    Get an observable sector by name:\n\n    &gt;&gt;&gt; ObservableSector.get('observable_sector_1')\n    '''\n    return cls._observable_sectors[name]\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_all","title":"<code>get_all()</code>  <code>classmethod</code>","text":"<p>Get all observable sectors.</p> <p>Returns:</p> Type Description <code>list</code> <p>A list of all observable sectors.</p> <p>Examples:</p> <p>Get all observable sectors:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all()\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef get_all(cls) -&gt; List['ObservableSector']:\n    '''\n    Get all observable sectors.\n\n    Returns\n    -------\n    list\n        A list of all observable sectors.\n\n    Examples\n    --------\n    Get all observable sectors:\n\n    &gt;&gt;&gt; ObservableSector.get_all()\n    '''\n    return [cls._observable_sectors[name] for name in cls.get_all_names()]\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_all_names","title":"<code>get_all_names(eft=None, basis=None, custom_basis=None)</code>  <code>classmethod</code>","text":"<p>Get the names of all observable sectors.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The EFT for which the observable sectors can provide predictions.</p> <code>None</code> <code>basis</code> <code>str</code> <p>The basis for which the observable sectors can provide predictions</p> <code>None</code> <code>custom_basis</code> <code>str</code> <p>The custom basis for which the observable sectors can provide predictions.</p> <code>None</code> Notes <p>If all parameters are None, all observable sectors are returned.</p> <p>Returns:</p> Type Description <code>list</code> <p>The names of all observable sectors.</p> <p>Examples:</p> <p>Get the names of all observable sectors:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names()\n</code></pre> <p>Get the names of all observable sectors that can provide predictions in the <code>SMEFT</code> basis <code>Warsaw</code>:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names('SMEFT', 'Warsaw')\n</code></pre> <p>Get the names of all observable sectors that can provide predictions in the <code>WET</code> basis <code>flavio</code>:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names('WET', 'flavio')\n</code></pre> <p>Get the names of all observable sectors that can provide predictions in the custom basis <code>custom_basis</code>:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_names(custom_basis='custom_basis')\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef get_all_names(cls, eft: Optional[str] = None, basis: Optional[str] = None, custom_basis: Optional[str]=None) -&gt; List[str]:\n    '''\n    Get the names of all observable sectors.\n\n    Parameters\n    ----------\n    eft : str, optional\n        The EFT for which the observable sectors can provide predictions.\n    basis : str, optional\n        The basis for which the observable sectors can provide predictions\n    custom_basis : str, optional\n        The custom basis for which the observable sectors can provide predictions.\n\n    Notes\n    -----\n    If all parameters are None, all observable sectors are returned.\n\n    Returns\n    -------\n    list\n        The names of all observable sectors.\n\n    Examples\n    --------\n    Get the names of all observable sectors:\n    &gt;&gt;&gt; ObservableSector.get_all_names()\n\n    Get the names of all observable sectors that can provide predictions in the `SMEFT` basis `Warsaw`:\n    &gt;&gt;&gt; ObservableSector.get_all_names('SMEFT', 'Warsaw')\n\n    Get the names of all observable sectors that can provide predictions in the `WET` basis `flavio`:\n    &gt;&gt;&gt; ObservableSector.get_all_names('WET', 'flavio')\n\n    Get the names of all observable sectors that can provide predictions in the custom basis `custom_basis`:\n    &gt;&gt;&gt; ObservableSector.get_all_names(custom_basis='custom_basis')\n    '''\n\n    if custom_basis is not None:\n        if eft is not None or basis is not None:\n            raise ValueError(\n                'The custom_basis parameter cannot be used together with the eft or basis parameters.'\n            )\n        return sorted(\n            name\n            for name, observable_sector in cls._observable_sectors.items()\n            if observable_sector.custom_basis == custom_basis\n        )\n    elif eft is not None:\n        if basis is not None:\n            observable_sectors_wcxf = sorted(name for name, observable_sector in cls._observable_sectors.items()\n                                             if observable_sector.basis_mode == 'wcxf'\n                                             and observable_sector.basis == basis\n                                             and observable_sector.eft == eft)\n            if observable_sectors_wcxf:\n                return observable_sectors_wcxf\n            else:\n                if basis in bases_installed.get(eft, []):\n                    return sorted(\n                        name\n                        for name, observable_sector in cls._observable_sectors.items()\n                        if observable_sector.basis_mode == 'rgevolve'\n                        and eft in efts_available.get(observable_sector.eft, [])\n                        )\n                else:\n                    return []\n        else:\n            raise ValueError(\n                'The basis parameter is required when the eft parameter is provided.'\n            )\n    elif basis is not None:\n        raise ValueError(\n            'The basis parameter is only valid when the eft parameter is also provided.'\n        )\n    else:\n        return sorted(cls._observable_sectors.keys())\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_class_prediction_data","title":"<code>get_class_prediction_data(eft, basis, observable_sector_names)</code>  <code>classmethod</code>","text":"<p>Get the data needed to make a prediction for a list of observable sectors.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The EFT to make the prediction in.</p> required <code>basis</code> <code>str</code> <p>The basis to make the prediction in.</p> required <code>observable_sector_names</code> <code>list</code> <p>The names of the observable sectors to make the prediction for.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of lists containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients for each observable sector.</p> <p>Examples:</p> <p>Get the data needed to make a prediction in the Warsaw basis of the SMEFT for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n</code></pre> <p>Get the data needed to make a prediction in the flavio basis of the WET for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; ObservableSector.get_all_prediction_data('WET', 'flavio', ['observable_sector_1', 'observable_sector_2'])\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef get_class_prediction_data(cls, eft: str, basis: str, observable_sector_names: List[str]) -&gt; List[jnp.array]:\n    '''\n    Get the data needed to make a prediction for a list of observable sectors.\n\n    Parameters\n    ----------\n    eft : str\n        The EFT to make the prediction in.\n    basis : str\n        The basis to make the prediction in.\n    observable_sector_names : list\n        The names of the observable sectors to make the prediction for.\n\n    Returns\n    -------\n    list\n        A list of lists containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients for each observable sector.\n\n    Examples\n    --------\n    Get the data needed to make a prediction in the Warsaw basis of the SMEFT for a list of observable sectors:\n\n    &gt;&gt;&gt; ObservableSector.get_all_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n\n    Get the data needed to make a prediction in the flavio basis of the WET for a list of observable sectors:\n\n    &gt;&gt;&gt; ObservableSector.get_all_prediction_data('WET', 'flavio', ['observable_sector_1', 'observable_sector_2'])\n    '''\n    return [\n        cls._observable_sectors[name].get_prediction_data(eft, basis)\n        for name in observable_sector_names\n    ]\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_class_prediction_function","title":"<code>get_class_prediction_function(observable_sector_names)</code>  <code>classmethod</code>","text":"<p>Get the function that makes a prediction for a list of observable sectors.</p> <p>Parameters:</p> Name Type Description Default <code>observable_sector_names</code> <code>list</code> <p>The names of the observable sectors to make the prediction for.</p> required <p>Returns:</p> Type Description <code>function</code> <p>The function that makes a prediction for a list of observable sectors.</p> <p>Parameters</p> <ul> <li> <p><code>par_array : jnp.ndarray</code>     The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.</p> </li> <li> <p><code>scale : float, int, or jnp.ndarray</code>     The scale at which to make the prediction. If <code>par_array</code> has batch dimensions, <code>scale</code> can be a scalar or an array with the same shape as the batch dimensions of <code>par_array</code>. If <code>par_array</code> has no batch dimensions, <code>scale</code> can be a scalar or an array.</p> </li> <li> <p><code>prediction_data : list</code>     A list of lists containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients for each observable sector.</p> </li> </ul> <p>Returns</p> <p><code>jnp.ndarray</code>     The observable predictions.</p> <p>Examples:</p> <p>Get the prediction function for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n</code></pre> <p>Make a prediction for a list of observable sectors:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> <p>Make a prediction for a list of observable sectors with batch dimensions in <code>par_array</code> and a scalar <code>scale</code>:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> <p>Make a prediction for a list of observable sectors with no batch dimensions in <code>par_array</code> and an array <code>scale</code>:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = jnp.array([1000, 2000])\n&gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> <p>Make a prediction for a list of observable sectors with both <code>par_array</code> and <code>scale</code> having batch dimensions:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n&gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n&gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n&gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef get_class_prediction_function(cls, observable_sector_names: List[str]) -&gt; Callable[\n    [jnp.ndarray, Union[float, int, jnp.ndarray], List[jnp.ndarray]],\n    jnp.ndarray\n]:\n    '''\n    Get the function that makes a prediction for a list of observable sectors.\n\n    Parameters\n    ----------\n    observable_sector_names : list\n        The names of the observable sectors to make the prediction for.\n\n    Returns\n    -------\n    function\n        The function that makes a prediction for a list of observable sectors.\n\n        Parameters\n\n          - `par_array : jnp.ndarray`\n            The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.\n\n          - `scale : float, int, or jnp.ndarray`\n            The scale at which to make the prediction. If `par_array` has batch dimensions, `scale` can be a scalar or an array with the same shape as the batch dimensions of `par_array`. If `par_array` has no batch dimensions, `scale` can be a scalar or an array.\n\n          - `prediction_data : list`\n            A list of lists containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients for each observable sector.\n\n        Returns\n\n        `jnp.ndarray`\n            The observable predictions.\n\n    Examples\n    --------\n    Get the prediction function for a list of observable sectors:\n\n    &gt;&gt;&gt; prediction = ObservableSector.get_class_prediction_function(['observable_sector_1', 'observable_sector_2'])\n\n    Make a prediction for a list of observable sectors:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n    Make a prediction for a list of observable sectors with batch dimensions in `par_array` and a scalar `scale`:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n    Make a prediction for a list of observable sectors with no batch dimensions in `par_array` and an array `scale`:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = jnp.array([1000, 2000])\n    &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n\n    Make a prediction for a list of observable sectors with both `par_array` and `scale` having batch dimensions:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n    &gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n    &gt;&gt;&gt; prediction_data = ObservableSector.get_class_prediction_data('SMEFT', 'Warsaw', ['observable_sector_1', 'observable_sector_2'])\n    &gt;&gt;&gt; prediction(par_array, scale, prediction_data)\n    '''\n    prediction_functions = [\n        cls._observable_sectors[name].prediction\n        for name in observable_sector_names\n    ]\n\n    def prediction(\n        par_array: jnp.array, scale: Union[float, int, jnp.array],\n        prediction_data: List[List[jnp.array]]\n    ) -&gt; jnp.array:\n        return jnp.concatenate([\n            prediction_function(par_array, scale, data)[0]\n            for prediction_function, data in zip(prediction_functions, prediction_data)\n        ], axis=-1)\n\n    return prediction\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_prediction_data","title":"<code>get_prediction_data(eft, basis)</code>","text":"<p>Get the data needed to make a prediction for a given EFT and basis.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The EFT to make the prediction in.</p> required <code>basis</code> <code>str</code> <p>The basis to make the prediction in.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients.</p> <p>Examples:</p> <p>Get the data needed to make a prediction in the Warsaw basis of the SMEFT:</p> <pre><code>&gt;&gt;&gt; observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n</code></pre> <p>Get the data needed to make a prediction in the flavio basis of the WET:</p> <pre><code>&gt;&gt;&gt; observable_sector.get_prediction_data('WET', 'flavio')\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def get_prediction_data(self, eft: str, basis: str) -&gt; List[jnp.array]:\n    '''\n    Get the data needed to make a prediction for a given EFT and basis.\n\n    Parameters\n    ----------\n    eft : str\n        The EFT to make the prediction in.\n    basis : str\n        The basis to make the prediction in.\n\n    Returns\n    -------\n    list\n        A list containing the sector indices, evolution matrices, evolution scales, and polynomial coefficients.\n\n    Examples\n    --------\n    Get the data needed to make a prediction in the Warsaw basis of the SMEFT:\n\n    &gt;&gt;&gt; observable_sector.get_prediction_data('SMEFT', 'Warsaw')\n\n    Get the data needed to make a prediction in the flavio basis of the WET:\n\n    &gt;&gt;&gt; observable_sector.get_prediction_data('WET', 'flavio')\n    '''\n    return [\n        jnp.array(self.sector_indices[eft][basis]),\n        jnp.array(self.evolution_matrices[eft][basis], dtype=jnp.float32),\n        jnp.concatenate([\n            jnp.array(self.get_scales(eft, basis), dtype=jnp.float32),\n            jnp.array([jnp.nan], dtype=jnp.float32) # Add NaN to handle out-of-range cases\n        ]),\n        jnp.array(\n            self.observable_central if self.observable_expressions is None\n            else self.polynomial_central,\n            dtype=jnp.float64\n        ),\n    ]\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.get_scales","title":"<code>get_scales(eft, basis)</code>","text":"<p>Get the scales at which the Renormalization Group evolution matrices are defined for a given EFT and basis.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The EFT to get the Renormalization Group scales for.</p> required <code>basis</code> <code>str</code> <p>The basis to get the Renormalization Group scales for.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The scales at which the Renormalization Group evolution matrices are defined.</p> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def get_scales(self, eft: str, basis: str) -&gt; np.ndarray:\n    '''\n    Get the scales at which the Renormalization Group evolution matrices are defined for a given EFT and basis.\n\n    Parameters\n    ----------\n    eft : str\n        The EFT to get the Renormalization Group scales for.\n    basis : str\n        The basis to get the Renormalization Group scales for.\n\n    Returns\n    -------\n    np.ndarray\n        The scales at which the Renormalization Group evolution matrices are defined.\n    '''\n    if self.basis_mode == 'rgevolve':\n        return get_scales(eft, basis)\n    else:\n        return np.array([self.scale])\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.ObservableSector.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load an observable sector from a json file or several observable sectors from a directory containing json files.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to a json file or a directory containing json files.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Load an observable sector from a json file:</p> <pre><code>&gt;&gt;&gt; ObservableSector.load('./observable_sector.json')\n</code></pre> <p>Load all observable sectors from a directory containing json files:</p> <pre><code>&gt;&gt;&gt; ObservableSector.load('./observable_sectors/')\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; None:\n    '''\n    Load an observable sector from a json file or several observable sectors from a directory containing json files.\n\n    Parameters\n    ----------\n    path : str\n        Path to a json file or a directory containing json files.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Load an observable sector from a json file:\n\n    &gt;&gt;&gt; ObservableSector.load('./observable_sector.json')\n\n    Load all observable sectors from a directory containing json files:\n\n    &gt;&gt;&gt; ObservableSector.load('./observable_sectors/')\n    '''\n    # load all json files in the directory\n    if os.path.isdir(path):\n        for file in os.listdir(path):\n            if file.endswith('.json'):\n                cls._load_file(os.path.join(path, file))\n    # load single json file\n    else:\n        cls._load_file(path)\n</code></pre>"},{"location":"jelli/core/observable_sector/#jelli.core.observable_sector.interpolate_rg_evolution","title":"<code>interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)</code>","text":"<p>Interpolate the Renormalization Group evolution of the parameters.</p> <p>Parameters:</p> Name Type Description Default <code>par_array</code> <code>ndarray</code> <p>The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.</p> required <code>scale</code> <code>(float, int or ndarray)</code> <p>The scale at which to make the prediction. If <code>par_array</code> has batch dimensions, <code>scale</code> can be a scalar or an array with the same shape as the batch dimensions of <code>par_array</code>. If <code>par_array</code> has no batch dimensions, <code>scale</code> can be a scalar or an array.</p> required <code>evolution_matrices</code> <code>ndarray</code> <p>The Renormalization Group evolution matrices.</p> required <code>evolution_scales</code> <code>ndarray</code> <p>The scales at which the Renormalization Group evolution matrices are defined.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The evolved parameter array.</p> <p>Examples:</p> <p>Interpolate the Renormalization Group evolution of the parameters:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n</code></pre> <p>Interpolate the Renormalization Group evolution of the parameters with batch dimensions in <code>par_array</code> and a scalar <code>scale</code>:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n&gt;&gt;&gt; scale = 1000\n&gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n</code></pre> <p>Interpolate the Renormalization Group evolution of the parameters with no batch dimensions in <code>par_array</code> and an array <code>scale</code>:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n&gt;&gt;&gt; scale = jnp.array([1000, 2000])\n&gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n</code></pre> <p>Interpolate the Renormalization Group evolution of the parameters with both <code>par_array</code> and <code>scale</code> having batch dimensions:</p> <pre><code>&gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n&gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n&gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n</code></pre> Source code in <code>jelli/core/observable_sector.py</code> <pre><code>def interpolate_rg_evolution(\n    par_array: jnp.ndarray,\n    scale: Union[float, int, jnp.ndarray],\n    evolution_matrices: jnp.ndarray,\n    evolution_scales: jnp.ndarray\n) -&gt; jnp.ndarray:\n    '''\n    Interpolate the Renormalization Group evolution of the parameters.\n\n    Parameters\n    ----------\n    par_array : jnp.ndarray\n        The parameter array. The last dimension corresponds to the parameters. The other dimensions are batch dimensions.\n\n    scale : float, int or jnp.ndarray\n        The scale at which to make the prediction. If `par_array` has batch dimensions, `scale` can be a scalar or an array with the same shape as the batch dimensions of `par_array`. If `par_array` has no batch dimensions, `scale` can be a scalar or an array.\n\n    evolution_matrices : jnp.ndarray\n        The Renormalization Group evolution matrices.\n\n    evolution_scales : jnp.ndarray\n        The scales at which the Renormalization Group evolution matrices are defined.\n\n    Returns\n    -------\n    jnp.ndarray\n        The evolved parameter array.\n\n    Examples\n    --------\n    Interpolate the Renormalization Group evolution of the parameters:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n\n    Interpolate the Renormalization Group evolution of the parameters with batch dimensions in `par_array` and a scalar `scale`:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2, 5, 2499))*1e-7\n    &gt;&gt;&gt; scale = 1000\n    &gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n\n    Interpolate the Renormalization Group evolution of the parameters with no batch dimensions in `par_array` and an array `scale`:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(2499))*1e-7\n    &gt;&gt;&gt; scale = jnp.array([1000, 2000])\n    &gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n\n    Interpolate the Renormalization Group evolution of the parameters with both `par_array` and `scale` having batch dimensions:\n\n    &gt;&gt;&gt; par_array = jnp.array(np.random.rand(3, 2499))*1e-7\n    &gt;&gt;&gt; scale = jnp.array([1000, 2000, 3000])\n    &gt;&gt;&gt; interpolate_rg_evolution(par_array, scale, evolution_matrices, evolution_scales)\n    '''\n\n    # Searchsorted logic (supports batched scale)\n    index_low = jnp.searchsorted(evolution_scales, scale, side='right') - 1\n    index_high = index_low + 1\n\n    # Extract scales and matrices (supports batched indices)\n    scale_low = jnp.take(evolution_scales, index_low)\n    scale_high = jnp.take(evolution_scales, index_high)\n    matrix_low = jnp.take(evolution_matrices, index_low, axis=0)\n    matrix_high = jnp.take(evolution_matrices, index_high, axis=0)\n\n    # Expand scales for broadcasting with matrices\n    scale = jnp.expand_dims(scale, axis=(-2, -1))\n    scale_low = jnp.expand_dims(scale_low, axis=(-2, -1))\n    scale_high = jnp.expand_dims(scale_high, axis=(-2, -1))\n\n    # Logarithmic interpolation\n    matrix = jnp.where(\n        scale == scale_low,\n        matrix_low,\n        matrix_low + (matrix_high - matrix_low) * jnp.log(scale / scale_low) / jnp.log(scale_high / scale_low)\n    )\n\n    # Non-batched case (fast path)\n    if par_array.ndim == 1 and matrix.ndim == 2:\n        return jnp.dot(matrix, par_array)\n\n    # Ensure `par_array` behaves like a batch\n    if par_array.ndim == 1:\n        par_array = par_array[None, :]\n\n    # Add batch dimension to `matrix` if it\u2019s 2D (non-batched)\n    if matrix.ndim == 2:\n        matrix = matrix[None, :, :]\n\n    # Batched matrix-vector multiplication\n    return jnp.einsum('...ij,...j-&gt;...i', matrix, par_array)\n</code></pre>"},{"location":"jelli/core/theory_correlations/","title":"jelli.core.theory_correlations","text":""},{"location":"jelli/core/theory_correlations/#jelli.core.theory_correlations.TheoryCorrelations","title":"<code>TheoryCorrelations</code>","text":"<p>A class to represent theory correlations.</p> <p>Parameters:</p> Name Type Description Default <code>hash_val</code> <code>str</code> <p>The hash value representing the combination of row and column observable names.</p> required <code>data</code> <code>ndarray</code> <p>The data array containing the correlation data.</p> required <code>row_names</code> <code>Dict[str, int]</code> <p>A dictionary mapping row observable names to their indices.</p> required <code>col_names</code> <code>Dict[str, int]</code> <p>A dictionary mapping column observable names to their indices.</p> required <p>Attributes:</p> Name Type Description <code>hash_val</code> <code>str</code> <p>The hash value representing the combination of row and column observable names.</p> <code>data</code> <code>ndarray</code> <p>The data array containing the correlation data.</p> <code>row_names</code> <code>Dict[str, int]</code> <p>A dictionary mapping row observable names to their indices.</p> <code>col_names</code> <code>Dict[str, int]</code> <p>A dictionary mapping column observable names to their indices.</p> <code>_correlations</code> <code>Dict[str, TheoryCorrelations]</code> <p>A class attribute to cache all theory correlations.</p> <code>_covariance_scaled</code> <code>Dict[str, ndarray]</code> <p>A class attribute to cache scaled covariance matrices.</p> <code>_popxf_h5_versions</code> <code>Set[str]</code> <p>A set of supported versions of the popxf-h5 JSON schema.</p> <p>Methods:</p> Name Description <code>load</code> <p>Load theory correlations from HDF5 files in the specified path.</p> <code>_load_file</code> <p>Load theory correlations from a single HDF5 file.</p> <code>from_hdf5_group</code> <p>Create a TheoryCorrelations instance from an HDF5 group.</p> <code>get_data</code> <p>Get the correlation data for the specified row and column observable names.</p> <code>get_cov_scaled</code> <p>include_measurements: Iterable[str], row_names: Iterable[str], col_names: Iterable[str], std_th_scaled_row: np.ndarray, std_th_scaled_col: np.ndarray</p> <code>) -&gt; jnp.ndarray</code> <p>Get the scaled covariance matrix for the specified measurements, and row and column observable names.</p> <p>Examples:</p> <p>Load theory correlations from HDF5 files in a directory:</p> <pre><code>&gt;&gt;&gt; TheoryCorrelations.load('path/to/directory')\n</code></pre> <p>Load theory correlations from a single HDF5 file:</p> <pre><code>&gt;&gt;&gt; TheoryCorrelations.load('path/to/file.hdf5')\n</code></pre> <p>Get correlation data for specific row and column observable names:</p> <pre><code>&gt;&gt;&gt; data = TheoryCorrelations.get_data(['obs1', 'obs2'], ['obs3', 'obs4'])\n</code></pre> <p>Get scaled covariance matrix for specific measurements and observable names:</p> <pre><code>&gt;&gt;&gt; cov_scaled = TheoryCorrelations.get_cov_scaled(\n...     include_measurements=['meas1', 'meas2'],\n...     row_names=['obs1', 'obs2'],\n...     col_names=['obs3', 'obs4'],\n...     std_th_scaled_row=np.array([[0.1, 0.2], [0.3, 0.4]]),\n...     std_th_scaled_col=np.array([[0.5, 0.6], [0.7, 0.8]])\n</code></pre> Source code in <code>jelli/core/theory_correlations.py</code> <pre><code>class TheoryCorrelations:\n    '''\n    A class to represent theory correlations.\n\n    Parameters\n    ----------\n    hash_val : str\n        The hash value representing the combination of row and column observable names.\n    data : np.ndarray\n        The data array containing the correlation data.\n    row_names : Dict[str, int]\n        A dictionary mapping row observable names to their indices.\n    col_names : Dict[str, int]\n        A dictionary mapping column observable names to their indices.\n\n    Attributes\n    ----------\n    hash_val : str\n        The hash value representing the combination of row and column observable names.\n    data : np.ndarray\n        The data array containing the correlation data.\n    row_names : Dict[str, int]\n        A dictionary mapping row observable names to their indices.\n    col_names : Dict[str, int]\n        A dictionary mapping column observable names to their indices.\n    _correlations : Dict[str, 'TheoryCorrelations']\n        A class attribute to cache all theory correlations.\n    _covariance_scaled : Dict[str, jnp.ndarray]\n        A class attribute to cache scaled covariance matrices.\n    _popxf_h5_versions : Set[str]\n        A set of supported versions of the popxf-h5 JSON schema.\n\n    Methods\n    -------\n    load(path: str) -&gt; None\n        Load theory correlations from HDF5 files in the specified path.\n    _load_file(path: str) -&gt; None\n        Load theory correlations from a single HDF5 file.\n    from_hdf5_group(hash_val: str, hdf5_group: h5py.Group) -&gt; None\n        Create a TheoryCorrelations instance from an HDF5 group.\n    get_data(row_names: Iterable[str], col_names: Iterable[str]) -&gt; np.ndarray or None\n        Get the correlation data for the specified row and column observable names.\n    get_cov_scaled(\n        include_measurements: Iterable[str],\n        row_names: Iterable[str],\n        col_names: Iterable[str],\n        std_th_scaled_row: np.ndarray,\n        std_th_scaled_col: np.ndarray\n    ) -&gt; jnp.ndarray\n        Get the scaled covariance matrix for the specified measurements, and row and column observable names.\n\n    Examples\n    --------\n    Load theory correlations from HDF5 files in a directory:\n\n    &gt;&gt;&gt; TheoryCorrelations.load('path/to/directory')\n\n    Load theory correlations from a single HDF5 file:\n\n    &gt;&gt;&gt; TheoryCorrelations.load('path/to/file.hdf5')\n\n    Get correlation data for specific row and column observable names:\n\n    &gt;&gt;&gt; data = TheoryCorrelations.get_data(['obs1', 'obs2'], ['obs3', 'obs4'])\n\n    Get scaled covariance matrix for specific measurements and observable names:\n\n    &gt;&gt;&gt; cov_scaled = TheoryCorrelations.get_cov_scaled(\n    ...     include_measurements=['meas1', 'meas2'],\n    ...     row_names=['obs1', 'obs2'],\n    ...     col_names=['obs3', 'obs4'],\n    ...     std_th_scaled_row=np.array([[0.1, 0.2], [0.3, 0.4]]),\n    ...     std_th_scaled_col=np.array([[0.5, 0.6], [0.7, 0.8]])\n    '''\n\n    _correlations: Dict[str, 'TheoryCorrelations'] = {}\n    _covariance_scaled: Dict[str, jnp.ndarray] = {}\n    _popxf_h5_versions = {'1.0'} # Set of supported versions of the popxf-h5 JSON schema\n\n    def __init__(\n        self,\n        hash_val: str,\n        data: np.ndarray,\n        row_names: Dict[str, int],\n        col_names: Dict[str, int]\n    ) -&gt; None:\n        '''\n        Initialize an instance of the `TheoryCorrelations` class.\n\n        Parameters\n        ----------\n        hash_val : str\n            The hash value representing the combination of row and column observable names.\n        data : np.ndarray\n            The data array containing the correlation data.\n        row_names : Dict[str, int]\n            A dictionary mapping row observable names to their indices.\n        col_names : Dict[str, int]\n            A dictionary mapping column observable names to their indices.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Initialize a TheoryCorrelations instance:\n\n        &gt;&gt;&gt; theory_corr = TheoryCorrelations(...)\n        '''\n        self.hash_val = hash_val\n        self.data = data\n        self.row_names = row_names\n        self.col_names = col_names\n        self._correlations[hash_val] = self\n\n    @classmethod\n    def _load_file(cls, path: str) -&gt; None:\n        '''\n        Load theory correlations from a single HDF5 file.\n\n        Parameters\n        ----------\n        path : str\n            The path to the HDF5 file.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Load theory correlations from a single HDF5 file:\n\n        &gt;&gt;&gt; TheoryCorrelations._load_file('path/to/file.hdf5')\n        '''\n        with h5py.File(path, 'r') as f:\n            schema_name, schema_version = get_json_schema(dict(f.attrs))\n            if schema_name == 'popxf-h5' and schema_version in cls._popxf_h5_versions:\n                for hash_val in f:\n                    cls.from_hdf5_group(hash_val, f[hash_val])\n\n    @classmethod\n    def load(cls, path: str) -&gt; None:\n        '''\n        Load theory correlations from HDF5 files in the specified path.\n\n        Parameters\n        ----------\n        path : str\n            The path to a directory containing HDF5 files or a single HDF5 file.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Load theory correlations from HDF5 files in a directory:\n\n        &gt;&gt;&gt; TheoryCorrelations.load('path/to/directory')\n\n        Load theory correlations from a single HDF5 file:\n\n        &gt;&gt;&gt; TheoryCorrelations.load('path/to/file.hdf5')\n        '''\n        # load all hdf5 files in the directory\n        if os.path.isdir(path):\n            for file in os.listdir(path):\n                if file.endswith('.hdf5'):\n                    cls._load_file(os.path.join(path, file))\n        # load single hdf5 file\n        else:\n            cls._load_file(path)\n\n    @classmethod\n    def from_hdf5_group(cls, hash_val: str, hdf5_group: h5py.Group) -&gt; None:\n        '''\n        Create a `TheoryCorrelations` instance from an HDF5 group.\n\n        Parameters\n        ----------\n        hash_val : str\n            The hash value representing the combination of row and column observable names.\n        hdf5_group : h5py.Group\n            The HDF5 group containing the correlation data.\n\n        Returns\n        -------\n        None\n\n        Examples\n        --------\n        Create a `TheoryCorrelations` instance from an HDF5 group:\n\n        &gt;&gt;&gt; TheoryCorrelations.from_hdf5_group('hash_value', hdf5_group)\n        '''\n        data = hdf5_group['data']\n        data = np.array(data[()], dtype=np.float64) * data.attrs.get('scale', 1.0)\n        row_names = {name: i for i, name in enumerate(hdf5_group['row_names'][()].astype(str))}\n        col_names = {name: i for i, name in enumerate(hdf5_group['col_names'][()].astype(str))}\n        cls(hash_val, data, row_names, col_names)\n\n    @classmethod\n    def get_data(\n        cls,\n        row_names: Iterable[str],\n        col_names: Iterable[str],\n    ):\n        '''\n        Get the correlation data for the specified row and column observable names.\n\n        Parameters\n        ----------\n        row_names : Iterable[str]\n            The names of the row observables.\n        col_names : Iterable[str]\n            The names of the column observables.\n\n        Returns\n        -------\n        np.ndarray or None\n            The correlation data array if found, otherwise None.\n\n        Examples\n        --------\n        Get correlation data for specific row and column observable names:\n\n        &gt;&gt;&gt; data = TheoryCorrelations.get_data(['obs1', 'obs2'], ['obs3', 'obs4'])\n        '''\n        hash_val = hash_names(row_names, col_names)\n        if hash_val in cls._correlations:\n            data = cls._correlations[hash_val].data\n        else:\n            hash_val = hash_names(col_names, row_names)\n            if hash_val in cls._correlations:\n                data = np.moveaxis(\n                    cls._correlations[hash_val].data,\n                    [0,1,2,3], [1,0,3,2]\n                )\n            else:\n                data = None\n        return data\n\n    @classmethod\n    def get_cov_scaled(\n        cls,\n        include_measurements: Iterable[str],\n        row_names: Iterable[str],\n        col_names: Iterable[str],\n        std_th_scaled_row: np.ndarray,\n        std_th_scaled_col: np.ndarray,\n    ):\n        '''\n        Get the scaled covariance matrix for the specified measurements, and row and column observable names.\n\n        Parameters\n        ----------\n        include_measurements : Iterable[str]\n            The names of the measurements to include.\n        row_names : Iterable[str]\n            The names of the row observables.\n        col_names : Iterable[str]\n            The names of the column observables.\n        std_th_scaled_row : np.ndarray\n            The standard deviations for the row observables.\n        std_th_scaled_col : np.ndarray\n            The standard deviations for the column observables.\n\n        Returns\n        -------\n        jnp.ndarray\n            The scaled covariance matrix.\n\n        Examples\n        --------\n        Get scaled covariance matrix for specific measurements and observable names:\n\n        &gt;&gt;&gt; cov_scaled = TheoryCorrelations.get_cov_scaled(\n        ...     include_measurements=['meas1', 'meas2'],\n        ...     row_names=['obs1', 'obs2'],\n        ...     col_names=['obs3', 'obs4'],\n        ...     std_th_scaled_row=np.array([[0.1, 0.2], [0.3, 0.4]]),\n        ...     std_th_scaled_col=np.array([[0.5, 0.6], [0.7, 0.8]])\n        '''\n        row_measurements = Measurement.get_measurements(row_names, include_measurements=include_measurements)\n        col_measurements = Measurement.get_measurements(col_names, include_measurements=include_measurements)\n        hash_val = hash_names(row_measurements, col_measurements, row_names, col_names)\n        if hash_val in cls._covariance_scaled:\n            cov_scaled = cls._covariance_scaled[hash_val]\n        else:\n            corr = cls.get_data(row_names, col_names)\n            if corr is None:\n                raise ValueError(f\"Correlation data for {row_names} and {col_names} not found.\")\n            cov_scaled = corr * np.einsum('ki,lj-&gt;ijkl', std_th_scaled_row, std_th_scaled_col)\n            cov_scaled = jnp.array(cov_scaled, dtype=jnp.float64)\n            cls._covariance_scaled[hash_val] = cov_scaled\n        return cov_scaled\n</code></pre>"},{"location":"jelli/core/theory_correlations/#jelli.core.theory_correlations.TheoryCorrelations.__init__","title":"<code>__init__(hash_val, data, row_names, col_names)</code>","text":"<p>Initialize an instance of the <code>TheoryCorrelations</code> class.</p> <p>Parameters:</p> Name Type Description Default <code>hash_val</code> <code>str</code> <p>The hash value representing the combination of row and column observable names.</p> required <code>data</code> <code>ndarray</code> <p>The data array containing the correlation data.</p> required <code>row_names</code> <code>Dict[str, int]</code> <p>A dictionary mapping row observable names to their indices.</p> required <code>col_names</code> <code>Dict[str, int]</code> <p>A dictionary mapping column observable names to their indices.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Initialize a TheoryCorrelations instance:</p> <pre><code>&gt;&gt;&gt; theory_corr = TheoryCorrelations(...)\n</code></pre> Source code in <code>jelli/core/theory_correlations.py</code> <pre><code>def __init__(\n    self,\n    hash_val: str,\n    data: np.ndarray,\n    row_names: Dict[str, int],\n    col_names: Dict[str, int]\n) -&gt; None:\n    '''\n    Initialize an instance of the `TheoryCorrelations` class.\n\n    Parameters\n    ----------\n    hash_val : str\n        The hash value representing the combination of row and column observable names.\n    data : np.ndarray\n        The data array containing the correlation data.\n    row_names : Dict[str, int]\n        A dictionary mapping row observable names to their indices.\n    col_names : Dict[str, int]\n        A dictionary mapping column observable names to their indices.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Initialize a TheoryCorrelations instance:\n\n    &gt;&gt;&gt; theory_corr = TheoryCorrelations(...)\n    '''\n    self.hash_val = hash_val\n    self.data = data\n    self.row_names = row_names\n    self.col_names = col_names\n    self._correlations[hash_val] = self\n</code></pre>"},{"location":"jelli/core/theory_correlations/#jelli.core.theory_correlations.TheoryCorrelations._load_file","title":"<code>_load_file(path)</code>  <code>classmethod</code>","text":"<p>Load theory correlations from a single HDF5 file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to the HDF5 file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Load theory correlations from a single HDF5 file:</p> <pre><code>&gt;&gt;&gt; TheoryCorrelations._load_file('path/to/file.hdf5')\n</code></pre> Source code in <code>jelli/core/theory_correlations.py</code> <pre><code>@classmethod\ndef _load_file(cls, path: str) -&gt; None:\n    '''\n    Load theory correlations from a single HDF5 file.\n\n    Parameters\n    ----------\n    path : str\n        The path to the HDF5 file.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Load theory correlations from a single HDF5 file:\n\n    &gt;&gt;&gt; TheoryCorrelations._load_file('path/to/file.hdf5')\n    '''\n    with h5py.File(path, 'r') as f:\n        schema_name, schema_version = get_json_schema(dict(f.attrs))\n        if schema_name == 'popxf-h5' and schema_version in cls._popxf_h5_versions:\n            for hash_val in f:\n                cls.from_hdf5_group(hash_val, f[hash_val])\n</code></pre>"},{"location":"jelli/core/theory_correlations/#jelli.core.theory_correlations.TheoryCorrelations.from_hdf5_group","title":"<code>from_hdf5_group(hash_val, hdf5_group)</code>  <code>classmethod</code>","text":"<p>Create a <code>TheoryCorrelations</code> instance from an HDF5 group.</p> <p>Parameters:</p> Name Type Description Default <code>hash_val</code> <code>str</code> <p>The hash value representing the combination of row and column observable names.</p> required <code>hdf5_group</code> <code>Group</code> <p>The HDF5 group containing the correlation data.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Create a <code>TheoryCorrelations</code> instance from an HDF5 group:</p> <pre><code>&gt;&gt;&gt; TheoryCorrelations.from_hdf5_group('hash_value', hdf5_group)\n</code></pre> Source code in <code>jelli/core/theory_correlations.py</code> <pre><code>@classmethod\ndef from_hdf5_group(cls, hash_val: str, hdf5_group: h5py.Group) -&gt; None:\n    '''\n    Create a `TheoryCorrelations` instance from an HDF5 group.\n\n    Parameters\n    ----------\n    hash_val : str\n        The hash value representing the combination of row and column observable names.\n    hdf5_group : h5py.Group\n        The HDF5 group containing the correlation data.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Create a `TheoryCorrelations` instance from an HDF5 group:\n\n    &gt;&gt;&gt; TheoryCorrelations.from_hdf5_group('hash_value', hdf5_group)\n    '''\n    data = hdf5_group['data']\n    data = np.array(data[()], dtype=np.float64) * data.attrs.get('scale', 1.0)\n    row_names = {name: i for i, name in enumerate(hdf5_group['row_names'][()].astype(str))}\n    col_names = {name: i for i, name in enumerate(hdf5_group['col_names'][()].astype(str))}\n    cls(hash_val, data, row_names, col_names)\n</code></pre>"},{"location":"jelli/core/theory_correlations/#jelli.core.theory_correlations.TheoryCorrelations.get_cov_scaled","title":"<code>get_cov_scaled(include_measurements, row_names, col_names, std_th_scaled_row, std_th_scaled_col)</code>  <code>classmethod</code>","text":"<p>Get the scaled covariance matrix for the specified measurements, and row and column observable names.</p> <p>Parameters:</p> Name Type Description Default <code>include_measurements</code> <code>Iterable[str]</code> <p>The names of the measurements to include.</p> required <code>row_names</code> <code>Iterable[str]</code> <p>The names of the row observables.</p> required <code>col_names</code> <code>Iterable[str]</code> <p>The names of the column observables.</p> required <code>std_th_scaled_row</code> <code>ndarray</code> <p>The standard deviations for the row observables.</p> required <code>std_th_scaled_col</code> <code>ndarray</code> <p>The standard deviations for the column observables.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>The scaled covariance matrix.</p> <p>Examples:</p> <p>Get scaled covariance matrix for specific measurements and observable names:</p> <pre><code>&gt;&gt;&gt; cov_scaled = TheoryCorrelations.get_cov_scaled(\n...     include_measurements=['meas1', 'meas2'],\n...     row_names=['obs1', 'obs2'],\n...     col_names=['obs3', 'obs4'],\n...     std_th_scaled_row=np.array([[0.1, 0.2], [0.3, 0.4]]),\n...     std_th_scaled_col=np.array([[0.5, 0.6], [0.7, 0.8]])\n</code></pre> Source code in <code>jelli/core/theory_correlations.py</code> <pre><code>@classmethod\ndef get_cov_scaled(\n    cls,\n    include_measurements: Iterable[str],\n    row_names: Iterable[str],\n    col_names: Iterable[str],\n    std_th_scaled_row: np.ndarray,\n    std_th_scaled_col: np.ndarray,\n):\n    '''\n    Get the scaled covariance matrix for the specified measurements, and row and column observable names.\n\n    Parameters\n    ----------\n    include_measurements : Iterable[str]\n        The names of the measurements to include.\n    row_names : Iterable[str]\n        The names of the row observables.\n    col_names : Iterable[str]\n        The names of the column observables.\n    std_th_scaled_row : np.ndarray\n        The standard deviations for the row observables.\n    std_th_scaled_col : np.ndarray\n        The standard deviations for the column observables.\n\n    Returns\n    -------\n    jnp.ndarray\n        The scaled covariance matrix.\n\n    Examples\n    --------\n    Get scaled covariance matrix for specific measurements and observable names:\n\n    &gt;&gt;&gt; cov_scaled = TheoryCorrelations.get_cov_scaled(\n    ...     include_measurements=['meas1', 'meas2'],\n    ...     row_names=['obs1', 'obs2'],\n    ...     col_names=['obs3', 'obs4'],\n    ...     std_th_scaled_row=np.array([[0.1, 0.2], [0.3, 0.4]]),\n    ...     std_th_scaled_col=np.array([[0.5, 0.6], [0.7, 0.8]])\n    '''\n    row_measurements = Measurement.get_measurements(row_names, include_measurements=include_measurements)\n    col_measurements = Measurement.get_measurements(col_names, include_measurements=include_measurements)\n    hash_val = hash_names(row_measurements, col_measurements, row_names, col_names)\n    if hash_val in cls._covariance_scaled:\n        cov_scaled = cls._covariance_scaled[hash_val]\n    else:\n        corr = cls.get_data(row_names, col_names)\n        if corr is None:\n            raise ValueError(f\"Correlation data for {row_names} and {col_names} not found.\")\n        cov_scaled = corr * np.einsum('ki,lj-&gt;ijkl', std_th_scaled_row, std_th_scaled_col)\n        cov_scaled = jnp.array(cov_scaled, dtype=jnp.float64)\n        cls._covariance_scaled[hash_val] = cov_scaled\n    return cov_scaled\n</code></pre>"},{"location":"jelli/core/theory_correlations/#jelli.core.theory_correlations.TheoryCorrelations.get_data","title":"<code>get_data(row_names, col_names)</code>  <code>classmethod</code>","text":"<p>Get the correlation data for the specified row and column observable names.</p> <p>Parameters:</p> Name Type Description Default <code>row_names</code> <code>Iterable[str]</code> <p>The names of the row observables.</p> required <code>col_names</code> <code>Iterable[str]</code> <p>The names of the column observables.</p> required <p>Returns:</p> Type Description <code>ndarray or None</code> <p>The correlation data array if found, otherwise None.</p> <p>Examples:</p> <p>Get correlation data for specific row and column observable names:</p> <pre><code>&gt;&gt;&gt; data = TheoryCorrelations.get_data(['obs1', 'obs2'], ['obs3', 'obs4'])\n</code></pre> Source code in <code>jelli/core/theory_correlations.py</code> <pre><code>@classmethod\ndef get_data(\n    cls,\n    row_names: Iterable[str],\n    col_names: Iterable[str],\n):\n    '''\n    Get the correlation data for the specified row and column observable names.\n\n    Parameters\n    ----------\n    row_names : Iterable[str]\n        The names of the row observables.\n    col_names : Iterable[str]\n        The names of the column observables.\n\n    Returns\n    -------\n    np.ndarray or None\n        The correlation data array if found, otherwise None.\n\n    Examples\n    --------\n    Get correlation data for specific row and column observable names:\n\n    &gt;&gt;&gt; data = TheoryCorrelations.get_data(['obs1', 'obs2'], ['obs3', 'obs4'])\n    '''\n    hash_val = hash_names(row_names, col_names)\n    if hash_val in cls._correlations:\n        data = cls._correlations[hash_val].data\n    else:\n        hash_val = hash_names(col_names, row_names)\n        if hash_val in cls._correlations:\n            data = np.moveaxis(\n                cls._correlations[hash_val].data,\n                [0,1,2,3], [1,0,3,2]\n            )\n        else:\n            data = None\n    return data\n</code></pre>"},{"location":"jelli/core/theory_correlations/#jelli.core.theory_correlations.TheoryCorrelations.load","title":"<code>load(path)</code>  <code>classmethod</code>","text":"<p>Load theory correlations from HDF5 files in the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to a directory containing HDF5 files or a single HDF5 file.</p> required <p>Returns:</p> Type Description <code>None</code> <p>Examples:</p> <p>Load theory correlations from HDF5 files in a directory:</p> <pre><code>&gt;&gt;&gt; TheoryCorrelations.load('path/to/directory')\n</code></pre> <p>Load theory correlations from a single HDF5 file:</p> <pre><code>&gt;&gt;&gt; TheoryCorrelations.load('path/to/file.hdf5')\n</code></pre> Source code in <code>jelli/core/theory_correlations.py</code> <pre><code>@classmethod\ndef load(cls, path: str) -&gt; None:\n    '''\n    Load theory correlations from HDF5 files in the specified path.\n\n    Parameters\n    ----------\n    path : str\n        The path to a directory containing HDF5 files or a single HDF5 file.\n\n    Returns\n    -------\n    None\n\n    Examples\n    --------\n    Load theory correlations from HDF5 files in a directory:\n\n    &gt;&gt;&gt; TheoryCorrelations.load('path/to/directory')\n\n    Load theory correlations from a single HDF5 file:\n\n    &gt;&gt;&gt; TheoryCorrelations.load('path/to/file.hdf5')\n    '''\n    # load all hdf5 files in the directory\n    if os.path.isdir(path):\n        for file in os.listdir(path):\n            if file.endswith('.hdf5'):\n                cls._load_file(os.path.join(path, file))\n    # load single hdf5 file\n    else:\n        cls._load_file(path)\n</code></pre>"},{"location":"jelli/utils/data_io/","title":"jelli.utils.data_io","text":""},{"location":"jelli/utils/data_io/#jelli.utils.data_io.escape","title":"<code>escape(name)</code>","text":"<p>Escape special characters in a name for hashing.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name to be escaped.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The escaped name.</p> Source code in <code>jelli/utils/data_io.py</code> <pre><code>def escape(name: str) -&gt; str:\n    '''\n    Escape special characters in a name for hashing.\n\n    Parameters\n    ----------\n    name : str\n        The name to be escaped.\n\n    Returns\n    -------\n    str\n        The escaped name.\n    '''\n    return name.replace('\\\\', '\\\\\\\\').replace('|', '\\\\|')\n</code></pre>"},{"location":"jelli/utils/data_io/#jelli.utils.data_io.get_json_schema","title":"<code>get_json_schema(json_data)</code>","text":"<p>Extract the schema name and version from the JSON data.</p> <p>Parameters:</p> Name Type Description Default <code>json_data</code> <code>dict</code> <p>The JSON data containing the schema information.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the schema name and version. If not found, returns <code>(None, None)</code>.</p> Source code in <code>jelli/utils/data_io.py</code> <pre><code>def get_json_schema(json_data):\n    '''\n    Extract the schema name and version from the JSON data.\n\n    Parameters\n    ----------\n    json_data : dict\n        The JSON data containing the schema information.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the schema name and version. If not found, returns `(None, None)`.\n    '''\n    schema_name = None\n    schema_version = None\n    if '$schema' in json_data:\n        schema = json_data['$schema']\n        if isinstance(schema, (np.ndarray, list)):\n            schema = str(schema[0])\n        else:\n            schema = str(schema)\n        match = json_schema_name_pattern.search(schema)\n        if match:\n            schema_name = match.group(1)\n            schema_version = match.group(3)\n    return schema_name, schema_version\n</code></pre>"},{"location":"jelli/utils/data_io/#jelli.utils.data_io.hash_names","title":"<code>hash_names(*name_groups)</code>","text":"<p>Generate a unique hash for a combination of name groups.</p> <p>Parameters:</p> Name Type Description Default <code>*name_groups</code> <code>Iterable[str]</code> <p>Variable number of iterables, each containing names (strings).</p> <code>()</code> <p>Returns:</p> Type Description <code>str</code> <p>A hexadecimal MD5 hash representing the combination of name groups.</p> Source code in <code>jelli/utils/data_io.py</code> <pre><code>def hash_names(*name_groups: Iterable[str]) -&gt; str:\n    '''\n    Generate a unique hash for a combination of name groups.\n\n    Parameters\n    ----------\n    *name_groups : Iterable[str]\n        Variable number of iterables, each containing names (strings).\n\n    Returns\n    -------\n    str\n        A hexadecimal MD5 hash representing the combination of name groups.\n    '''\n    parts = []\n    for group in name_groups:\n        if group:\n            escaped = '|'.join(escape(o) for o in sorted(group))\n            parts.append(escaped)\n    block_id = '||'.join(parts)\n    return hashlib.md5(block_id.encode('utf-8')).hexdigest()\n</code></pre>"},{"location":"jelli/utils/data_io/#jelli.utils.data_io.pad_arrays","title":"<code>pad_arrays(arrays)</code>","text":"<p>Pad arrays to the same length by repeating the last element.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>list of np.ndarray</code> <p>List of 1D numpy arrays to be padded.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D numpy array where each row corresponds to a padded input array.</p> Source code in <code>jelli/utils/data_io.py</code> <pre><code>def pad_arrays(arrays):\n    '''\n    Pad arrays to the same length by repeating the last element.\n\n    Parameters\n    ----------\n    arrays : list of np.ndarray\n        List of 1D numpy arrays to be padded.\n\n    Returns\n    -------\n    np.ndarray\n        A 2D numpy array where each row corresponds to a padded input array.\n    '''\n    max_len = max(len(arr) for arr in arrays)\n    return np.array([\n        np.pad(arr, (0, max_len - len(arr)), mode='edge')\n        for arr in arrays\n    ])\n</code></pre>"},{"location":"jelli/utils/distributions/","title":"jelli.utils.distributions","text":""},{"location":"jelli/utils/distributions/#jelli.utils.distributions.combine_distributions_numerically","title":"<code>combine_distributions_numerically(constraints, n_points=1000)</code>","text":"<p>Combine multiple distributions into a single numerical distribution by summing their logpdfs on a common support.</p> <p>Parameters:</p> Name Type Description Default <code>constraints</code> <code>Dict[str, Dict[str, ndarray]]</code> <p>A dictionary where keys are distribution types (e.g., <code>NumericalDistribution</code>, <code>NormalDistribution</code>, etc.) and values are dictionaries containing distribution information such as <code>central_value</code>, <code>standard_deviation</code>, etc.</p> required <code>n_points</code> <code>int</code> <p>Number of points in the common support for the output distribution. Default is <code>1000</code>.</p> <code>1000</code> <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>A dictionary containing the combined numerical distribution information, including <code>measurement_name</code>, <code>observables</code>, <code>observable_indices</code>, <code>x</code>, <code>y</code>, and <code>log_y</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; constraints = {\n...     'NormalDistribution': {\n...         'measurement_name': np.array(['measurement1']),\n...         'observables': np.array(['observable1']),\n...         'observable_indices': np.array([0]),\n...         'central_value': np.array([1.0]),\n...         'standard_deviation': np.array([0.8])\n...     },\n...     'HalfNormalDistribution': {\n...         'measurement_name': np.array(['measurement2', 'measurement3']),\n...         'observables': np.array(['observable1', 'observable1']),\n...         'observable_indices': np.array([0, 0]),\n...         'standard_deviation': np.array([0.3, 0.4])\n...     }\n... }\n&gt;&gt;&gt; combine_distributions(constraints, n_points=1000)\n{\n    'measurement_name': np.array(['measurement1, measurement2, measurement3']),\n    'observables': np.array(['observable1']),\n    'observable_indices': np.array([0]),\n    'x': np.array([...]),  # combined support\n    'y': np.array([...]),  # combined pdf values\n    'log_y': np.array([...])  # combined log pdf values\n}\n</code></pre> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def combine_distributions_numerically(\n        constraints: Dict[str, Dict[str, np.ndarray]],\n        n_points: int = 1000,\n) -&gt; Dict[str, np.ndarray]:\n    '''\n    Combine multiple distributions into a single numerical distribution by summing their logpdfs on a common support.\n\n    Parameters\n    ----------\n    constraints : Dict[str, Dict[str, np.ndarray]]\n        A dictionary where keys are distribution types (e.g., `NumericalDistribution`, `NormalDistribution`, etc.)\n        and values are dictionaries containing distribution information such as `central_value`, `standard_deviation`, etc.\n    n_points : int, optional\n        Number of points in the common support for the output distribution. Default is `1000`.\n\n    Returns\n    -------\n    Dict[str, np.ndarray]\n        A dictionary containing the combined numerical distribution information, including `measurement_name`, `observables`,\n        `observable_indices`, `x`, `y`, and `log_y`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; constraints = {\n    ...     'NormalDistribution': {\n    ...         'measurement_name': np.array(['measurement1']),\n    ...         'observables': np.array(['observable1']),\n    ...         'observable_indices': np.array([0]),\n    ...         'central_value': np.array([1.0]),\n    ...         'standard_deviation': np.array([0.8])\n    ...     },\n    ...     'HalfNormalDistribution': {\n    ...         'measurement_name': np.array(['measurement2', 'measurement3']),\n    ...         'observables': np.array(['observable1', 'observable1']),\n    ...         'observable_indices': np.array([0, 0]),\n    ...         'standard_deviation': np.array([0.3, 0.4])\n    ...     }\n    ... }\n    &gt;&gt;&gt; combine_distributions(constraints, n_points=1000)\n    {\n        'measurement_name': np.array(['measurement1, measurement2, measurement3']),\n        'observables': np.array(['observable1']),\n        'observable_indices': np.array([0]),\n        'x': np.array([...]),  # combined support\n        'y': np.array([...]),  # combined pdf values\n        'log_y': np.array([...])  # combined log pdf values\n    }\n    '''\n\n    # get universal parameters for output\n    dist_info = next(iter(constraints.values()))\n    observables_out = dist_info['observables'][:1]\n    observable_indices_out = dist_info['observable_indices'][:1]\n\n    # get measurement names in each constraint and supports of distributions\n    measurement_names = []\n    supports = []\n    for dist_type, dist_info in constraints.items():\n        supports.append(\n            get_distribution_support(dist_type, dist_info)\n        )\n        measurement_names.append(dist_info['measurement_name'])\n\n    # combine measurement names for output\n    measurement_name_out = np.expand_dims(', '.join(np.unique(np.concatenate(measurement_names))), axis=0)\n\n    # common support for all distributions\n    support_min = np.min(np.concatenate([s[0] for s in supports]))\n    support_max = np.max(np.concatenate([s[1] for s in supports]))\n    xp_out = np.linspace(support_min, support_max, n_points)\n\n    # sum the logpdfs of all distributions on the common support\n    log_fp_out = np.zeros_like(xp_out)\n    for dist_type, dist_info in constraints.items():\n        unique_observables = np.unique(dist_info['observables'])\n        if len(unique_observables) &gt; 1 or unique_observables[0] != observables_out[0]:\n            raise ValueError(f\"Only distributions constraining the same observable can be combined.\")\n        n_constraints = len(dist_info['observables'])\n        x = np.broadcast_to(xp_out, (n_constraints, n_points)).reshape(-1)\n        observable_indices = np.arange(len(x))\n        selector_matrix = np.concatenate([np.eye(n_points)]*n_constraints, axis=1)\n        if dist_type == 'NumericalDistribution':\n            xp = dist_info['x']\n            log_fp = dist_info['log_y']\n            xp = np.broadcast_to(xp[:, None, :], (xp.shape[0], n_points, xp.shape[1]))\n            xp = xp.reshape(-1, xp.shape[2])\n            log_fp = np.broadcast_to(log_fp[:, None, :], (log_fp.shape[0], n_points, log_fp.shape[1]))\n            log_fp = log_fp.reshape(-1, log_fp.shape[2])\n            log_fp_out += logpdf_functions_summed[dist_type](\n                x,\n                selector_matrix,\n                observable_indices,\n                xp,\n                log_fp,\n            )\n        elif dist_type == 'NormalDistribution':\n            central_value = np.broadcast_to(dist_info['central_value'], (n_points, n_constraints)).T.reshape(-1)\n            standard_deviation = np.broadcast_to(dist_info['standard_deviation'], (n_points, n_constraints)).T.reshape(-1)\n            log_fp_out += logpdf_functions_summed[dist_type](\n                x,\n                selector_matrix,\n                observable_indices,\n                central_value,\n                standard_deviation,\n            )\n        elif dist_type == 'HalfNormalDistribution':\n            standard_deviation = np.broadcast_to(dist_info['standard_deviation'], (n_points, n_constraints)).T.reshape(-1)\n            log_fp_out += logpdf_functions_summed[dist_type](\n                x,\n                selector_matrix,\n                observable_indices,\n                standard_deviation,\n            )\n        elif dist_type == 'GammaDistributionPositive':\n            a = np.broadcast_to(dist_info['a'], (n_points, n_constraints)).T.reshape(-1)\n            loc = np.broadcast_to(dist_info['loc'], (n_points, n_constraints)).T.reshape(-1)\n            scale = np.broadcast_to(dist_info['scale'], (n_points, n_constraints)).T.reshape(-1)\n            log_fp_out += logpdf_functions_summed[dist_type](\n                x,\n                selector_matrix,\n                observable_indices,\n                a,\n                loc,\n                scale,\n            )\n        else:\n            raise NotImplementedError(f\"Combining distributions not implemented for {dist_type}.\")\n\n    # normalize the output distribution\n    log_fp_out -= log_trapz_exp(log_fp_out, xp_out)\n\n    return {\n        'measurement_name': measurement_name_out,\n        'observables': observables_out,\n        'observable_indices': observable_indices_out,\n        'x': xp_out,\n        'y': np.exp(log_fp_out),\n        'log_y': log_fp_out,\n    }\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.combine_normal_distributions","title":"<code>combine_normal_distributions(measurement_name, observables, observable_indices, central_value, standard_deviation)</code>","text":"<p>Combine multiple normal distributions into a single normal distribution.</p> <p>Parameters:</p> Name Type Description Default <code>measurement_name</code> <code>ndarray</code> <p>Names of the measurements.</p> required <code>observables</code> <code>ndarray</code> <p>Names of the observables.</p> required <code>observable_indices</code> <code>ndarray</code> <p>Indices of the observables.</p> required <code>central_value</code> <code>ndarray</code> <p>Central values of the normal distributions.</p> required <code>standard_deviation</code> <code>ndarray</code> <p>Standard deviations of the normal distributions.</p> required <p>Returns:</p> Type Description <code>Dict[str, ndarray]</code> <p>A dictionary containing the combined measurement name, observables, observable indices, central value, and standard deviation.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; combine_normal_distributions(\n...     measurement_name=np.array(['measurement1', 'measurement2']),\n...     observables=np.array(['observable1', 'observable1']),\n...     observable_indices=np.array([3, 3]),\n...     central_value=np.array([1.0, 2.0]),\n...     standard_deviation=np.array([0.1, 0.2])\n... )\n{\n    'measurement_name': np.array(['measurement1, measurement2']),\n    'observables': np.array(['observable1']),\n    'observable_indices': np.array([3]),\n    'central_value': np.array([1.2]),\n    'standard_deviation': np.array([0.08944272])\n}\n</code></pre> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def combine_normal_distributions(\n        measurement_name: np.ndarray,\n        observables: np.ndarray,\n        observable_indices: np.ndarray,\n        central_value: np.ndarray,\n        standard_deviation: np.ndarray,\n    ) -&gt; Dict[str, np.ndarray]:\n    '''\n    Combine multiple normal distributions into a single normal distribution.\n\n    Parameters\n    ----------\n    measurement_name : np.ndarray\n        Names of the measurements.\n    observables : np.ndarray\n        Names of the observables.\n    observable_indices : np.ndarray\n        Indices of the observables.\n    central_value : np.ndarray\n        Central values of the normal distributions.\n    standard_deviation : np.ndarray\n        Standard deviations of the normal distributions.\n\n    Returns\n    -------\n    Dict[str, np.ndarray]\n        A dictionary containing the combined measurement name, observables, observable indices,\n        central value, and standard deviation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; combine_normal_distributions(\n    ...     measurement_name=np.array(['measurement1', 'measurement2']),\n    ...     observables=np.array(['observable1', 'observable1']),\n    ...     observable_indices=np.array([3, 3]),\n    ...     central_value=np.array([1.0, 2.0]),\n    ...     standard_deviation=np.array([0.1, 0.2])\n    ... )\n    {\n        'measurement_name': np.array(['measurement1, measurement2']),\n        'observables': np.array(['observable1']),\n        'observable_indices': np.array([3]),\n        'central_value': np.array([1.2]),\n        'standard_deviation': np.array([0.08944272])\n    }\n    '''\n\n    if len(measurement_name) &gt; 1:\n        if len(np.unique(observables)) &gt; 1:\n            raise ValueError(f\"Only distributions constraining the same observable can be combined.\")\n        measurement_name = np.expand_dims(', '.join(np.unique(measurement_name)), axis=0)\n        observables = observables[:1]\n        observable_indices = observable_indices[:1]\n        weights = 1 / standard_deviation**2\n        central_value = np.average(central_value, weights=weights, keepdims=True)\n        standard_deviation = np.sqrt(1 / np.sum(weights, keepdims=True))\n    return {\n        'measurement_name': measurement_name,\n        'observables': observables,\n        'observable_indices': observable_indices,\n        'central_value': central_value,\n        'standard_deviation': standard_deviation,\n    }\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.convert_GeneralGammaDistributionPositive","title":"<code>convert_GeneralGammaDistributionPositive(a, loc, scale, gaussian_standard_deviation)</code>","text":"<p>Convert a <code>GeneralGammaDistributionPositive</code> to either a <code>GammaDistributionPositive</code> or a <code>NumericalDistribution</code>.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>float</code> <p>Shape parameter of the Generalized Gamma distribution.</p> required <code>loc</code> <code>float</code> <p>Location parameter of the Generalized Gamma distribution.</p> required <code>scale</code> <code>float</code> <p>Scale parameter of the Generalized Gamma distribution.</p> required <code>gaussian_standard_deviation</code> <code>float</code> <p>Standard deviation of the Gaussian smearing. If zero, no smearing is applied.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple containing the type of the resulting distribution (<code>'GammaDistributionPositive'</code> or <code>'NumericalDistribution'</code>) and its parameters.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def convert_GeneralGammaDistributionPositive(a, loc, scale, gaussian_standard_deviation):\n    '''\n    Convert a `GeneralGammaDistributionPositive` to either a `GammaDistributionPositive` or a `NumericalDistribution`.\n\n    Parameters\n    ----------\n    a : float\n        Shape parameter of the Generalized Gamma distribution.\n    loc : float\n        Location parameter of the Generalized Gamma distribution.\n    scale : float\n        Scale parameter of the Generalized Gamma distribution.\n    gaussian_standard_deviation : float\n        Standard deviation of the Gaussian smearing. If zero, no smearing is applied.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the type of the resulting distribution (`'GammaDistributionPositive'` or `'NumericalDistribution'`) and its parameters.\n    '''\n    loc_scaled = loc/scale\n    if gaussian_standard_deviation == 0:\n        distribution_type = 'GammaDistributionPositive'\n        parameters = {'a': a, 'loc': loc_scaled, 'scale': 1}\n    else:\n        distribution_type = 'NumericalDistribution'\n        gamma_unscaled = GammaDistribution(a = a, loc = loc_scaled, scale = 1)\n        norm_bg = NormalDistribution(0, gaussian_standard_deviation)\n        numerical = [NumericalDistribution.from_pd(p, nsteps=1000) for p in [gamma_unscaled, norm_bg]]\n        num_unscaled = _convolve_numerical(numerical, central_values='sum')\n        x = np.array(num_unscaled.x)\n        y = np.array(num_unscaled.y_norm)\n        if loc_scaled in x:\n            to_mirror = y[x&lt;=loc_scaled][::-1]\n            y_pos = y[len(to_mirror)-1:len(to_mirror)*2-1]\n            y[len(to_mirror)-1:len(to_mirror)*2-1] += to_mirror[:len(y_pos)]\n        else:\n            to_mirror = y[x&lt;loc_scaled][::-1]\n            y_pos = y[len(to_mirror):len(to_mirror)*2]\n            y[len(to_mirror):len(to_mirror)*2] += to_mirror[:len(y_pos)]\n        y = y[x &gt;= 0]\n        x = x[x &gt;= 0]\n        if x[0] != 0:  #  make sure the PDF at 0 exists\n            x = np.insert(x, 0, 0.)  # add 0 as first element\n            y = np.insert(y, 0, y[0])  # copy first element\n        x = x * scale\n        y = np.maximum(0, y)  # make sure PDF is positive\n        y = y /  np.trapz(y, x=x)  # normalize PDF to 1\n        # ignore warning from log(0)=-np.inf\n        with np.errstate(divide='ignore', invalid='ignore'):\n            log_y = np.log(y)\n        # replace -np.inf with a large negative number\n        log_y[np.isneginf(log_y)] = LOG_ZERO\n        parameters = {\n            'x': x,\n            'y': y,\n            'log_y': log_y,\n        }\n    return distribution_type, parameters\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.cov_coeff_to_cov_obs","title":"<code>cov_coeff_to_cov_obs(par_monomials, cov_th_scaled)</code>","text":"<p>Convert a covariance matrix in the space of parameters to a covariance matrix in the space of observables.</p> <p>Parameters:</p> Name Type Description Default <code>par_monomials</code> <code>List[array]</code> <p>List of parameter monomials for each sector.</p> required <code>cov_th_scaled</code> <code>List[List[array]]</code> <p>Covariance matrix in the space of parameters, scaled by the SM+exp standard deviations.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Covariance matrix in the space of observables.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def cov_coeff_to_cov_obs(par_monomials, cov_th_scaled): # TODO (maybe) optimize\n    '''\n    Convert a covariance matrix in the space of parameters to a covariance matrix in the space of observables.\n\n    Parameters\n    ----------\n    par_monomials : List[jnp.array]\n        List of parameter monomials for each sector.\n    cov_th_scaled : List[List[jnp.array]]\n        Covariance matrix in the space of parameters, scaled by the SM+exp standard deviations.\n\n    Returns\n    -------\n    jnp.array\n        Covariance matrix in the space of observables.\n    '''\n    n_sectors = len(par_monomials)\n\n    cov = np.empty((n_sectors,n_sectors), dtype=object).tolist()\n\n    for i in range(n_sectors):\n        for j in range(n_sectors):\n            if i&gt;= j:\n                cov[i][j] = jnp.einsum('ijkl,k,l-&gt;ij',cov_th_scaled[i][j],par_monomials[i],par_monomials[j])\n            else:\n                shape = cov_th_scaled[j][i].shape\n                cov[i][j] = jnp.zeros((shape[1], shape[0]))\n    cov_matrix_tril = jnp.tril(jnp.block(cov))\n    return cov_matrix_tril + cov_matrix_tril.T - jnp.diag(jnp.diag(cov_matrix_tril))\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.get_distribution_samples","title":"<code>get_distribution_samples(dist_type, dist_info, n_samples, seed=None)</code>","text":"<p>Generate samples from a specified distribution type using the provided distribution information.</p> <p>Parameters:</p> Name Type Description Default <code>dist_type</code> <code>str</code> <p>Type of the distribution (e.g., <code>NumericalDistribution</code>, <code>NormalDistribution</code>, etc.).</p> required <code>dist_info</code> <code>Dict[str, ndarray]</code> <p>Information about the distribution, such as <code>central_value</code>, <code>standard_deviation</code>, etc.</p> required <code>n_samples</code> <code>int</code> <p>Number of samples to generate.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducibility. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[ndarray] or ndarray</code> <p>A list of arrays in case of <code>MultivariateNormalDistribution</code>, where the length of the list is the number of constraints, and each array is of shape <code>(n_observables, n_samples)</code>. For other distributions, returns a single array of shape <code>(n_constraints, n_samples)</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; dist_info = {\n...     'central_value': np.array([0.0, 1.0]),\n...     'standard_deviation': np.array([1.0, 2.0])\n... }\n&gt;&gt;&gt; get_distribution_samples('NormalDistribution', dist_info, n_samples=1000)\narray([[ 0.12345678,  1.23456789, ...\n       [-0.98765432,  2.34567890, ...]])\n</code></pre> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def get_distribution_samples(\n        dist_type: str,\n        dist_info: Dict[str, np.ndarray],\n        n_samples: int,\n        seed: Optional[int] = None,\n) -&gt; Union[List[np.ndarray], np.ndarray]:\n    \"\"\"\n    Generate samples from a specified distribution type using the provided distribution information.\n\n    Parameters\n    ----------\n    dist_type : str\n        Type of the distribution (e.g., `NumericalDistribution`, `NormalDistribution`, etc.).\n    dist_info : Dict[str, np.ndarray]\n        Information about the distribution, such as `central_value`, `standard_deviation`, etc.\n    n_samples : int\n        Number of samples to generate.\n    seed : int, optional\n        Random seed for reproducibility. Default is `None`.\n\n    Returns\n    -------\n    List[np.ndarray] or np.ndarray\n        A list of arrays in case of `MultivariateNormalDistribution`, where the length of the list is the number of constraints,\n        and each array is of shape `(n_observables, n_samples)`.\n        For other distributions, returns a single array of shape `(n_constraints, n_samples)`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; dist_info = {\n    ...     'central_value': np.array([0.0, 1.0]),\n    ...     'standard_deviation': np.array([1.0, 2.0])\n    ... }\n    &gt;&gt;&gt; get_distribution_samples('NormalDistribution', dist_info, n_samples=1000)\n    array([[ 0.12345678,  1.23456789, ...\n           [-0.98765432,  2.34567890, ...]])\n    \"\"\"\n    if seed is not None:\n        np.random.seed(seed)\n    if dist_type == 'GammaDistributionPositive':\n        a = dist_info['a']\n        loc = dist_info['loc']\n        scale = dist_info['scale']\n        n_constraints = len(a)\n        ppf = get_ppf_gamma_distribution_positive(a, loc, scale)\n        return get_inverse_transform_samples(ppf, n_samples, n_constraints)\n    elif dist_type == 'NumericalDistribution':\n        xp = dist_info['x']\n        fp = dist_info['y']\n        ppf = get_ppf_numerical_distribution(xp, fp)\n        n_constraints = len(xp)\n        return get_inverse_transform_samples(ppf, n_samples, n_constraints)\n    elif dist_type == 'NormalDistribution':\n        central_value = dist_info['central_value']\n        standard_deviation = dist_info['standard_deviation']\n        n_constraints = len(central_value)\n        return np.random.normal(central_value, standard_deviation, size=(n_samples, n_constraints)).T\n    elif dist_type == 'MultivariateNormalDistribution':\n        central_value = dist_info['central_value']\n        standard_deviation = dist_info['standard_deviation']\n        inverse_correlation = dist_info['inverse_correlation']\n        samples = []\n        n_constraints = len(central_value)\n        for i in range(n_constraints):\n            correlation = np.linalg.inv(inverse_correlation[i])  # TODO: think about saving the correlation matrix also in the constraint dict\n            samples.append(\n                np.random.multivariate_normal(\n                    central_value[i],\n                    correlation * np.outer(standard_deviation[i], standard_deviation[i]), n_samples).T\n            )\n        return samples\n    elif dist_type == 'HalfNormalDistribution':\n        standard_deviation = dist_info['standard_deviation']\n        n_constraints = len(standard_deviation)\n        return np.abs(np.random.normal(0, standard_deviation, size=(n_samples, n_constraints)).T)\n    else:\n        raise ValueError(f\"Sampling not implemented for distribution type: {dist_type}\")\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.get_distribution_support","title":"<code>get_distribution_support(dist_type, dist_info)</code>","text":"<p>Get the support of one or more distributions based on the distribution parameters.</p> <p>Parameters:</p> Name Type Description Default <code>dist_type</code> <code>str</code> <p>Type of the distribution (e.g., <code>NumericalDistribution</code>, <code>NormalDistribution</code>, etc.).</p> required <code>dist_info</code> <code>Dict[str, ndarray]</code> <p>Information about the distribution, such as <code>central_value</code>, <code>standard_deviation</code>, etc.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple containing the minimum and maximum values of the support of the distributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_distribution_support('NormalDistribution', {'central_value': np.array([0.0, 1.0]), 'standard_deviation': np.array([1.0, 2.0])})\n(array([-6., -11.]), array([6., 13.]))\n</code></pre> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def get_distribution_support(\n        dist_type: str,\n        dist_info: Dict[str, np.ndarray]\n    ) -&gt; Tuple[np.ndarray, np.ndarray]:\n    '''\n    Get the support of one or more distributions based on the distribution parameters.\n\n    Parameters\n    ----------\n    dist_type : str\n        Type of the distribution (e.g., `NumericalDistribution`, `NormalDistribution`, etc.).\n    dist_info : Dict[str, np.ndarray]\n        Information about the distribution, such as `central_value`, `standard_deviation`, etc.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        A tuple containing the minimum and maximum values of the support of the distributions.\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_distribution_support('NormalDistribution', {'central_value': np.array([0.0, 1.0]), 'standard_deviation': np.array([1.0, 2.0])})\n    (array([-6., -11.]), array([6., 13.]))\n\n    '''\n\n    if dist_type == 'NumericalDistribution':\n        xp = dist_info['x']\n        return np.min(xp, axis=1), np.max(xp, axis=1)\n    elif dist_type == 'NormalDistribution':\n        central_value = dist_info['central_value']\n        standard_deviation = dist_info['standard_deviation']\n        return central_value - 6*standard_deviation, central_value + 6*standard_deviation\n    elif dist_type == 'HalfNormalDistribution':\n        standard_deviation = dist_info['standard_deviation']\n        return np.zeros_like(standard_deviation), 6*standard_deviation\n    elif dist_type == 'GammaDistributionPositive':\n        a = dist_info['a']\n        loc = dist_info['loc']\n        scale = dist_info['scale']\n        mode = np.maximum(loc + (a-1)*scale, 0)\n        gamma = sp.stats.gamma(a, loc, scale)\n        support_min = np.maximum(np.minimum(gamma.ppf(1e-9), mode), 0)\n        support_max = gamma.ppf(1-1e-9*(1-gamma.cdf(0)))\n        return support_min, support_max\n    else:\n        raise NotImplementedError(f\"Computing the support not implemented for {dist_type}.\")\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.get_inverse_transform_samples","title":"<code>get_inverse_transform_samples(ppf, n_samples, n_constraints)</code>","text":"<p>Generate samples from a distribution using inverse transform sampling.</p> <p>Parameters:</p> Name Type Description Default <code>ppf</code> <code>Callable</code> <p>The percent-point function (PPF) of the distribution.</p> required <code>n_samples</code> <code>int</code> <p>The number of samples to generate.</p> required <code>n_constraints</code> <code>int</code> <p>The number of constraints.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of samples drawn from the distribution defined by the PPF.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def get_inverse_transform_samples(\n        ppf: Callable,\n        n_samples: int,\n        n_constraints: int\n    ) -&gt; np.ndarray:\n    \"\"\"\n    Generate samples from a distribution using inverse transform sampling.\n\n    Parameters\n    ----------\n    ppf : Callable\n        The percent-point function (PPF) of the distribution.\n    n_samples : int\n        The number of samples to generate.\n    n_constraints : int\n        The number of constraints.\n\n    Returns\n    -------\n    np.ndarray\n        An array of samples drawn from the distribution defined by the PPF.\n    \"\"\"\n    return ppf(np.random.uniform(0, 1, (n_samples, n_constraints))).T\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.get_mode_and_uncertainty","title":"<code>get_mode_and_uncertainty(dist_type, dist_info)</code>","text":"<p>Get the mode and uncertainty of one or more distributions based on the distribution parameters.</p> <p>A Gaussian approximation or an upper limit based on the 95% confidence level is used, depending on the distribution type and parameters.</p> <p>In case of the upper limit, the mode is set to <code>nan</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dist_type</code> <code>str</code> <p>Type of the distribution (e.g., <code>NumericalDistribution</code>, <code>NormalDistribution</code>, etc.).</p> required <code>dist_info</code> <code>Dict[str, ndarray]</code> <p>Information about the distribution, such as <code>central_value</code>, <code>standard_deviation</code>, etc.</p> required <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>A tuple containing the mode and uncertainty of the distributions.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; get_mode_and_uncertainty('NormalDistribution', {'central_value': np.array([0.0, 1.0]), 'standard_deviation': np.array([1.0, 2.0])})\n(array([0., 1.]), array([1., 2.]))\n&gt;&gt;&gt; get_mode_and_uncertainty('HalfNormalDistribution', {'standard_deviation': np.array([0.3, 0.4])})\n(array([nan, nan]), array([0.588, 0.784]))\n&gt;&gt;&gt; get_mode_and_uncertainty('GammaDistributionPositive', {'a': np.array([2.0, 4.0]), 'loc': np.array([-1.0, 0.0]), 'scale': np.array([1.0, 2.0])})\n(array([nan,  6.]), array([4.11300328, 3.46410162]))\n&gt;&gt;&gt; central_value = np.array([[0.0], [6.4]])\n&gt;&gt;&gt; standard_deviation = np.array([[1.0], [1.2]])\n&gt;&gt;&gt; xp = np.broadcast_to(np.linspace(0, 10, 10000), (2, 10000))\n&gt;&gt;&gt; fp = sp.stats.norm.pdf(xp, loc=central_value, scale=standard_deviation)\n&gt;&gt;&gt; get_mode_and_uncertainty('NumericalDistribution', {'x': xp, 'y': fp, 'log_y': np.log(fp)})\n(array([nan, 6.4]), array([1.96, 1.2]))\n</code></pre> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def get_mode_and_uncertainty(\n        dist_type: str,\n        dist_info: Dict[str, np.ndarray],\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    '''\n    Get the mode and uncertainty of one or more distributions based on the distribution parameters.\n\n    A Gaussian approximation or an upper limit based on the 95% confidence level is used, depending on the distribution type and parameters.\n\n    In case of the upper limit, the mode is set to `nan`.\n\n    Parameters\n    ----------\n    dist_type : str\n        Type of the distribution (e.g., `NumericalDistribution`, `NormalDistribution`, etc.).\n    dist_info : Dict[str, np.ndarray]\n        Information about the distribution, such as `central_value`, `standard_deviation`, etc.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        A tuple containing the mode and uncertainty of the distributions.\n\n    Examples\n    --------\n    &gt;&gt;&gt; get_mode_and_uncertainty('NormalDistribution', {'central_value': np.array([0.0, 1.0]), 'standard_deviation': np.array([1.0, 2.0])})\n    (array([0., 1.]), array([1., 2.]))\n    &gt;&gt;&gt; get_mode_and_uncertainty('HalfNormalDistribution', {'standard_deviation': np.array([0.3, 0.4])})\n    (array([nan, nan]), array([0.588, 0.784]))\n    &gt;&gt;&gt; get_mode_and_uncertainty('GammaDistributionPositive', {'a': np.array([2.0, 4.0]), 'loc': np.array([-1.0, 0.0]), 'scale': np.array([1.0, 2.0])})\n    (array([nan,  6.]), array([4.11300328, 3.46410162]))\n    &gt;&gt;&gt; central_value = np.array([[0.0], [6.4]])\n    &gt;&gt;&gt; standard_deviation = np.array([[1.0], [1.2]])\n    &gt;&gt;&gt; xp = np.broadcast_to(np.linspace(0, 10, 10000), (2, 10000))\n    &gt;&gt;&gt; fp = sp.stats.norm.pdf(xp, loc=central_value, scale=standard_deviation)\n    &gt;&gt;&gt; get_mode_and_uncertainty('NumericalDistribution', {'x': xp, 'y': fp, 'log_y': np.log(fp)})\n    (array([nan, 6.4]), array([1.96, 1.2]))\n    '''\n    if dist_type == 'NormalDistribution':\n        mode = dist_info['central_value']\n        uncertainty = dist_info['standard_deviation']\n        return mode, uncertainty\n    elif dist_type == 'HalfNormalDistribution':\n        uncertainty = dist_info['standard_deviation']*1.96  # 95% CL\n        return np.full_like(uncertainty, np.nan), uncertainty\n    elif dist_type == 'GammaDistributionPositive':\n        a = dist_info['a']\n        loc = dist_info['loc']\n        scale = dist_info['scale']\n        mode = np.maximum(loc + (a-1)*scale, 0)\n\n        # if mode is negative, use the 95% CL upper limit, otherwise use the standard deviation at the mode\n        upper_limit = mode &lt;= 0\n        gaussian = ~upper_limit\n        uncertainty = np.empty_like(mode, dtype=float)\n        uncertainty[gaussian] = np.sqrt((loc[gaussian]-mode[gaussian])**2 / (a[gaussian]-1))  # standard deviation at the mode, defined as sqrt(-1/(d^2/dx^2 log(gamma(x, a, loc, scale))))\n        ppf = get_ppf_gamma_distribution_positive(a[upper_limit], loc[upper_limit], scale[upper_limit])\n        uncertainty[upper_limit] = ppf(0.95)  # 95% CL upper limit using the ppf of the gamma distribution restricted to positive values\n        mode[upper_limit] = np.nan  # set the modes to nan where they are not defined\n\n        # check if mode/uncertainty is smaller than 1.7 and mode &gt; 0, in this case compute 95% CL upper limit\n        # 1.7 is selected as threshold where the gaussian and halfnormal approximation are approximately equally good based on the KL divergence\n        upper_limit = (mode/uncertainty &lt; 1.7) &amp; (mode &gt; 0)\n        ppf = get_ppf_gamma_distribution_positive(a[upper_limit], loc[upper_limit], scale[upper_limit])\n        uncertainty[upper_limit] = ppf(0.95)  # 95% CL upper limit using the ppf of the gamma distribution restricted to positive values\n        mode[upper_limit] = np.nan\n        return mode, uncertainty\n    elif dist_type == 'NumericalDistribution':\n        xp = dist_info['x']\n        log_fp = dist_info['log_y']\n        fp = dist_info['y']\n        n_constraints = len(log_fp)\n        mode = np.empty(n_constraints, dtype=float)\n        uncertainty = np.empty(n_constraints, dtype=float)\n        for i in range(n_constraints):\n            log_fp_i = log_fp[i]\n            fp_i = fp[i]\n            xp_i = xp[i]\n            fit_points = log_fp_i &gt; np.max(log_fp_i) - 0.5  # points of logpdf within 0.5 of the maximum\n            a, b, _ = np.polyfit(xp_i[fit_points], log_fp_i[fit_points], 2)  # fit a quadratic polynomial to the logpdf\n            mode_i = -b / (2 * a)\n            uncertainty_i = np.sqrt(-1 / (2 * a))\n            if np.abs(mode_i/uncertainty_i) &gt; 1.7:  # if mode/uncertainty is larger than 1.7, use gaussian approximation\n                mode[i] = mode_i\n                uncertainty[i] = uncertainty_i\n            else:  # compute 95% CL upper limit using ppf of the numerical distribution\n                ppf = get_ppf_numerical_distribution(xp_i, fp_i)\n                mode[i] = np.nan\n                uncertainty[i] = ppf(0.95)\n        return mode, uncertainty\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.get_ppf_gamma_distribution_positive","title":"<code>get_ppf_gamma_distribution_positive(a, loc, scale)</code>","text":"<p>Get the percent-point function (PPF) for a gamma distribution restricted to positive values.</p> <p>Parameters:</p> Name Type Description Default <code>a</code> <code>ndarray</code> <p>Shape parameter of the gamma distribution.</p> required <code>loc</code> <code>ndarray</code> <p>Location parameter of the gamma distribution.</p> required <code>scale</code> <code>ndarray</code> <p>Scale parameter of the gamma distribution.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The PPF that can be used to compute the quantiles for given probabilities.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def get_ppf_gamma_distribution_positive(\n        a: np.ndarray,\n        loc: np.ndarray,\n        scale: np.ndarray,\n) -&gt; Callable:\n    \"\"\"\n    Get the percent-point function (PPF) for a gamma distribution restricted to positive values.\n\n    Parameters\n    ----------\n    a : np.ndarray\n        Shape parameter of the gamma distribution.\n    loc : np.ndarray\n        Location parameter of the gamma distribution.\n    scale : np.ndarray\n        Scale parameter of the gamma distribution.\n\n    Returns\n    -------\n    Callable\n        The PPF that can be used to compute the quantiles for given probabilities.\n    \"\"\"\n    gamma = sp.stats.gamma(a, loc, scale)\n    def ppf(q):\n        return gamma.ppf(q + (1-q)*gamma.cdf(0))\n    return ppf\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.get_ppf_numerical_distribution","title":"<code>get_ppf_numerical_distribution(xp, fp)</code>","text":"<p>Get the percent-point function (PPF) for one or more numerical distributions.</p> <p>Parameters:</p> Name Type Description Default <code>xp</code> <code>ndarray</code> <p>Points at which the PDF is defined.</p> required <code>fp</code> <code>ndarray</code> <p>PDF values at the points <code>xp</code>.</p> required <p>Returns:</p> Type Description <code>Callable</code> <p>The PPF that can be used to compute the quantiles for given probabilities.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def get_ppf_numerical_distribution(\n        xp: np.ndarray,\n        fp: np.ndarray,\n) -&gt; Callable:\n    '''\n    Get the percent-point function (PPF) for one or more numerical distributions.\n\n    Parameters\n    ----------\n    xp : np.ndarray\n        Points at which the PDF is defined.\n    fp : np.ndarray\n        PDF values at the points `xp`.\n\n    Returns\n    -------\n    Callable\n        The PPF that can be used to compute the quantiles for given probabilities.\n    '''\n    if xp.ndim == 1: # single distribution\n        cdf = np.concatenate([[0], np.cumsum((fp[1:] + fp[:-1]) * 0.5 * np.diff(xp))])\n        cdf /= cdf[-1]\n        return partial(np.interp, xp=cdf, fp=xp)\n    elif xp.ndim == 2: # multiple distributions\n        dx = np.diff(xp, axis=1)\n        avg_fp = 0.5 * (fp[:, 1:] + fp[:, :-1])\n        cdf = np.cumsum(avg_fp * dx, axis=1)\n        cdf = np.concatenate([np.zeros((cdf.shape[0], 1)), cdf], axis=1)\n        cdf /= cdf[:, [-1]]\n\n        def batched_ppf(q: Union[float, np.ndarray]) -&gt; np.ndarray:\n            \"\"\"\n            Batched PPF for multiple distributions.\n\n            Parameters\n            ----------\n            q : float or np.ndarray\n                Lower-tail probabilities at which to compute the PPF.\n\n                  - If scalar, computes PPF for that probability across all distributions.\n\n                  - If 1D array of shape (k,), computes PPF at k probabilities for all distributions.\n\n                  - If 2D array of shape (k, m), computes PPF at k probabilities for each of the m distributions.\n\n            Returns\n            -------\n            np.ndarray\n                The quantiles corresponding to the input probabilities.\n\n                  - If input is scalar, returns 1D array of shape (m,)\n\n                  - Otherwise returns array of shape (k, m)\n            \"\"\"\n\n            q = np.asarray(q)\n            scalar_input = False\n            if q.ndim == 0:  # single probability for all distributions\n                q = np.full((1, cdf.shape[0]), q)\n                scalar_input = True\n            elif q.ndim == 1:  # a vector of probabilities for all distributions\n                q = np.tile(q[None, :], (1, cdf.shape[0]))\n            result = np.empty_like(q)\n            for i in range(q.shape[1]):  # iterate over distributions\n                result[:, i] = np.interp(q[:, i], cdf[i], xp[i])\n            return result[0] if scalar_input else result\n        return batched_ppf\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logL_correlated_sectors","title":"<code>logL_correlated_sectors(predictions_scaled, observable_indices, exp_central_scaled, cov_matrix_exp_scaled, cov_matrix_th_scaled)</code>","text":"<p>Compute the log likelihood values for observables with correlated theoretical and experimental uncertainties.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_scaled</code> <code>array</code> <p>The predicted values, scaled by the SM+exp standard deviations.</p> required <code>observable_indices</code> <code>List[array]</code> <p>The indices of the constrained observables.</p> required <code>exp_central_scaled</code> <code>array</code> <p>The experimental central values, scaled by the SM+exp standard deviations.</p> required <code>cov_matrix_exp_scaled</code> <code>array</code> <p>The experimental covariance matrix, scaled by the SM+exp standard deviations.</p> required <code>cov_matrix_th_scaled</code> <code>array</code> <p>The theoretical covariance matrix in the space of parameters, scaled by the SM+exp standard deviations.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The log likelihood values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logL_correlated_sectors(\n    predictions_scaled: jnp.array,\n    observable_indices: List[jnp.array],\n    exp_central_scaled: jnp.array,\n    cov_matrix_exp_scaled: jnp.array,\n    cov_matrix_th_scaled: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log likelihood values for observables with correlated theoretical and experimental uncertainties.\n\n    Parameters\n    ----------\n    predictions_scaled : jnp.array\n        The predicted values, scaled by the SM+exp standard deviations.\n    observable_indices : List[jnp.array]\n        The indices of the constrained observables.\n    exp_central_scaled : jnp.array\n        The experimental central values, scaled by the SM+exp standard deviations.\n    cov_matrix_exp_scaled : jnp.array\n        The experimental covariance matrix, scaled by the SM+exp standard deviations.\n    cov_matrix_th_scaled : jnp.array\n        The theoretical covariance matrix in the space of parameters, scaled by the SM+exp standard deviations.\n\n    Returns\n    -------\n    jnp.array\n        The log likelihood values.\n    '''\n\n    cov_scaled = cov_matrix_th_scaled + cov_matrix_exp_scaled\n    std_scaled = jnp.sqrt(jnp.diag(cov_scaled))\n    C = cov_scaled / jnp.outer(std_scaled, std_scaled)\n    D = (predictions_scaled - exp_central_scaled)/std_scaled\n\n    logL_rows = []\n    for i in range(len(observable_indices)):\n        logL_total = jnp.zeros_like(predictions_scaled)\n        d = jnp.take(D, observable_indices[i])\n        c = jnp.take(jnp.take(C, observable_indices[i], axis=0), observable_indices[i], axis=1)\n        logL = -0.5 * d * jsp.linalg.cho_solve(jsp.linalg.cho_factor(c), d)\n        logL_total = logL_total.at[observable_indices[i]].add(logL)\n        logL_rows.append(logL_total)\n    return jnp.array(logL_rows)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logL_correlated_sectors_summed","title":"<code>logL_correlated_sectors_summed(predictions_scaled, selector_matrix, observable_indices, exp_central_scaled, cov_matrix_exp_scaled, cov_matrix_th_scaled)</code>","text":"<p>Compute the summed log likelihood values for observables with correlated theoretical and experimental uncertainties.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_scaled</code> <code>array</code> <p>The predicted values, scaled by the SM+exp standard deviations.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to sum the log likelihood values of different unique multivariate normal distributions. Of shape (n_likelihoods, n_distributions).</p> required <code>observable_indices</code> <code>List[array]</code> <p>The indices of the constrained observables.</p> required <code>exp_central_scaled</code> <code>array</code> <p>The experimental central values, scaled by the SM+exp standard deviations.</p> required <code>cov_matrix_exp_scaled</code> <code>array</code> <p>The experimental covariance matrix, scaled by the SM+exp standard deviations.</p> required <code>cov_matrix_th_scaled</code> <code>array</code> <p>The theoretical covariance matrix in the space of parameters, scaled by the SM+exp standard deviations</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log likelihood values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logL_correlated_sectors_summed(\n    predictions_scaled: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: List[jnp.array],\n    exp_central_scaled: jnp.array,\n    cov_matrix_exp_scaled: jnp.array,\n    cov_matrix_th_scaled: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the summed log likelihood values for observables with correlated theoretical and experimental uncertainties.\n\n    Parameters\n    ----------\n    predictions_scaled : jnp.array\n        The predicted values, scaled by the SM+exp standard deviations.\n    selector_matrix : jnp.array\n        The selector matrix to sum the log likelihood values of different unique multivariate normal distributions. Of shape (n_likelihoods, n_distributions).\n    observable_indices : List[jnp.array]\n        The indices of the constrained observables.\n    exp_central_scaled : jnp.array\n        The experimental central values, scaled by the SM+exp standard deviations.\n    cov_matrix_exp_scaled : jnp.array\n        The experimental covariance matrix, scaled by the SM+exp standard deviations.\n    cov_matrix_th_scaled : jnp.array\n        The theoretical covariance matrix in the space of parameters, scaled by the SM+exp standard deviations\n\n    Returns\n    -------\n    jnp.array\n        The summed log likelihood values.\n    '''\n    cov_scaled = cov_matrix_th_scaled + cov_matrix_exp_scaled\n    std_scaled = jnp.sqrt(jnp.diag(cov_scaled))\n    C = cov_scaled / jnp.outer(std_scaled, std_scaled)\n    D = (predictions_scaled - exp_central_scaled)/std_scaled\n\n    logL_rows = []\n    for i in range(len(observable_indices)):\n        logL_total = jnp.zeros_like(predictions_scaled)\n        d = jnp.take(D, observable_indices[i])\n        c = jnp.take(jnp.take(C, observable_indices[i], axis=0), observable_indices[i], axis=1)\n        logL = -0.5 * jnp.dot(d, jsp.linalg.cho_solve(jsp.linalg.cho_factor(c), d))\n        logL_rows.append(logL)\n    logL_total = jnp.array(logL_rows)\n    return selector_matrix @ logL_total\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logL_gamma_distribution_positive","title":"<code>logL_gamma_distribution_positive(predictions, observable_indices, a, loc, scale)</code>","text":"<p>Compute the log likelihood values of positive gamma distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>a</code> <code>array</code> <p>The shape parameters for the gamma distributions.</p> required <code>loc</code> <code>array</code> <p>The location parameters for the gamma distributions.</p> required <code>scale</code> <code>array</code> <p>The scale parameters for the gamma distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The log likelihood values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logL_gamma_distribution_positive(\n    predictions: jnp.array,\n    observable_indices: jnp.array,\n    a: jnp.array,\n    loc: jnp.array,\n    scale: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log likelihood values of positive gamma distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    a : jnp.array\n        The shape parameters for the gamma distributions.\n    loc : jnp.array\n        The location parameters for the gamma distributions.\n    scale : jnp.array\n        The scale parameters for the gamma distributions.\n\n    Returns\n    -------\n    jnp.array\n        The log likelihood values.\n    '''\n    logL_total = jnp.zeros_like(predictions)\n    predictions = jnp.take(predictions, observable_indices)\n    mode = jnp.maximum(loc + (a-1)*scale, 0)\n    logL_pred = (a-1)*jnp.log((predictions-loc)/scale) - (predictions-loc)/scale\n    logL_mode = (a-1)*jnp.log((mode-loc)/scale) - (mode-loc)/scale\n    logL = jnp.where(predictions&gt;=0, logL_pred-logL_mode, LOG_ZERO)\n    logL_total = logL_total.at[observable_indices].add(logL)\n    return logL_total\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logL_gamma_distribution_positive_summed","title":"<code>logL_gamma_distribution_positive_summed(predictions, selector_matrix, observable_indices, a, loc, scale)</code>","text":"<p>Compute the log likelihood values of positive gamma distributions for given predictions and sum them using a selector matrix.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to apply to the log likelihood values. Of shape (n_likelihoods, n_observables).</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>a</code> <code>array</code> <p>The shape parameters for the gamma distributions.</p> required <code>loc</code> <code>array</code> <p>The location parameters for the gamma distributions.</p> required <code>scale</code> <code>array</code> <p>The scale parameters for the gamma distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log likelihood values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logL_gamma_distribution_positive_summed(\n    predictions: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: jnp.array,\n    a: jnp.array,\n    loc: jnp.array,\n    scale: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log likelihood values of positive gamma distributions for given predictions and sum them using a selector matrix.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    selector_matrix : jnp.array\n        The selector matrix to apply to the log likelihood values. Of shape (n_likelihoods, n_observables).\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    a : jnp.array\n        The shape parameters for the gamma distributions.\n    loc : jnp.array\n        The location parameters for the gamma distributions.\n    scale : jnp.array\n        The scale parameters for the gamma distributions.\n\n    Returns\n    -------\n    jnp.array\n        The summed log likelihood values.\n    '''\n    return selector_matrix @ logL_gamma_distribution_positive(predictions, observable_indices, a, loc, scale)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logL_half_normal_distribution","title":"<code>logL_half_normal_distribution(predictions, observable_indices, std)</code>","text":"<p>Compute the log likelihood values of half normal distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>std</code> <code>array</code> <p>The standard deviation values for the half normal distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The log likelihood values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logL_half_normal_distribution(\n    predictions: jnp.array,\n    observable_indices: jnp.array,\n    std: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log likelihood values of half normal distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    std : jnp.array\n        The standard deviation values for the half normal distributions.\n\n    Returns\n    -------\n    jnp.array\n        The log likelihood values.\n    '''\n    logL_total = jnp.zeros_like(predictions)\n    predictions = jnp.take(predictions, observable_indices)\n    logL = -0.5 * (predictions/std)**2\n    logL = jnp.where(predictions&gt;=0, logL, LOG_ZERO)\n    logL_total = logL_total.at[observable_indices].add(logL)\n    return logL_total\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logL_half_normal_distribution_summed","title":"<code>logL_half_normal_distribution_summed(predictions, selector_matrix, observable_indices, std)</code>","text":"<p>Compute the log likelihood values of half normal distributions for given predictions and sum them using a selector matrix.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to apply to the log likelihood values. Of shape (n_likelihoods, n_observables).</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>std</code> <code>array</code> <p>The standard deviation values for the half normal distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log likelihood values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logL_half_normal_distribution_summed(\n    predictions: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: jnp.array,\n    std: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log likelihood values of half normal distributions for given predictions and sum them using a selector matrix.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    selector_matrix : jnp.array\n        The selector matrix to apply to the log likelihood values. Of shape (n_likelihoods, n_observables).\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    std : jnp.array\n        The standard deviation values for the half normal distributions.\n\n    Returns\n    -------\n    jnp.array\n        The summed log likelihood values.\n    '''\n    return selector_matrix @ logL_half_normal_distribution(predictions, observable_indices, std)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logL_multivariate_normal_distribution","title":"<code>logL_multivariate_normal_distribution(predictions, observable_indices, mean, standard_deviation, inverse_correlation)</code>","text":"<p>Compute the log likelihood values of multivariate normal distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>observable_indices</code> <code>List[array]</code> <p>The indices of the constrained observables.</p> required <code>mean</code> <code>List[array]</code> <p>The mean values of the multivariate normal distributions.</p> required <code>standard_deviation</code> <code>List[array]</code> <p>The standard deviations of the multivariate normal distributions.</p> required <code>inverse_correlation</code> <code>List[array]</code> <p>The inverse correlation matrices of the multivariate normal distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The log likelihood values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logL_multivariate_normal_distribution(\n    predictions: jnp.array,\n    observable_indices: List[jnp.array],\n    mean: List[jnp.array],\n    standard_deviation: List[jnp.array],\n    inverse_correlation: List[jnp.array],\n) -&gt; jnp.array:\n    '''\n    Compute the log likelihood values of multivariate normal distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    observable_indices : List[jnp.array]\n        The indices of the constrained observables.\n    mean : List[jnp.array]\n        The mean values of the multivariate normal distributions.\n    standard_deviation : List[jnp.array]\n        The standard deviations of the multivariate normal distributions.\n    inverse_correlation : List[jnp.array]\n        The inverse correlation matrices of the multivariate normal distributions.\n\n    Returns\n    -------\n    jnp.array\n        The log likelihood values.\n    '''\n    logLs = []\n    for i in range(len(observable_indices)):\n        logL_total = jnp.zeros_like(predictions)\n        d = (jnp.take(predictions, observable_indices[i]) - mean[i]) / standard_deviation[i]\n        logL = -0.5 * d * jnp.dot(inverse_correlation[i], d)\n        logL_total = logL_total.at[observable_indices[i]].add(logL)\n        logLs.append(logL_total)\n    return jnp.stack(logLs)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logL_multivariate_normal_distribution_summed","title":"<code>logL_multivariate_normal_distribution_summed(predictions, selector_matrix, observable_indices, mean, standard_deviation, inverse_correlation)</code>","text":"<p>Compute the summed log likelihood values of multivariate normal distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to sum the log likelihood values of different multivariate normal distributions. Of shape (n_likelihoods, n_distributions).</p> required <code>observable_indices</code> <code>List[array]</code> <p>The indices of the constrained observables.</p> required <code>mean</code> <code>List[array]</code> <p>The mean values of the multivariate normal distributions.</p> required <code>standard_deviation</code> <code>List[array]</code> <p>The standard deviations of the multivariate normal distributions.</p> required <code>inverse_correlation</code> <code>List[array]</code> <p>The inverse correlation matrices of the multivariate normal distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log likelihood values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logL_multivariate_normal_distribution_summed(\n    predictions: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: List[jnp.array],\n    mean: List[jnp.array],\n    standard_deviation: List[jnp.array],\n    inverse_correlation: List[jnp.array],\n) -&gt; jnp.array:\n    '''\n    Compute the summed log likelihood values of multivariate normal distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    selector_matrix : jnp.array\n        The selector matrix to sum the log likelihood values of different multivariate normal distributions. Of shape (n_likelihoods, n_distributions).\n    observable_indices : List[jnp.array]\n        The indices of the constrained observables.\n    mean : List[jnp.array]\n        The mean values of the multivariate normal distributions.\n    standard_deviation : List[jnp.array]\n        The standard deviations of the multivariate normal distributions.\n    inverse_correlation : List[jnp.array]\n        The inverse correlation matrices of the multivariate normal distributions.\n\n    Returns\n    -------\n    jnp.array\n        The summed log likelihood values.\n    '''\n    logL_rows = []\n    for i in range(len(observable_indices)):\n        d = (jnp.take(predictions, observable_indices[i]) - mean[i]) / standard_deviation[i]\n        logL = -0.5 * jnp.dot(d, jnp.dot(inverse_correlation[i], d))\n        logL_rows.append(logL)\n    logL_total = jnp.stack(logL_rows)\n    return selector_matrix @ logL_total\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logL_normal_distribution","title":"<code>logL_normal_distribution(predictions, observable_indices, mean, std)</code>","text":"<p>Compute the log likelihood values of normal distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>mean</code> <code>array</code> <p>The mean values for the normal distributions.</p> required <code>std</code> <code>array</code> <p>The standard deviation values for the normal distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The log likelihood values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logL_normal_distribution(\n    predictions: jnp.array,\n    observable_indices: jnp.array,\n    mean: jnp.array,\n    std: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log likelihood values of normal distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    mean : jnp.array\n        The mean values for the normal distributions.\n    std : jnp.array\n        The standard deviation values for the normal distributions.\n\n    Returns\n    -------\n    jnp.array\n        The log likelihood values.\n    '''\n    logL_total = jnp.zeros_like(predictions)\n    predictions = jnp.take(predictions, observable_indices)\n    logL = -0.5 * ((predictions-mean)/std)**2\n    logL_total = logL_total.at[observable_indices].add(logL)\n    return logL_total\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logL_normal_distribution_summed","title":"<code>logL_normal_distribution_summed(predictions, selector_matrix, observable_indices, mean, std)</code>","text":"<p>Compute the log likelihood values of normal distributions for given predictions and sum them using a selector matrix.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to apply to the log likelihood values. Of shape (n_likelihoods, n_observables).</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>mean</code> <code>array</code> <p>The mean values for the normal distributions.</p> required <code>std</code> <code>array</code> <p>The standard deviation values for the normal distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log likelihood values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logL_normal_distribution_summed(\n    predictions: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: jnp.array,\n    mean: jnp.array,\n    std: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log likelihood values of normal distributions for given predictions and sum them using a selector matrix.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    selector_matrix : jnp.array\n        The selector matrix to apply to the log likelihood values. Of shape (n_likelihoods, n_observables).\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    mean : jnp.array\n        The mean values for the normal distributions.\n    std : jnp.array\n        The standard deviation values for the normal distributions.\n\n    Returns\n    -------\n    jnp.array\n        The summed log likelihood values.\n    '''\n    return selector_matrix @ logL_normal_distribution(predictions, observable_indices, mean, std)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logL_numerical_distribution","title":"<code>logL_numerical_distribution(predictions, observable_indices, x, log_y)</code>","text":"<p>Compute the log likelihood values of numerical distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>x</code> <code>array</code> <p>The x values for the numerical distributions.</p> required <code>log_y</code> <code>array</code> <p>The log y values for the numerical distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The log likelihood values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logL_numerical_distribution(\n    predictions: jnp.array,\n    observable_indices: jnp.array,\n    x: jnp.array,\n    log_y: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log likelihood values of numerical distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    x : jnp.array\n        The x values for the numerical distributions.\n    log_y : jnp.array\n        The log y values for the numerical distributions.\n\n    Returns\n    -------\n    jnp.array\n        The log likelihood values.\n    '''\n    logL_total = jnp.zeros_like(predictions)\n    predictions = jnp.take(predictions, observable_indices)\n    logL = vmap(interp_log_pdf)(predictions, x, log_y - jnp.max(log_y, axis=1, keepdims=True))\n    logL_total = logL_total.at[observable_indices].add(logL)\n    return logL_total\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logL_numerical_distribution_summed","title":"<code>logL_numerical_distribution_summed(predictions, selector_matrix, observable_indices, x, log_y)</code>","text":"<p>Compute the log likelihood values of numerical distributions for given predictions and sum them using a selector matrix.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to apply to the log likelihood values. Of shape (n_likelihoods, n_observables).</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>x</code> <code>array</code> <p>The x values for the numerical distributions.</p> required <code>log_y</code> <code>array</code> <p>The log y values for the numerical distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log likelihood values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logL_numerical_distribution_summed(\n    predictions: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: jnp.array,\n    x: jnp.array,\n    log_y: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log likelihood values of numerical distributions for given predictions and sum them using a selector matrix.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    selector_matrix : jnp.array\n        The selector matrix to apply to the log likelihood values. Of shape (n_likelihoods, n_observables).\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    x : jnp.array\n        The x values for the numerical distributions.\n    log_y : jnp.array\n        The log y values for the numerical distributions.\n\n    Returns\n    -------\n    jnp.array\n        The summed log likelihood values.\n    '''\n    return selector_matrix @ logL_numerical_distribution(predictions, observable_indices, x, log_y)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.log_trapz_exp","title":"<code>log_trapz_exp(log_y, x)</code>","text":"<p>Compute the log of the trapezoidal integral of the exponential of <code>log_y</code> over <code>x</code>.</p> <p>Parameters:</p> Name Type Description Default <code>log_y</code> <code>ndarray</code> <p>Logarithm of the values to be integrated.</p> required <code>x</code> <code>ndarray</code> <p>Points at which <code>log_y</code> is defined. It is assumed that <code>x</code> is uniformly spaced.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The logarithm of the trapezoidal integral of <code>exp(log_y)</code> over <code>x</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; log_y = np.array([0.1, 0.2, 0.3])\n&gt;&gt;&gt; x = np.array([1.0, 2.0, 3.0])\n&gt;&gt;&gt; log_trapz_exp(log_y, x)\n0.8956461395871966\n</code></pre> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def log_trapz_exp(\n        log_y: np.ndarray,\n        x: np.ndarray,\n    ) -&gt; np.float64:\n    '''\n    Compute the log of the trapezoidal integral of the exponential of `log_y` over `x`.\n\n    Parameters\n    ----------\n    log_y : np.ndarray\n        Logarithm of the values to be integrated.\n    x : np.ndarray\n        Points at which `log_y` is defined. It is assumed that `x` is uniformly spaced.\n\n    Returns\n    -------\n    float\n        The logarithm of the trapezoidal integral of `exp(log_y)` over `x`.\n\n    Examples\n    --------\n    &gt;&gt;&gt; log_y = np.array([0.1, 0.2, 0.3])\n    &gt;&gt;&gt; x = np.array([1.0, 2.0, 3.0])\n    &gt;&gt;&gt; log_trapz_exp(log_y, x)\n    0.8956461395871966\n    '''\n    log_dx = np.log(x[1] - x[0])  # assume uniform spacing\n    log_weights = np.zeros(len(x))\n    log_weights[[0,-1]] = np.log(0.5)\n    return log_dx + sp.special.logsumexp(log_y + log_weights)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_correlated_sectors","title":"<code>logpdf_correlated_sectors(predictions_scaled, std_sm_exp, observable_indices, exp_central_scaled, cov_matrix_exp_scaled, cov_matrix_th_scaled)</code>","text":"<p>Compute the log PDF values for observables with correlated theoretical and experimental uncertainties.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_scaled</code> <code>array</code> <p>The predicted values, scaled by the SM+exp standard deviations.</p> required <code>std_sm_exp</code> <code>array</code> <p>The SM+exp standard deviations.</p> required <code>observable_indices</code> <code>List[array]</code> <p>The indices of the constrained observables.</p> required <code>exp_central_scaled</code> <code>array</code> <p>The experimental central values, scaled by the SM+exp standard deviations.</p> required <code>cov_matrix_exp_scaled</code> <code>array</code> <p>The experimental covariance matrix, scaled by the SM+exp standard deviations.</p> required <code>cov_matrix_th_scaled</code> <code>array</code> <p>The theoretical covariance matrix in the space of parameters, scaled by the SM+exp standard deviations.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The log PDF values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_correlated_sectors(\n    predictions_scaled: jnp.array,\n    std_sm_exp: jnp.array,\n    observable_indices: List[jnp.array],\n    exp_central_scaled: jnp.array,\n    cov_matrix_exp_scaled: jnp.array,\n    cov_matrix_th_scaled: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log PDF values for observables with correlated theoretical and experimental uncertainties.\n\n    Parameters\n    ----------\n    predictions_scaled : jnp.array\n        The predicted values, scaled by the SM+exp standard deviations.\n    std_sm_exp : jnp.array\n        The SM+exp standard deviations.\n    observable_indices : List[jnp.array]\n        The indices of the constrained observables.\n    exp_central_scaled : jnp.array\n        The experimental central values, scaled by the SM+exp standard deviations.\n    cov_matrix_exp_scaled : jnp.array\n        The experimental covariance matrix, scaled by the SM+exp standard deviations.\n    cov_matrix_th_scaled : jnp.array\n        The theoretical covariance matrix in the space of parameters, scaled by the SM+exp standard deviations.\n\n    Returns\n    -------\n    jnp.array\n        The log PDF values.\n    '''\n    cov_scaled = cov_matrix_th_scaled + cov_matrix_exp_scaled\n    std_scaled = jnp.sqrt(jnp.diag(cov_scaled))\n    std = std_scaled  * std_sm_exp\n    C = cov_scaled / jnp.outer(std_scaled, std_scaled)\n    D = (predictions_scaled - exp_central_scaled)/std_scaled\n\n    logpdf_rows = []\n    for i in range(len(observable_indices)):\n        logpdf_total = jnp.zeros_like(predictions_scaled)\n        d = jnp.take(D, observable_indices[i])\n        c = jnp.take(jnp.take(C, observable_indices[i], axis=0), observable_indices[i], axis=1)\n\n        logdet_corr = jnp.linalg.slogdet(c)[1]\n        logprod_std2 = 2 * jnp.sum(jnp.log(jnp.take(std, observable_indices[i])))\n\n        logpdf = -0.5 * (\n            d * jsp.linalg.cho_solve(jsp.linalg.cho_factor(c), d)\n            + (logdet_corr\n            + logprod_std2)/len(d)\n            + jnp.log(2 * jnp.pi)\n        )\n        logpdf_total = logpdf_total.at[observable_indices[i]].add(logpdf)\n        logpdf_rows.append(logpdf_total)\n    return jnp.array(logpdf_rows)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_correlated_sectors_summed","title":"<code>logpdf_correlated_sectors_summed(predictions_scaled, std_sm_exp, selector_matrix, observable_indices, exp_central_scaled, cov_matrix_exp_scaled, cov_matrix_th_scaled)</code>","text":"<p>Compute the summed log PDF values for observables with correlated theoretical and experimental uncertainties.</p> <p>Parameters:</p> Name Type Description Default <code>predictions_scaled</code> <code>array</code> <p>The predicted values, scaled by the SM+exp standard deviations.</p> required <code>std_sm_exp</code> <code>array</code> <p>The SM+exp standard deviations.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to sum the log PDF values of different unique multivariate normal distributions. Of shape (n_likelihoods, n_distributions).</p> required <code>observable_indices</code> <code>List[array]</code> <p>The indices of the constrained observables.</p> required <code>exp_central_scaled</code> <code>array</code> <p>The experimental central values, scaled by the SM+exp standard deviations.</p> required <code>cov_matrix_exp_scaled</code> <code>array</code> <p>The experimental covariance matrix, scaled by the SM+exp standard deviations.</p> required <code>cov_matrix_th_scaled</code> <code>array</code> <p>The theoretical covariance matrix in the space of parameters, scaled by the SM+exp standard deviations.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log PDF values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_correlated_sectors_summed(\n    predictions_scaled: jnp.array,\n    std_sm_exp: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: List[jnp.array],\n    exp_central_scaled: jnp.array,\n    cov_matrix_exp_scaled: jnp.array,\n    cov_matrix_th_scaled: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the summed log PDF values for observables with correlated theoretical and experimental uncertainties.\n\n    Parameters\n    ----------\n    predictions_scaled : jnp.array\n        The predicted values, scaled by the SM+exp standard deviations.\n    std_sm_exp : jnp.array\n        The SM+exp standard deviations.\n    selector_matrix : jnp.array\n        The selector matrix to sum the log PDF values of different unique multivariate normal distributions. Of shape (n_likelihoods, n_distributions).\n    observable_indices : List[jnp.array]\n        The indices of the constrained observables.\n    exp_central_scaled : jnp.array\n        The experimental central values, scaled by the SM+exp standard deviations.\n    cov_matrix_exp_scaled : jnp.array\n        The experimental covariance matrix, scaled by the SM+exp standard deviations.\n    cov_matrix_th_scaled : jnp.array\n        The theoretical covariance matrix in the space of parameters, scaled by the SM+exp standard deviations.\n\n    Returns\n    -------\n    jnp.array\n        The summed log PDF values.\n    '''\n\n    cov_scaled = cov_matrix_th_scaled + cov_matrix_exp_scaled\n    std_scaled = jnp.sqrt(jnp.diag(cov_scaled))\n    std = std_scaled  * std_sm_exp\n    C = cov_scaled / jnp.outer(std_scaled, std_scaled)\n    D = (predictions_scaled - exp_central_scaled)/std_scaled\n\n    logpdf_rows = []\n    for i in range(len(observable_indices)):\n\n        d = jnp.take(D, observable_indices[i])\n        c = jnp.take(jnp.take(C, observable_indices[i], axis=0), observable_indices[i], axis=1)\n\n        logdet_corr = jnp.linalg.slogdet(c)[1]\n        logprod_std2 = 2 * jnp.sum(jnp.log(jnp.take(std, observable_indices[i])))\n\n        logpdf = -0.5 * (\n            jnp.dot(d, jsp.linalg.cho_solve(jsp.linalg.cho_factor(c), d))\n            + logdet_corr\n            + logprod_std2\n            + len(d) * jnp.log(2 * jnp.pi)\n        )\n        logpdf = jnp.where(jnp.isnan(logpdf), len(d)*LOG_ZERO, logpdf)\n        logpdf_rows.append(logpdf)\n    logpdf_total = jnp.array(logpdf_rows)\n    return selector_matrix @ logpdf_total\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_folded_normal_distribution","title":"<code>logpdf_folded_normal_distribution(predictions, observable_indices, mean, std)</code>","text":"<p>Compute the log PDF values of folded normal distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>mean</code> <code>array</code> <p>The means of the folded normal distributions.</p> required <code>std</code> <code>array</code> <p>The standard deviations of the folded normal distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The log PDF values for the predictions.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_folded_normal_distribution(\n    predictions: jnp.array,\n    observable_indices: jnp.array,\n    mean: jnp.array,\n    std: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log PDF values of folded normal distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    mean : jnp.array\n        The means of the folded normal distributions.\n    std : jnp.array\n        The standard deviations of the folded normal distributions.\n\n    Returns\n    -------\n    jnp.array\n        The log PDF values for the predictions.\n    '''\n    logpdf_total = jnp.zeros_like(predictions)\n    predictions = jnp.take(predictions, observable_indices)\n    folded_logpdf = jnp.log(\n        jsp.stats.norm.pdf(predictions, loc=mean, scale=std)\n        + jsp.stats.norm.pdf(predictions, loc=-mean, scale=std)\n    )\n    logpdf = jnp.where(predictions &gt;= 0, folded_logpdf, LOG_ZERO)\n    logpdf_total = logpdf_total.at[observable_indices].add(logpdf)\n    return logpdf_total\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_folded_normal_distribution_summed","title":"<code>logpdf_folded_normal_distribution_summed(predictions, selector_matrix, observable_indices, mean, std)</code>","text":"<p>Compute the log PDF values of folded normal distributions for given predictions and sum them using a selector matrix.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to sum the log PDF values. Of shape (n_likelihoods, n_observables).</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>mean</code> <code>array</code> <p>The means of the folded normal distributions.</p> required <code>std</code> <code>array</code> <p>The standard deviations of the folded normal distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log PDF values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_folded_normal_distribution_summed(\n    predictions: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: jnp.array,\n    mean: jnp.array,\n    std: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log PDF values of folded normal distributions for given predictions and sum them using a selector matrix.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    selector_matrix : jnp.array\n        The selector matrix to sum the log PDF values. Of shape (n_likelihoods, n_observables).\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    mean : jnp.array\n        The means of the folded normal distributions.\n    std : jnp.array\n        The standard deviations of the folded normal distributions.\n\n    Returns\n    -------\n    jnp.array\n        The summed log PDF values.\n    '''\n    return selector_matrix @ logpdf_folded_normal_distribution(predictions, observable_indices, mean, std)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_gamma_distribution_positive","title":"<code>logpdf_gamma_distribution_positive(predictions, observable_indices, a, loc, scale)</code>","text":"<p>Compute the log PDF values of positive gamma distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>a</code> <code>array</code> <p>The shape parameters of the gamma distributions.</p> required <code>loc</code> <code>array</code> <p>The location parameters of the gamma distributions.</p> required <code>scale</code> <code>array</code> <p>The scale parameters of the gamma distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The log PDF values for the predictions.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_gamma_distribution_positive(\n    predictions: jnp.array,\n    observable_indices: jnp.array,\n    a: jnp.array,\n    loc: jnp.array,\n    scale: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log PDF values of positive gamma distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    a : jnp.array\n        The shape parameters of the gamma distributions.\n    loc : jnp.array\n        The location parameters of the gamma distributions.\n    scale : jnp.array\n        The scale parameters of the gamma distributions.\n\n    Returns\n    -------\n    jnp.array\n        The log PDF values for the predictions.\n    '''\n    logpdf_total = jnp.zeros_like(predictions)\n    predictions = jnp.take(predictions, observable_indices)\n    log_pdf_scale = jnp.log(1/(1-jsp.stats.gamma.cdf(0, a, loc=loc, scale=scale)))\n    positive_logpdf = jsp.stats.gamma.logpdf(\n        predictions, a, loc=loc, scale=scale\n    ) + log_pdf_scale\n    logpdf = jnp.where(predictions&gt;=0, positive_logpdf, LOG_ZERO)\n    logpdf_total = logpdf_total.at[observable_indices].add(logpdf)\n    return logpdf_total\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_gamma_distribution_positive_summed","title":"<code>logpdf_gamma_distribution_positive_summed(predictions, selector_matrix, observable_indices, a, loc, scale)</code>","text":"<p>Compute the log PDF values of positive gamma distributions for given predictions and sum them using a selector matrix.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to sum the log PDF values. Of shape (n_likelihoods, n_observables).</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>a</code> <code>array</code> <p>The shape parameters of the gamma distributions.</p> required <code>loc</code> <code>array</code> <p>The location parameters of the gamma distributions.</p> required <code>scale</code> <code>array</code> <p>The scale parameters of the gamma distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log PDF values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_gamma_distribution_positive_summed(\n    predictions: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: jnp.array,\n    a: jnp.array,\n    loc: jnp.array,\n    scale: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log PDF values of positive gamma distributions for given predictions and sum them using a selector matrix.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    selector_matrix : jnp.array\n        The selector matrix to sum the log PDF values. Of shape (n_likelihoods, n_observables).\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    a : jnp.array\n        The shape parameters of the gamma distributions.\n    loc : jnp.array\n        The location parameters of the gamma distributions.\n    scale : jnp.array\n        The scale parameters of the gamma distributions.\n\n    Returns\n    -------\n    jnp.array\n        The summed log PDF values.\n    '''\n    return selector_matrix @ logpdf_gamma_distribution_positive(predictions, observable_indices, a, loc, scale)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_half_normal_distribution","title":"<code>logpdf_half_normal_distribution(predictions, observable_indices, std)</code>","text":"<p>Compute the log PDF values of half normal distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>std</code> <code>array</code> <p>The standard deviations of the half normal distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The log PDF values for the predictions.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_half_normal_distribution(\n    predictions: jnp.array,\n    observable_indices: jnp.array,\n    std: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log PDF values of half normal distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    std : jnp.array\n        The standard deviations of the half normal distributions.\n\n    Returns\n    -------\n    jnp.array\n        The log PDF values for the predictions.\n    '''\n    return logpdf_folded_normal_distribution(predictions, observable_indices, 0, std)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_half_normal_distribution_summed","title":"<code>logpdf_half_normal_distribution_summed(predictions, selector_matrix, observable_indices, std)</code>","text":"<p>Compute the log PDF values of half normal distributions for given predictions and sum them using a selector matrix.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to sum the log PDF values. Of shape (n_likelihoods, n_observables).</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>std</code> <code>array</code> <p>The standard deviations of the half normal distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log PDF values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_half_normal_distribution_summed(\n    predictions: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: jnp.array,\n    std: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log PDF values of half normal distributions for given predictions and sum them using a selector matrix.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    selector_matrix : jnp.array\n        The selector matrix to sum the log PDF values. Of shape (n_likelihoods, n_observables).\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    std : jnp.array\n        The standard deviations of the half normal distributions.\n\n    Returns\n    -------\n    jnp.array\n        The summed log PDF values.\n    '''\n    return logpdf_folded_normal_distribution_summed(predictions, selector_matrix, observable_indices, 0, std)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_multivariate_normal_distribution","title":"<code>logpdf_multivariate_normal_distribution(predictions, observable_indices, mean, standard_deviation, inverse_correlation, logpdf_normalization_per_observable)</code>","text":"<p>Compute the log PDF values of multivariate normal distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>observable_indices</code> <code>List[array]</code> <p>The indices of the constrained observables.</p> required <code>mean</code> <code>List[array]</code> <p>The mean values of the multivariate normal distributions.</p> required <code>standard_deviation</code> <code>List[array]</code> <p>The standard deviations of the multivariate normal distributions.</p> required <code>inverse_correlation</code> <code>List[array]</code> <p>The inverse correlation matrices of the multivariate normal distributions.</p> required <code>logpdf_normalization_per_observable</code> <code>List[array]</code> <p>The log PDF normalization constants for each observable.</p> required <p>Returns:</p> Type Description <code>List[array]</code> <p>The log PDF values for each observable and distribution.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_multivariate_normal_distribution(\n    predictions: jnp.array,\n    observable_indices: List[jnp.array],\n    mean: List[jnp.array],\n    standard_deviation: List[jnp.array],\n    inverse_correlation: List[jnp.array],\n    logpdf_normalization_per_observable: List[jnp.array],\n) -&gt; List[jnp.array]:\n    '''\n    Compute the log PDF values of multivariate normal distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    observable_indices : List[jnp.array]\n        The indices of the constrained observables.\n    mean : List[jnp.array]\n        The mean values of the multivariate normal distributions.\n    standard_deviation : List[jnp.array]\n        The standard deviations of the multivariate normal distributions.\n    inverse_correlation : List[jnp.array]\n        The inverse correlation matrices of the multivariate normal distributions.\n    logpdf_normalization_per_observable : List[jnp.array]\n        The log PDF normalization constants for each observable.\n\n    Returns\n    -------\n    List[jnp.array]\n        The log PDF values for each observable and distribution.\n    '''\n    logpdfs = []\n    for i in range(len(observable_indices)):\n        logpdf_total = jnp.zeros_like(predictions)\n        d = (jnp.take(predictions, observable_indices[i]) - mean[i]) / standard_deviation[i]\n        logpdf = -0.5 * d * jnp.dot(inverse_correlation[i], d) + logpdf_normalization_per_observable[i]\n        logpdf_total = logpdf_total.at[observable_indices[i]].add(logpdf)\n        logpdfs.append(logpdf_total)\n    return jnp.stack(logpdfs)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_multivariate_normal_distribution_summed","title":"<code>logpdf_multivariate_normal_distribution_summed(predictions, selector_matrix, observable_indices, mean, standard_deviation, inverse_correlation, logpdf_normalization_per_observable)</code>","text":"<p>Compute the summed log PDF values of multivariate normal distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to sum the log PDF values of different multivariate normal distributions. Of shape (n_likelihoods, n_distributions).</p> required <code>observable_indices</code> <code>List[array]</code> <p>The indices of the constrained observables.</p> required <code>mean</code> <code>List[array]</code> <p>The mean values of the multivariate normal distributions.</p> required <code>standard_deviation</code> <code>List[array]</code> <p>The standard deviations of the multivariate normal distributions.</p> required <code>inverse_correlation</code> <code>List[array]</code> <p>The inverse correlation matrices of the multivariate normal distributions.</p> required <code>logpdf_normalization_per_observable</code> <code>List[array]</code> <p>The log PDF normalization constants for each observable.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log PDF values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_multivariate_normal_distribution_summed(\n    predictions: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: List[jnp.array],\n    mean: List[jnp.array],\n    standard_deviation: List[jnp.array],\n    inverse_correlation: List[jnp.array],\n    logpdf_normalization_per_observable: List[jnp.array],\n) -&gt; jnp.array:\n    '''\n    Compute the summed log PDF values of multivariate normal distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    selector_matrix : jnp.array\n        The selector matrix to sum the log PDF values of different multivariate normal distributions. Of shape (n_likelihoods, n_distributions).\n    observable_indices : List[jnp.array]\n        The indices of the constrained observables.\n    mean : List[jnp.array]\n        The mean values of the multivariate normal distributions.\n    standard_deviation : List[jnp.array]\n        The standard deviations of the multivariate normal distributions.\n    inverse_correlation : List[jnp.array]\n        The inverse correlation matrices of the multivariate normal distributions.\n    logpdf_normalization_per_observable : List[jnp.array]\n        The log PDF normalization constants for each observable.\n\n    Returns\n    -------\n    jnp.array\n        The summed log PDF values.\n    '''\n    logpdf_rows = []\n    for i in range(len(observable_indices)):\n        d = (jnp.take(predictions, observable_indices[i]) - mean[i]) / standard_deviation[i]\n        n_obs = d.shape[0]\n        logpdf = -0.5 * jnp.dot(d, jnp.dot(inverse_correlation[i], d)) + n_obs * logpdf_normalization_per_observable[i]\n        logpdf_rows.append(logpdf)\n    logpdf_total = jnp.stack(logpdf_rows)\n    return selector_matrix @ logpdf_total\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_normal_distribution","title":"<code>logpdf_normal_distribution(predictions, observable_indices, mean, std)</code>","text":"<p>Compute the log PDF values of normal distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>mean</code> <code>array</code> <p>The means of the normal distributions.</p> required <code>std</code> <code>array</code> <p>The standard deviations of the normal distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The log PDF values for the predictions.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_normal_distribution(\n    predictions: jnp.array,\n    observable_indices: jnp.array,\n    mean: jnp.array,\n    std: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log PDF values of normal distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    mean : jnp.array\n        The means of the normal distributions.\n    std : jnp.array\n        The standard deviations of the normal distributions.\n\n    Returns\n    -------\n    jnp.array\n        The log PDF values for the predictions.\n    '''\n    logpdf_total = jnp.zeros_like(predictions)\n    predictions = jnp.take(predictions, observable_indices)\n    logpdf = jsp.stats.norm.logpdf(predictions, loc=mean, scale=std)\n    logpdf_total = logpdf_total.at[observable_indices].add(logpdf)\n    return logpdf_total\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_normal_distribution_summed","title":"<code>logpdf_normal_distribution_summed(predictions, selector_matrix, observable_indices, mean, std)</code>","text":"<p>Compute the log PDF values of normal distributions for given predictions and sum them using a selector matrix.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to sum the log PDF values. Of shape (n_likelihoods, n_observables).</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>mean</code> <code>array</code> <p>The means of the normal distributions.</p> required <code>std</code> <code>array</code> <p>The standard deviations of the normal distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log PDF values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_normal_distribution_summed(\n    predictions: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: jnp.array,\n    mean: jnp.array,\n    std: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log PDF values of normal distributions for given predictions and sum them using a selector matrix.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    selector_matrix : jnp.array\n        The selector matrix to sum the log PDF values. Of shape (n_likelihoods, n_observables).\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    mean : jnp.array\n        The means of the normal distributions.\n    std : jnp.array\n        The standard deviations of the normal distributions.\n\n    Returns\n    -------\n    jnp.array\n        The summed log PDF values.\n    '''\n    return selector_matrix @ logpdf_normal_distribution(predictions, observable_indices, mean, std)\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_numerical_distribution","title":"<code>logpdf_numerical_distribution(predictions, observable_indices, x, log_y)</code>","text":"<p>Compute the log PDF values of numerical distributions for given predictions.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>x</code> <code>array</code> <p>The x values of the numerical distributions.</p> required <code>log_y</code> <code>array</code> <p>The log PDF values of the numerical distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The log PDF values for the predictions.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_numerical_distribution(\n    predictions: jnp.array,\n    observable_indices: jnp.array,\n    x: jnp.array,\n    log_y: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log PDF values of numerical distributions for given predictions.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    x : jnp.array\n        The x values of the numerical distributions.\n    log_y : jnp.array\n        The log PDF values of the numerical distributions.\n\n    Returns\n    -------\n    jnp.array\n        The log PDF values for the predictions.\n    '''\n    logpdf_total = jnp.zeros_like(predictions)\n    predictions = jnp.take(predictions, observable_indices)\n    logpdf = vmap(interp_log_pdf)(predictions, x, log_y)\n    logpdf_total = logpdf_total.at[observable_indices].add(logpdf)\n    return logpdf_total\n</code></pre>"},{"location":"jelli/utils/distributions/#jelli.utils.distributions.logpdf_numerical_distribution_summed","title":"<code>logpdf_numerical_distribution_summed(predictions, selector_matrix, observable_indices, x, log_y)</code>","text":"<p>Compute the log PDF values of numerical distributions for given predictions and sum them using a selector matrix.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>array</code> <p>The predicted values.</p> required <code>selector_matrix</code> <code>array</code> <p>The selector matrix to sum the log PDF values. Of shape (n_likelihoods, n_observables).</p> required <code>observable_indices</code> <code>array</code> <p>The indices of the constrained observables.</p> required <code>x</code> <code>array</code> <p>The x values of the numerical distributions.</p> required <code>log_y</code> <code>array</code> <p>The log PDF values of the numerical distributions.</p> required <p>Returns:</p> Type Description <code>array</code> <p>The summed log PDF values.</p> Source code in <code>jelli/utils/distributions.py</code> <pre><code>def logpdf_numerical_distribution_summed(\n    predictions: jnp.array,\n    selector_matrix: jnp.array,\n    observable_indices: jnp.array,\n    x: jnp.array,\n    log_y: jnp.array,\n) -&gt; jnp.array:\n    '''\n    Compute the log PDF values of numerical distributions for given predictions and sum them using a selector matrix.\n\n    Parameters\n    ----------\n    predictions : jnp.array\n        The predicted values.\n    selector_matrix : jnp.array\n        The selector matrix to sum the log PDF values. Of shape (n_likelihoods, n_observables).\n    observable_indices : jnp.array\n        The indices of the constrained observables.\n    x : jnp.array\n        The x values of the numerical distributions.\n    log_y : jnp.array\n        The log PDF values of the numerical distributions.\n\n    Returns\n    -------\n    jnp.array\n        The summed log PDF values.\n    '''\n    return selector_matrix @ logpdf_numerical_distribution(predictions, observable_indices, x, log_y)\n</code></pre>"},{"location":"jelli/utils/jax_helpers/","title":"jelli.utils.jax_helpers","text":""},{"location":"jelli/utils/jax_helpers/#jelli.utils.jax_helpers.batched_outer_ravel","title":"<code>batched_outer_ravel(arr)</code>","text":"<p>Compute the outer product for each 1D array in a batch and return them as raveled 1D arrays.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>A JAX array of shape (..., N), where ... represents any number of batch dimensions</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A JAX array of shape (..., N*N), where each slice along the batch dimensions corresponds to the raveled outer product of the respective input array.</p> Source code in <code>jelli/utils/jax_helpers.py</code> <pre><code>def batched_outer_ravel(arr):\n    '''\n    Compute the outer product for each 1D array in a batch and return them as raveled 1D arrays.\n\n    Parameters\n    ----------\n    arr : jnp.ndarray\n        A JAX array of shape (..., N), where ... represents any number of batch dimensions\n\n    Returns\n    -------\n    jnp.ndarray\n        A JAX array of shape (..., N*N), where each slice along the batch dimensions\n        corresponds to the raveled outer product of the respective input array.\n    '''\n    # Dynamically detect batch dimensions\n    batch_shape = arr.shape[:-1]  # All dimensions except the last one\n\n    # Reshape to flatten batch dimensions for efficient `vmap`\n    arr = arr.reshape((-1, arr.shape[-1]))\n\n    # Vectorize over the flattened batch axis\n    result = vmap(outer_ravel)(arr)\n\n    # Reshape result back to original batch structure\n    return result.reshape(batch_shape + (-1,))\n</code></pre>"},{"location":"jelli/utils/jax_helpers/#jelli.utils.jax_helpers.outer_ravel","title":"<code>outer_ravel(arr)</code>","text":"<p>Compute the outer product of a 1D array and return it as a raveled 1D array.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>A 1D JAX array.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A 1D JAX array representing the raveled outer product of the input array.</p> Source code in <code>jelli/utils/jax_helpers.py</code> <pre><code>def outer_ravel(arr):\n    '''\n    Compute the outer product of a 1D array and return it as a raveled 1D array.\n\n    Parameters\n    ----------\n    arr : jnp.ndarray\n        A 1D JAX array.\n\n    Returns\n    -------\n    jnp.ndarray\n        A 1D JAX array representing the raveled outer product of the input array.\n    '''\n    return jnp.outer(arr, arr).ravel()\n</code></pre>"},{"location":"jelli/utils/par_helpers/","title":"jelli.utils.par_helpers","text":""},{"location":"jelli/utils/par_helpers/#jelli.utils.par_helpers.get_par_monomial_indices","title":"<code>get_par_monomial_indices(keys_pars, keys_coeff)</code>","text":"<p>Computes sorted indices mapping linear parameters to bilinear ones that exist in the provided coefficient list.</p> <p>Parameters:</p> Name Type Description Default <code>keys_pars</code> <code>list</code> <p>List of linear parameter keys, each element is a tuple <code>(w, c)</code>, where <code>w</code> is the parameter name and <code>c</code> is <code>R</code> for real or <code>I</code> for imaginary.</p> required <code>keys_coeff</code> <code>list</code> <p>List of bilinear coefficient keys. Each element is a tuple <code>(w1, w2, c)</code>, where <code>w1</code> and <code>w2</code> are the parameter names and <code>c</code> is <code>RR</code>, <code>RI</code>, <code>IR</code>, or <code>II</code>, denoting all possible interferences.</p> required <p>Returns:</p> Type Description <code>`np.ndarray`</code> <p>Sorted indices of bilinear coefficients that match <code>keys_coeff</code>.</p> Source code in <code>jelli/utils/par_helpers.py</code> <pre><code>def get_par_monomial_indices(keys_pars, keys_coeff):\n    \"\"\"\n    Computes sorted indices mapping linear parameters\n    to bilinear ones that exist in the provided coefficient list.\n\n    Parameters\n    ----------\n    keys_pars : list\n        List of linear parameter keys, each element is a tuple `(w, c)`,\n        where `w` is the parameter name and `c` is `R` for real or `I` for imaginary.\n    keys_coeff : list\n        List of bilinear coefficient keys. Each element is a tuple `(w1, w2, c)`,\n        where `w1` and `w2` are the parameter names and `c` is `RR`, `RI`, `IR`, or `II`,\n        denoting all possible interferences.\n\n    Returns\n    -------\n    `np.ndarray`\n        Sorted indices of bilinear coefficients that match `keys_coeff`.\n    \"\"\"\n    # Generate all possible bilinear combinations of keys_pars\n    keys_pars_bilinears = keys_array(keys_product(\n        keys_pars, keys_pars\n    ))\n    bilin_bools = keys_isin(keys_pars_bilinears, keys_coeff)\n    # Take elements of keys_pars_bilinears that exist in keys_coeff and obtain indices that sort them\n    sort_indices = np.argsort(keys_pars_bilinears[bilin_bools])\n    bilin_indices = np.where(bilin_bools)[0]\n    bilin_sort_indices = bilin_indices[sort_indices]\n    return bilin_sort_indices\n</code></pre>"},{"location":"jelli/utils/par_helpers/#jelli.utils.par_helpers.get_sector_indices_from_wcxf","title":"<code>get_sector_indices_from_wcxf(eft, basis, sectors)</code>","text":"<p>Get indices of Wilson coefficients from the full basis corresponding to specified sectors.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The effective field theory (EFT) name, e.g., <code>SMEFT</code> or <code>WET</code>.</p> required <code>basis</code> <code>str</code> <p>The Wilson coefficient basis name, e.g., <code>Warsaw</code> or <code>JMS</code>.</p> required <code>sectors</code> <code>list</code> <p>A list of sector names to extract indices for.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>An array of indices corresponding to the Wilson coefficients in the specified sectors.</p> Source code in <code>jelli/utils/par_helpers.py</code> <pre><code>def get_sector_indices_from_wcxf(eft, basis, sectors):\n    '''\n    Get indices of Wilson coefficients from the full basis corresponding to specified sectors.\n\n    Parameters\n    ----------\n    eft : str\n        The effective field theory (EFT) name, e.g., `SMEFT` or `WET`.\n    basis : str\n        The Wilson coefficient basis name, e.g., `Warsaw` or `JMS`.\n    sectors : list\n        A list of sector names to extract indices for.\n\n    Returns\n    -------\n    np.ndarray\n        An array of indices corresponding to the Wilson coefficients in the specified sectors.\n    '''\n    basis_full = get_wc_basis_from_wcxf(eft, basis)\n    return np.concatenate([\n        [basis_full.index(wc) for wc in get_wc_basis_from_wcxf(eft, basis, sector)]\n        for sector in sectors\n    ])\n</code></pre>"},{"location":"jelli/utils/par_helpers/#jelli.utils.par_helpers.get_wc_basis_from_wcxf","title":"<code>get_wc_basis_from_wcxf(eft, basis, sector=None, split_re_im=True)</code>","text":"<p>Retrieve the Wilson coefficient basis from WCxf.</p> <p>Parameters:</p> Name Type Description Default <code>eft</code> <code>str</code> <p>The effective field theory (EFT) name, e.g., <code>SMEFT</code> or <code>WET</code>.</p> required <code>basis</code> <code>str</code> <p>The Wilson coefficient basis name, e.g., <code>Warsaw</code> or <code>JMS</code>.</p> required <code>sector</code> <code>str</code> <p>The RGE sector of interest. If <code>None</code>, all sectors are included.</p> <code>None</code> <code>split_re_im</code> <code>bool</code> <p>If <code>True</code>, split complex Wilson coefficients into their real and imaginary parts. If <code>False</code>, keep them as complex parameters. Default is <code>True</code>.</p> <code>True</code> <p>Returns:</p> Type Description <code>list</code> <p>A sorted list of Wilson coefficient names. If <code>split_re_im</code> is <code>True</code>, complex coefficients are represented as tuples <code>(name, 'R')</code> and <code>(name, 'I')</code> for their real and imaginary parts, respectively.</p> Source code in <code>jelli/utils/par_helpers.py</code> <pre><code>def get_wc_basis_from_wcxf(eft, basis, sector=None, split_re_im=True):\n    '''\n    Retrieve the Wilson coefficient basis from WCxf.\n\n    Parameters\n    ----------\n    eft : str\n        The effective field theory (EFT) name, e.g., `SMEFT` or `WET`.\n    basis : str\n        The Wilson coefficient basis name, e.g., `Warsaw` or `JMS`.\n    sector : str, optional\n        The RGE sector of interest. If `None`, all sectors are included.\n    split_re_im : bool, optional\n        If `True`, split complex Wilson coefficients into their real and imaginary parts.\n        If `False`, keep them as complex parameters. Default is `True`.\n\n    Returns\n    -------\n    list\n        A sorted list of Wilson coefficient names. If `split_re_im` is `True`,\n        complex coefficients are represented as tuples `(name, 'R')` and `(name, 'I')`\n        for their real and imaginary parts, respectively.\n    '''\n    from wilson import wcxf\n    basis_obj = wcxf.Basis[eft, basis]\n    wc_list = []\n\n    if sector and sector not in basis_obj.sectors.keys():\n        raise ValueError(f\"Sector {sector} not found in basis {basis} of EFT {eft}\")\n\n    if split_re_im:\n        for sec, s in basis_obj.sectors.items():\n            if not sector or sec == sector:\n                for name, d in s.items():\n                    if not d or 'real' not in d or not d['real']:\n                        wc_list.append((name, 'R'))\n                        wc_list.append((name, 'I'))\n                    else:\n                        wc_list.append((name, 'R'))\n    else:\n        for sec, s in basis_obj.sectors.items():\n            if not sector or sec == sector:\n                for name, d in s.items():\n                    wc_list.append(name)\n    return sorted(wc_list)\n</code></pre>"},{"location":"jelli/utils/par_helpers/#jelli.utils.par_helpers.keys_array","title":"<code>keys_array(keys)</code>","text":"<p>Converts a list of tuples into a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>list</code> <p>A list containing tuples.</p> required <p>Returns:</p> Type Description <code>`np.ndarray`</code> <p>A numpy array with dtype=tuple containing the provided keys.</p> Source code in <code>jelli/utils/par_helpers.py</code> <pre><code>def keys_array(keys):\n    \"\"\"\n    Converts a list of tuples into a numpy array.\n\n    Parameters\n    ----------\n    keys : list\n        A list containing tuples.\n\n    Returns\n    -------\n    `np.ndarray`\n        A numpy array with dtype=tuple containing the provided keys.\n    \"\"\"\n    array = np.empty(len(keys), dtype=tuple)\n    array[:] = keys\n    return array\n</code></pre>"},{"location":"jelli/utils/par_helpers/#jelli.utils.par_helpers.keys_isin","title":"<code>keys_isin(keys_a, keys_b)</code>","text":"<p>Checks if elements in <code>keys_a</code> exist in <code>keys_b</code>.</p> <p>Parameters:</p> Name Type Description Default <code>keys_a</code> <code>list</code> <p>List of keys to check.</p> required <code>keys_b</code> <code>list</code> <p>List of reference keys.</p> required <p>Returns:</p> Type Description <code>`np.ndarray`</code> <p>Boolean numpy array indicating presence of each key in <code>keys_a</code> within <code>keys_b</code>.</p> Source code in <code>jelli/utils/par_helpers.py</code> <pre><code>def keys_isin(keys_a, keys_b):\n    \"\"\"\n    Checks if elements in `keys_a` exist in `keys_b`.\n\n    Parameters\n    ----------\n    keys_a : list\n        List of keys to check.\n    keys_b : list\n        List of reference keys.\n\n    Returns\n    -------\n    `np.ndarray`\n        Boolean numpy array indicating presence of each key in `keys_a` within `keys_b`.\n    \"\"\"\n    set_b = set(keys_b)\n    res = np.array([item in set_b for item in keys_a])\n    return res if res.size &gt; 0 else np.array([], dtype=bool)\n</code></pre>"},{"location":"jelli/utils/par_helpers/#jelli.utils.par_helpers.keys_product","title":"<code>keys_product(keys_a, keys_b)</code>","text":"<p>Computes the Cartesian product of two sets of keys, producing bilinear combinations.</p> <p>Parameters:</p> Name Type Description Default <code>keys_a</code> <p>A list where each element is a tuple of the form (w, c).</p> required <code>keys_b</code> <p>Another list with elements of the form (w, c).</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of bilinear combinations in the form (w_a, w_b, c_a + c_b).</p> Source code in <code>jelli/utils/par_helpers.py</code> <pre><code>def keys_product(keys_a, keys_b):\n    \"\"\"\n    Computes the Cartesian product of two sets of keys, producing bilinear combinations.\n\n    Parameters\n    ----------\n    keys_a: list\n        A list where each element is a tuple of the form (w, c).\n    keys_b: list\n        Another list with elements of the form (w, c).\n\n    Returns\n    -------\n    list\n        A list of bilinear combinations in the form (w_a, w_b, c_a + c_b).\n    \"\"\"\n    if len(keys_a[0]) == 2:\n        return [\n            (w_a, w_b, c_a+c_b)\n            for ((w_a, c_a), (w_b, c_b)) in product(keys_a, keys_b)\n        ]\n    else:\n        raise ValueError(\"keys must be of the form (w,c)\")\n</code></pre>"},{"location":"jelli/utils/probability/","title":"jelli.utils.probability","text":""},{"location":"jelli/utils/probability/#jelli.utils.probability.GammaDistribution","title":"<code>GammaDistribution</code>","text":"<p>               Bases: <code>ProbabilityDistribution</code></p> <p>A Gamma distribution defined like the <code>gamma</code> distribution in <code>scipy.stats</code> (with parameters <code>a</code>, <code>loc</code>, <code>scale</code>).</p> <p>The <code>central_value</code> attribute returns the location of the mode.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>class GammaDistribution(ProbabilityDistribution):\n    r\"\"\"A Gamma distribution defined like the `gamma` distribution in\n    `scipy.stats` (with parameters `a`, `loc`, `scale`).\n\n    The `central_value` attribute returns the location of the mode.\n    \"\"\"\n\n    def __init__(self, a, loc, scale):\n        if loc &gt; 0:\n            raise ValueError(\"loc must be negative or zero\")\n        # \"frozen\" scipy distribution object\n        self.scipy_dist = scipy.stats.gamma(a=a, loc=loc, scale=scale)\n        mode = loc + (a-1)*scale\n        # support extends until the CDF is roughly \"6 sigma\"\n        support_min = min(self.scipy_dist.ppf(1e-9), mode)\n        support_max = self.scipy_dist.ppf(1-1e-9)\n        super().__init__(central_value=mode, # the mode\n                         support=(support_min, support_max))\n        self.a = a\n        self.loc = loc\n        self.scale = scale\n\n    def __repr__(self):\n        return 'flavio.statistics.probability.GammaDistribution' + \\\n               '({}, {}, {})'.format(self.a, self.loc, self.scale)\n\n    def get_random(self, size):\n        return self.scipy_dist.rvs(size=size)\n\n    def cdf(self, x):\n        return self.scipy_dist.cdf(x)\n\n    def ppf(self, x):\n        return self.scipy_dist.ppf(x)\n\n    def logpdf(self, x):\n        return self.scipy_dist.logpdf(x)\n\n    def _find_error_cdf(self, confidence_level):\n        # find the value of the CDF at the position of the left boundary\n        # of the `confidence_level`% CL range by demanding that the value\n        # of the PDF is the same at the two boundaries\n        def x_left(a):\n            return self.ppf(a)\n        def x_right(a):\n            return self.ppf(a + confidence_level)\n        def diff_logpdf(a):\n            logpdf_x_left = self.logpdf(x_left(a))\n            logpdf_x_right = self.logpdf(x_right(a))\n            return logpdf_x_left - logpdf_x_right\n        return scipy.optimize.brentq(diff_logpdf, 0,  1 - confidence_level-1e-6)\n\n    def get_error_left(self, nsigma=1, **kwargs):\n        \"\"\"Return the lower error\"\"\"\n        a = self._find_error_cdf(confidence_level(nsigma))\n        return self.central_value - self.ppf(a)\n\n    def get_error_right(self, nsigma=1, **kwargs):\n        \"\"\"Return the upper error\"\"\"\n        a = self._find_error_cdf(confidence_level(nsigma))\n        return self.ppf(a + confidence_level(nsigma)) - self.central_value\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.GammaDistribution.get_error_left","title":"<code>get_error_left(nsigma=1, **kwargs)</code>","text":"<p>Return the lower error</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_error_left(self, nsigma=1, **kwargs):\n    \"\"\"Return the lower error\"\"\"\n    a = self._find_error_cdf(confidence_level(nsigma))\n    return self.central_value - self.ppf(a)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.GammaDistribution.get_error_right","title":"<code>get_error_right(nsigma=1, **kwargs)</code>","text":"<p>Return the upper error</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_error_right(self, nsigma=1, **kwargs):\n    \"\"\"Return the upper error\"\"\"\n    a = self._find_error_cdf(confidence_level(nsigma))\n    return self.ppf(a + confidence_level(nsigma)) - self.central_value\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NormalDistribution","title":"<code>NormalDistribution</code>","text":"<p>               Bases: <code>ProbabilityDistribution</code></p> <p>Univariate normal or Gaussian distribution.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>class NormalDistribution(ProbabilityDistribution):\n    \"\"\"Univariate normal or Gaussian distribution.\"\"\"\n\n    def __init__(self, central_value, standard_deviation):\n        \"\"\"Initialize the distribution.\n\n        Parameters:\n\n        - central_value: location (mode and mean)\n        - standard_deviation: standard deviation\n        \"\"\"\n        super().__init__(central_value,\n                         support=(central_value - 6 * standard_deviation,\n                                  central_value + 6 * standard_deviation))\n        if standard_deviation &lt;= 0:\n            raise ValueError(\"Standard deviation must be positive number\")\n        self.standard_deviation = standard_deviation\n\n    def __repr__(self):\n        return 'flavio.statistics.probability.NormalDistribution' + \\\n               '({}, {})'.format(self.central_value, self.standard_deviation)\n\n    def get_random(self, size=None):\n        return np.random.normal(self.central_value, self.standard_deviation, size)\n\n    def logpdf(self, x):\n        return normal_logpdf(x, self.central_value, self.standard_deviation)\n\n    def pdf(self, x):\n        return normal_pdf(x, self.central_value, self.standard_deviation)\n\n    def cdf(self, x):\n        return scipy.stats.norm.cdf(x, self.central_value, self.standard_deviation)\n\n    def ppf(self, x):\n        return scipy.stats.norm.ppf(x, self.central_value, self.standard_deviation)\n\n    def get_error_left(self, nsigma=1, **kwargs):\n        \"\"\"Return the lower error\"\"\"\n        return nsigma * self.standard_deviation\n\n    def get_error_right(self, nsigma=1, **kwargs):\n        \"\"\"Return the upper error\"\"\"\n        return nsigma * self.standard_deviation\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NormalDistribution.__init__","title":"<code>__init__(central_value, standard_deviation)</code>","text":"<p>Initialize the distribution.</p> <p>Parameters:</p> <ul> <li>central_value: location (mode and mean)</li> <li>standard_deviation: standard deviation</li> </ul> Source code in <code>jelli/utils/probability.py</code> <pre><code>def __init__(self, central_value, standard_deviation):\n    \"\"\"Initialize the distribution.\n\n    Parameters:\n\n    - central_value: location (mode and mean)\n    - standard_deviation: standard deviation\n    \"\"\"\n    super().__init__(central_value,\n                     support=(central_value - 6 * standard_deviation,\n                              central_value + 6 * standard_deviation))\n    if standard_deviation &lt;= 0:\n        raise ValueError(\"Standard deviation must be positive number\")\n    self.standard_deviation = standard_deviation\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NormalDistribution.get_error_left","title":"<code>get_error_left(nsigma=1, **kwargs)</code>","text":"<p>Return the lower error</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_error_left(self, nsigma=1, **kwargs):\n    \"\"\"Return the lower error\"\"\"\n    return nsigma * self.standard_deviation\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NormalDistribution.get_error_right","title":"<code>get_error_right(nsigma=1, **kwargs)</code>","text":"<p>Return the upper error</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_error_right(self, nsigma=1, **kwargs):\n    \"\"\"Return the upper error\"\"\"\n    return nsigma * self.standard_deviation\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NumericalDistribution","title":"<code>NumericalDistribution</code>","text":"<p>               Bases: <code>ProbabilityDistribution</code></p> <p>Univariate distribution defined in terms of numerical values for the PDF.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>class NumericalDistribution(ProbabilityDistribution):\n    \"\"\"Univariate distribution defined in terms of numerical values for the\n    PDF.\"\"\"\n\n    def __init__(self, x, y, central_value=None):\n        \"\"\"Initialize a 1D numerical distribution.\n\n        Parameters:\n\n        - `x`: x-axis values. Must be a 1D array of real values in strictly\n          ascending order (but not necessarily evenly spaced)\n        - `y`: PDF values. Must be a 1D array of real positive values with the\n          same length as `x`\n        - central_value: if None (default), will be set to the mode of the\n          distribution, i.e. the x-value where y is largest (by looking up\n          the input arrays, i.e. without interpolation!)\n        \"\"\"\n        self.x = x\n        self.y = y\n        if central_value is not None:\n            if x[0] &lt;= central_value &lt;= x[-1]:\n                super().__init__(central_value=central_value,\n                                 support=(x[0], x[-1]))\n            else:\n                raise ValueError(\"Central value must be within range provided\")\n        else:\n            mode = x[np.argmax(y)]\n            super().__init__(central_value=mode, support=(x[0], x[-1]))\n        self.y_norm = y /  np.trapz(y, x=x)  # normalize PDF to 1\n        self.y_norm[self.y_norm &lt; 0] = 0\n        self.pdf_interp = interp1d(x, self.y_norm,\n                                        fill_value=0, bounds_error=False)\n        _cdf = np.zeros(len(x))\n        _cdf[1:] = np.cumsum(self.y_norm[:-1] * np.diff(x))\n        _cdf = _cdf/_cdf[-1] # normalize CDF to 1\n        self.ppf_interp = interp1d(_cdf, x)\n        self.cdf_interp = interp1d(x, _cdf)\n\n    def __repr__(self):\n        return 'flavio.statistics.probability.NumericalDistribution' + \\\n               '({}, {})'.format(self.x, self.y)\n\n    def get_random(self, size=None):\n        \"\"\"Draw a random number from the distribution.\n\n        If size is not None but an integer N, return an array of N numbers.\"\"\"\n        r = np.random.uniform(size=size)\n        return self.ppf_interp(r)\n\n    def ppf(self, x):\n        return self.ppf_interp(x)\n\n    def cdf(self, x):\n        return self.cdf_interp(x)\n\n    def pdf(self, x):\n        return self.pdf_interp(x)\n\n    def logpdf(self, x):\n        # ignore warning from log(0)=-np.inf\n        with np.errstate(divide='ignore', invalid='ignore'):\n            return np.log(self.pdf_interp(x))\n\n    def _find_error_cdf(self, confidence_level):\n        # find the value of the CDF at the position of the left boundary\n        # of the `confidence_level`% CL range by demanding that the value\n        # of the PDF is the same at the two boundaries\n        def x_left(a):\n            return self.ppf(a)\n        def x_right(a):\n            return self.ppf(a + confidence_level)\n        def diff_logpdf(a):\n            logpdf_x_left = self.logpdf(x_left(a))\n            logpdf_x_right = self.logpdf(x_right(a))\n            return logpdf_x_left - logpdf_x_right\n        return scipy.optimize.brentq(diff_logpdf, 0,  1 - confidence_level-1e-6)\n\n    def get_error_left(self, nsigma=1, method='central'):\n        \"\"\"Return the lower error.\n\n        'method' should be one of:\n\n        - 'central' for a central interval (same probability on both sides of\n          the central value)\n        - 'hpd' for highest posterior density, i.e. probability is larger inside\n          the interval than outside\n        - 'limit' for a one-sided error, i.e. a lower limit\"\"\"\n        if method == 'limit':\n            return self.central_value - self.ppf(1 - confidence_level(nsigma))\n        cdf_central = self.cdf(self.central_value)\n        err_left = self.central_value - self.ppf(cdf_central * (1 - confidence_level(nsigma)))\n        if method == 'central':\n            return err_left\n        elif method == 'hpd':\n            if self.pdf(self.central_value + self.get_error_right(method='central')) == self.pdf(self.central_value - err_left):\n                return err_left\n            try:\n                a = self._find_error_cdf(confidence_level(nsigma))\n            except ValueError:\n                return np.nan\n            return self.central_value - self.ppf(a)\n        else:\n            raise ValueError(\"Method \" + str(method) + \" unknown\")\n\n    def get_error_right(self, nsigma=1, method='central'):\n        \"\"\"Return the upper error\n\n        'method' should be one of:\n\n        - 'central' for a central interval (same probability on both sides of\n          the central value)\n        - 'hpd' for highest posterior density, i.e. probability is larger inside\n          the interval than outside\n        - 'limit' for a one-sided error, i.e. an upper limit\"\"\"\n        if method == 'limit':\n            return self.ppf(confidence_level(nsigma)) - self.central_value\n        cdf_central = self.cdf(self.central_value)\n        err_right = self.ppf(cdf_central + (1 - cdf_central) * confidence_level(nsigma)) - self.central_value\n        if method == 'central':\n            return err_right\n        elif method == 'hpd':\n            if self.pdf(self.central_value - self.get_error_left(method='central')) == self.pdf(self.central_value + err_right):\n                return err_right\n            try:\n                a = self._find_error_cdf(confidence_level(nsigma))\n            except ValueError:\n                return np.nan\n            return self.ppf(a + confidence_level(nsigma)) - self.central_value\n        else:\n            raise ValueError(\"Method \" + str(method) + \" unknown\")\n\n    @classmethod\n    def from_pd(cls, pd, nsteps=1000):\n        if isinstance(pd, NumericalDistribution):\n            return pd\n        _x = np.linspace(pd.support[0], pd.support[-1], nsteps)\n        _y = np.exp(pd.logpdf(_x))\n        return cls(central_value=pd.central_value, x=_x, y=_y)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NumericalDistribution.__init__","title":"<code>__init__(x, y, central_value=None)</code>","text":"<p>Initialize a 1D numerical distribution.</p> <p>Parameters:</p> <ul> <li><code>x</code>: x-axis values. Must be a 1D array of real values in strictly   ascending order (but not necessarily evenly spaced)</li> <li><code>y</code>: PDF values. Must be a 1D array of real positive values with the   same length as <code>x</code></li> <li>central_value: if None (default), will be set to the mode of the   distribution, i.e. the x-value where y is largest (by looking up   the input arrays, i.e. without interpolation!)</li> </ul> Source code in <code>jelli/utils/probability.py</code> <pre><code>def __init__(self, x, y, central_value=None):\n    \"\"\"Initialize a 1D numerical distribution.\n\n    Parameters:\n\n    - `x`: x-axis values. Must be a 1D array of real values in strictly\n      ascending order (but not necessarily evenly spaced)\n    - `y`: PDF values. Must be a 1D array of real positive values with the\n      same length as `x`\n    - central_value: if None (default), will be set to the mode of the\n      distribution, i.e. the x-value where y is largest (by looking up\n      the input arrays, i.e. without interpolation!)\n    \"\"\"\n    self.x = x\n    self.y = y\n    if central_value is not None:\n        if x[0] &lt;= central_value &lt;= x[-1]:\n            super().__init__(central_value=central_value,\n                             support=(x[0], x[-1]))\n        else:\n            raise ValueError(\"Central value must be within range provided\")\n    else:\n        mode = x[np.argmax(y)]\n        super().__init__(central_value=mode, support=(x[0], x[-1]))\n    self.y_norm = y /  np.trapz(y, x=x)  # normalize PDF to 1\n    self.y_norm[self.y_norm &lt; 0] = 0\n    self.pdf_interp = interp1d(x, self.y_norm,\n                                    fill_value=0, bounds_error=False)\n    _cdf = np.zeros(len(x))\n    _cdf[1:] = np.cumsum(self.y_norm[:-1] * np.diff(x))\n    _cdf = _cdf/_cdf[-1] # normalize CDF to 1\n    self.ppf_interp = interp1d(_cdf, x)\n    self.cdf_interp = interp1d(x, _cdf)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NumericalDistribution.get_error_left","title":"<code>get_error_left(nsigma=1, method='central')</code>","text":"<p>Return the lower error.</p> <p>'method' should be one of:</p> <ul> <li>'central' for a central interval (same probability on both sides of   the central value)</li> <li>'hpd' for highest posterior density, i.e. probability is larger inside   the interval than outside</li> <li>'limit' for a one-sided error, i.e. a lower limit</li> </ul> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_error_left(self, nsigma=1, method='central'):\n    \"\"\"Return the lower error.\n\n    'method' should be one of:\n\n    - 'central' for a central interval (same probability on both sides of\n      the central value)\n    - 'hpd' for highest posterior density, i.e. probability is larger inside\n      the interval than outside\n    - 'limit' for a one-sided error, i.e. a lower limit\"\"\"\n    if method == 'limit':\n        return self.central_value - self.ppf(1 - confidence_level(nsigma))\n    cdf_central = self.cdf(self.central_value)\n    err_left = self.central_value - self.ppf(cdf_central * (1 - confidence_level(nsigma)))\n    if method == 'central':\n        return err_left\n    elif method == 'hpd':\n        if self.pdf(self.central_value + self.get_error_right(method='central')) == self.pdf(self.central_value - err_left):\n            return err_left\n        try:\n            a = self._find_error_cdf(confidence_level(nsigma))\n        except ValueError:\n            return np.nan\n        return self.central_value - self.ppf(a)\n    else:\n        raise ValueError(\"Method \" + str(method) + \" unknown\")\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NumericalDistribution.get_error_right","title":"<code>get_error_right(nsigma=1, method='central')</code>","text":"<p>Return the upper error</p> <p>'method' should be one of:</p> <ul> <li>'central' for a central interval (same probability on both sides of   the central value)</li> <li>'hpd' for highest posterior density, i.e. probability is larger inside   the interval than outside</li> <li>'limit' for a one-sided error, i.e. an upper limit</li> </ul> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_error_right(self, nsigma=1, method='central'):\n    \"\"\"Return the upper error\n\n    'method' should be one of:\n\n    - 'central' for a central interval (same probability on both sides of\n      the central value)\n    - 'hpd' for highest posterior density, i.e. probability is larger inside\n      the interval than outside\n    - 'limit' for a one-sided error, i.e. an upper limit\"\"\"\n    if method == 'limit':\n        return self.ppf(confidence_level(nsigma)) - self.central_value\n    cdf_central = self.cdf(self.central_value)\n    err_right = self.ppf(cdf_central + (1 - cdf_central) * confidence_level(nsigma)) - self.central_value\n    if method == 'central':\n        return err_right\n    elif method == 'hpd':\n        if self.pdf(self.central_value - self.get_error_left(method='central')) == self.pdf(self.central_value + err_right):\n            return err_right\n        try:\n            a = self._find_error_cdf(confidence_level(nsigma))\n        except ValueError:\n            return np.nan\n        return self.ppf(a + confidence_level(nsigma)) - self.central_value\n    else:\n        raise ValueError(\"Method \" + str(method) + \" unknown\")\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.NumericalDistribution.get_random","title":"<code>get_random(size=None)</code>","text":"<p>Draw a random number from the distribution.</p> <p>If size is not None but an integer N, return an array of N numbers.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_random(self, size=None):\n    \"\"\"Draw a random number from the distribution.\n\n    If size is not None but an integer N, return an array of N numbers.\"\"\"\n    r = np.random.uniform(size=size)\n    return self.ppf_interp(r)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution","title":"<code>ProbabilityDistribution</code>","text":"<p>               Bases: <code>object</code></p> <p>Common base class for all probability distributions</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>class ProbabilityDistribution(object):\n    \"\"\"Common base class for all probability distributions\"\"\"\n\n    def __init__(self, central_value, support):\n        self.central_value = central_value\n        self.support = support\n\n    @classmethod\n    def get_subclasses(cls):\n        \"\"\"Return all subclasses (including subclasses of subclasses).\"\"\"\n        for subclass in cls.__subclasses__():\n            yield from subclass.get_subclasses()\n            yield subclass\n\n    def get_central(self):\n        return self.central_value\n\n    @property\n    def error_left(self):\n        \"\"\"Return the lower error\"\"\"\n        return self.get_error_left()\n\n    @property\n    def error_right(self):\n        \"\"\"Return the upper error\"\"\"\n        return self.get_error_right()\n\n    @classmethod\n    def class_to_string(cls):\n        \"\"\"Get a string name for a given ProbabilityDistribution subclass.\n\n        This converts camel case to underscore and removes the word\n        'distribution'.\n\n        Example: class_to_string(AsymmetricNormalDistribution) returns\n        'asymmetric_normal'.\n        \"\"\"\n        name = _camel_to_underscore(cls.__name__)\n        return name.replace('_distribution', '')\n\n    def get_dict(self, distribution=False, iterate=False, arraytolist=False):\n        \"\"\"Get an ordered dictionary with arguments and values needed to\n        the instantiate the distribution.\n\n        Optional arguments (default to False):\n\n        - `distribution`: add a 'distribution' key to the dictionary with the\n        value being the string representation of the distribution's name\n        (e.g. 'asymmetric_normal').\n        - `iterate`: If ProbabilityDistribution instances are among the\n        arguments (e.g. for KernelDensityEstimate), return the instance's\n        get_dict instead of the instance as value.\n        - `arraytolist`: convert numpy arrays to lists\n        \"\"\"\n        args = inspect.signature(self.__class__).parameters.keys()\n        d = self.__dict__\n        od = OrderedDict()\n        if distribution:\n            od['distribution'] = self.class_to_string()\n        od.update(OrderedDict((a, d[a]) for a in args))\n        if iterate:\n            for k in od:\n                if isinstance(od[k], ProbabilityDistribution):\n                    od[k] = od[k].get_dict(distribution=True)\n        if arraytolist:\n            for k in od:\n                if isinstance(od[k], np.ndarray):\n                    od[k] = od[k].tolist()\n                if isinstance(od[k], list):\n                    for i, x in enumerate(od[k]):\n                        if isinstance(x, np.ndarray):\n                            od[k][i] = od[k][i].tolist()\n        for k in od:\n            if isinstance(od[k], int):\n                od[k] = int(od[k])\n            elif isinstance(od[k], float):\n                od[k] = float(od[k])\n            if isinstance(od[k], list):\n                for i, x in enumerate(od[k]):\n                    if isinstance(x, float):\n                        od[k][i] = float(od[k][i])\n                    elif isinstance(x, int):\n                        od[k][i] = int(od[k][i])\n        return od\n\n    def get_yaml(self, *args, **kwargs):\n        \"\"\"Get a YAML string representing the dictionary returned by the\n        get_dict method.\n\n        Arguments will be passed to `yaml.dump`.\"\"\"\n        od = self.get_dict(distribution=True, iterate=True, arraytolist=True)\n        return yaml.dump(od, *args, **kwargs)\n\n    def delta_logpdf(self, x, **kwargs):\n        exclude = kwargs.get('exclude', None)\n        if exclude is not None:\n            d = len(self.central_value)\n            cv = [self.central_value[i] for i in range(d) if i not in exclude]\n        else:\n            cv = self.central_value\n        return self.logpdf(x, **kwargs) - self.logpdf(cv, **kwargs)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution.error_left","title":"<code>error_left</code>  <code>property</code>","text":"<p>Return the lower error</p>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution.error_right","title":"<code>error_right</code>  <code>property</code>","text":"<p>Return the upper error</p>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution.class_to_string","title":"<code>class_to_string()</code>  <code>classmethod</code>","text":"<p>Get a string name for a given ProbabilityDistribution subclass.</p> <p>This converts camel case to underscore and removes the word 'distribution'.</p> <p>Example: class_to_string(AsymmetricNormalDistribution) returns 'asymmetric_normal'.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>@classmethod\ndef class_to_string(cls):\n    \"\"\"Get a string name for a given ProbabilityDistribution subclass.\n\n    This converts camel case to underscore and removes the word\n    'distribution'.\n\n    Example: class_to_string(AsymmetricNormalDistribution) returns\n    'asymmetric_normal'.\n    \"\"\"\n    name = _camel_to_underscore(cls.__name__)\n    return name.replace('_distribution', '')\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution.get_dict","title":"<code>get_dict(distribution=False, iterate=False, arraytolist=False)</code>","text":"<p>Get an ordered dictionary with arguments and values needed to the instantiate the distribution.</p> <p>Optional arguments (default to False):</p> <ul> <li><code>distribution</code>: add a 'distribution' key to the dictionary with the value being the string representation of the distribution's name (e.g. 'asymmetric_normal').</li> <li><code>iterate</code>: If ProbabilityDistribution instances are among the arguments (e.g. for KernelDensityEstimate), return the instance's get_dict instead of the instance as value.</li> <li><code>arraytolist</code>: convert numpy arrays to lists</li> </ul> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_dict(self, distribution=False, iterate=False, arraytolist=False):\n    \"\"\"Get an ordered dictionary with arguments and values needed to\n    the instantiate the distribution.\n\n    Optional arguments (default to False):\n\n    - `distribution`: add a 'distribution' key to the dictionary with the\n    value being the string representation of the distribution's name\n    (e.g. 'asymmetric_normal').\n    - `iterate`: If ProbabilityDistribution instances are among the\n    arguments (e.g. for KernelDensityEstimate), return the instance's\n    get_dict instead of the instance as value.\n    - `arraytolist`: convert numpy arrays to lists\n    \"\"\"\n    args = inspect.signature(self.__class__).parameters.keys()\n    d = self.__dict__\n    od = OrderedDict()\n    if distribution:\n        od['distribution'] = self.class_to_string()\n    od.update(OrderedDict((a, d[a]) for a in args))\n    if iterate:\n        for k in od:\n            if isinstance(od[k], ProbabilityDistribution):\n                od[k] = od[k].get_dict(distribution=True)\n    if arraytolist:\n        for k in od:\n            if isinstance(od[k], np.ndarray):\n                od[k] = od[k].tolist()\n            if isinstance(od[k], list):\n                for i, x in enumerate(od[k]):\n                    if isinstance(x, np.ndarray):\n                        od[k][i] = od[k][i].tolist()\n    for k in od:\n        if isinstance(od[k], int):\n            od[k] = int(od[k])\n        elif isinstance(od[k], float):\n            od[k] = float(od[k])\n        if isinstance(od[k], list):\n            for i, x in enumerate(od[k]):\n                if isinstance(x, float):\n                    od[k][i] = float(od[k][i])\n                elif isinstance(x, int):\n                    od[k][i] = int(od[k][i])\n    return od\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution.get_subclasses","title":"<code>get_subclasses()</code>  <code>classmethod</code>","text":"<p>Return all subclasses (including subclasses of subclasses).</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>@classmethod\ndef get_subclasses(cls):\n    \"\"\"Return all subclasses (including subclasses of subclasses).\"\"\"\n    for subclass in cls.__subclasses__():\n        yield from subclass.get_subclasses()\n        yield subclass\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.ProbabilityDistribution.get_yaml","title":"<code>get_yaml(*args, **kwargs)</code>","text":"<p>Get a YAML string representing the dictionary returned by the get_dict method.</p> <p>Arguments will be passed to <code>yaml.dump</code>.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def get_yaml(self, *args, **kwargs):\n    \"\"\"Get a YAML string representing the dictionary returned by the\n    get_dict method.\n\n    Arguments will be passed to `yaml.dump`.\"\"\"\n    od = self.get_dict(distribution=True, iterate=True, arraytolist=True)\n    return yaml.dump(od, *args, **kwargs)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.confidence_level","title":"<code>confidence_level(nsigma)</code>  <code>cached</code>","text":"<p>Return the confidence level corresponding to a number of sigmas, i.e. the probability contained in the normal distribution between \\(-n\\sigma\\) and \\(+n\\sigma\\).</p> <p>Example: <code>confidence_level(1)</code> returns approximately 0.68.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>@lru_cache(maxsize=10)\ndef confidence_level(nsigma):\n    r\"\"\"Return the confidence level corresponding to a number of sigmas,\n    i.e. the probability contained in the normal distribution between $-n\\sigma$\n    and $+n\\sigma$.\n\n    Example: `confidence_level(1)` returns approximately 0.68.\"\"\"\n    return (scipy.stats.norm.cdf(nsigma)-0.5)*2\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.normal_logpdf","title":"<code>normal_logpdf(x, mu, sigma)</code>","text":"<p>Logarithm of the PDF of the normal distribution</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def normal_logpdf(x, mu, sigma):\n    \"\"\"Logarithm of the PDF of the normal distribution\"\"\"\n    # this turns out to be 2 orders of magnitude faster than scipy.stats.norm.logpdf\n    if isinstance(x, float):\n        _x = x\n    else:\n        _x = np.asarray(x)\n    return -(_x-mu)**2/sigma**2/2 - math.log(math.sqrt(2*math.pi)*sigma)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.normal_pdf","title":"<code>normal_pdf(x, mu, sigma)</code>","text":"<p>PDF of the normal distribution</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def normal_pdf(x, mu, sigma):\n    \"\"\"PDF of the normal distribution\"\"\"\n    # this turns out to be 2 orders of magnitude faster than scipy.stats.norm.logpdf\n    if isinstance(x, float):\n        _x = x\n    else:\n        _x = np.asarray(x)\n    return np.exp(-(_x-mu)**2/sigma**2/2)/(np.sqrt(2*math.pi)*sigma)\n</code></pre>"},{"location":"jelli/utils/probability/#jelli.utils.probability.string_to_class","title":"<code>string_to_class(string)</code>","text":"<p>Get a ProbabilityDistribution subclass from a string. This can either be the class name itself or a string in underscore format as returned from <code>class_to_string</code>.</p> Source code in <code>jelli/utils/probability.py</code> <pre><code>def string_to_class(string):\n    \"\"\"Get a ProbabilityDistribution subclass from a string. This can\n    either be the class name itself or a string in underscore format\n    as returned from `class_to_string`.\"\"\"\n    try:\n        return eval(string)\n    except NameError:\n        pass\n    for c in ProbabilityDistribution.get_subclasses():\n        if c.class_to_string() == string:\n            return c\n    raise NameError(\"Distribution \" + string + \" not found.\")\n</code></pre>"}]}